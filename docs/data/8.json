{
    "800": {
        "file_id": 46,
        "content": "Katerina Fragkiadaki](https://www.cs.cmu.edu/~katef/), [Zackory Erickson](https://zackory.com/), [David Held](https://davheld.github.io/), [Chuang Gan](https://people.csail.mit.edu/ganchuang/)   \nRoboGen is a **self-guided** and **generative** robotic agent that autonomously proposes **new tasks**, generates corresponding **environments**, and acquires **new robotic skills** continuously.\nRoboGen is powered by [Genesis](https://github.com/Genesis-Embodied-AI/Genesis), a multi-material multi-solver generative simulation engine for general-purpose robot learning. \nGenesis is still under active development and will be released soon. This repo contains a re-implementation of RoboGen using PyBullet, containing generation and learning of rigid manipulation and locomotion tasks. Our full pipeline containing soft-body manipulation and more tasks will be released later together with Genesis.\nWe are still in the process of cleaning the code & testing, please stay tuned!\n## Table of Contents\n- [Setup](#setup)",
        "type": "code",
        "location": "/readme.md:25-35"
    },
    "801": {
        "file_id": 46,
        "content": "RoboGen is a self-guided and generative robotic agent that proposes new tasks, generates environments, and acquires skills autonomously. It's powered by Genesis, an AI engine for robot learning, which is still under development. The codebase here uses PyBullet for rigid manipulation and locomotion tasks; soft-body manipulation and more tasks will be added later with Genesis. The code is currently being cleaned and tested, stay tuned!",
        "type": "comment"
    },
    "802": {
        "file_id": 46,
        "content": "  - [RoboGen](#RoboGen)\n  - [OMPL](#Open-Motion-Planning-Library)\n  - [Dataset](#dataset)\n- [Let's Rock!](#lets-rock)\n  - [Automated Task Generation & Skill Learning](#One-click-for-all)\n  - [Generate Tasks](#Generate-tasks)\n  - [Learn Skills](#Learn-skills)\n  - [Pre-generated Tasks](#Pre-generated-tasks)\n## Setup\n### RoboGen\nClone this git repo.\n```\ngit clone https://github.com/Genesis-Embodied-AI/RoboGen.git\n```\nWe recommend working with a conda environment.\n```\nconda env create -f environment.yaml\nconda activate robogen\n```\nIf installing from this yaml file doesn't work, manual installation of missing packages should also work.\n### Open Motion Planning Library\nRoboGen leverages [Open Motion Planning Library (OMPL)](https://ompl.kavrakilab.org/) for motion planning as part of the pipeline to solve the generated task. \nTo install OMPL, run\n```\n./install_ompl_1.5.2.sh --python\n```\nwhich will install the ompl with system-wide python. Note at line 19 of the installation script OMPL requries you to run `sudo apt-g",
        "type": "code",
        "location": "/readme.md:36-63"
    },
    "803": {
        "file_id": 46,
        "content": "The code is providing instructions for setting up the RoboGen environment using Git and a conda environment. It also explains that the project utilizes Open Motion Planning Library (OMPL) for motion planning as part of its pipeline, and provides commands to install OMPL system-wide for Python.",
        "type": "comment"
    },
    "804": {
        "file_id": 46,
        "content": "et -y upgrade`. This might cause trouble for your system packages, so you could probably comment this line during installation (the installation might fail, not fully tested with the line commented).\nThen, export the installation to the conda environment to be used with RoboGen:\n```\necho \"path_to_your_ompl_installation_from_last_step/OMPL/ompl-1.5.2/py-bindings\" >> ~/miniconda3/envs/robogen/lib/python3.9/site-packages/ompl.pth\n```\nremember to change the path to be your ompl installed path and conda environment path.\n### Dataset\nRoboGen uses [PartNet-Mobility](https://sapien.ucsd.edu/browse) for task generation and scene population. We provide a parsed version [here](https://drive.google.com/file/d/1d-1txzcg_ke17NkHKAolXlfDnmPePFc6/view?usp=sharing) (which parses the urdf to extract the articulation tree as a shortened input to GPT-4). After downloading, please unzip it and put it in the `data` folder, so it looks like `data/dataset`.\nFor retrieving objects from objaverse, we embed object descriptions from objaverse using [SentenceBert](https://www.sbert.net/). ",
        "type": "code",
        "location": "/readme.md:63-74"
    },
    "805": {
        "file_id": 46,
        "content": "Upgrade the system packages, which might cause trouble if commented during installation (not fully tested). Export OMPL installation to conda environment for use with RoboGen. Update the path accordingly. RoboGen uses PartNet-Mobility dataset and provides parsed version [here]. Download and unzip it to `data/dataset`. Retrieve objects from objaverse using SentenceBert.",
        "type": "comment"
    },
    "806": {
        "file_id": 46,
        "content": "If you want to generate these embeddings by yourself, run\n```\npython objaverse_utils/embed_all_annotations.py\npython objaverse_utils/embed_cap3d.py\npython objaverse_utils/embed_partnet_annotations.py\n```\nWe also provide the embeddings [here](https://drive.google.com/file/d/1dFDpG3tlckTUSy7VYdfkNqtfVctpn3T6/view?usp=sharing) if generation takes too much time. After downloading, unzip and put it under `objaverse_utils/data/` folder, so it looks like \n```\nobjaverse_utils/data/default_tag_embeddings_*.pt\nobjaverse_utils/data/default_tag_names_*.pt\nobjaverse_utils/data/default_tag_uids_*.pt\nobjaverse_utils/data/cap3d_sentence_bert_embeddings.pt\nobjaverse_utils/data/partnet_mobility_category_embeddings.pt\n```\n## Let's Rock!\n### One click for all\nPut your OpenAI API key at the top of `gpt_4/query.py`, and simply run\n```\nsource prepare.sh\npython run.py\n``` \nRoboGen will then generate the task, build the scene in pybullet, and solve it to learn the corresponding skill.  \nIf you wish to generate manipulation tasks relevant to a specific object, e.g., microwave, you can run  ",
        "type": "code",
        "location": "/readme.md:75-99"
    },
    "807": {
        "file_id": 46,
        "content": "The code provides instructions for generating embeddings, obtaining default tag embeddings from a Google Drive link or generating them by running specific scripts. It also mentions the requirement to place the files in a specified folder structure and highlights the need to put an OpenAI API key in `gpt_4/query.py` before running the main script (`run.py`) for RoboGen to generate tasks, build scenes, and solve problems to learn skills.",
        "type": "comment"
    },
    "808": {
        "file_id": 46,
        "content": "```\npython run.py --category Microwave\n```\n### Generate tasks\nIf you wish to just generate the tasks, run\n```\npython run.py --train 0\n```\nwhich will generate the tasks, scene config yaml files, and training supervisions. The generated tasks will be stored at `data/generated_tasks_release/`.  \nIf you want to generate task given a text description, you can run\n```\npython gpt_4/prompts/prompt_from_description.py --task_description [TASK_DESCRIPTION] --object [PARTNET_ARTICULATION_OBJECT_CATEGORY]\n``` \nFor example,\n```\npython gpt_4/prompts/prompt_from_description.py --task_description \"Put a pen into the box\" --object \"Box\"\n```\n### Learn skills\nIf you wish to just learn the skill with a generated task, run\n```\npython execute.py --task_config_path [PATH_TO_THE_GENERATED_TASK_CONFIG] # for manipulation tasks\npython execute_locomotion.py --task_config_path [PATH_TO_THE_GENERATED_TASK_CONFIG] # for locomotion tasks\n```\nFor example,\n```\npython execute.py --task_config_path example_tasks/Change_Lamp_Direction/Change_Lam",
        "type": "code",
        "location": "/readme.md:100-127"
    },
    "809": {
        "file_id": 46,
        "content": "This code describes how to use the RoboGen software for generating tasks, learning skills, and executing them. The script allows users to generate tasks by running `python run.py --train 0` or generate tasks from a given description using `python gpt_4/prompts/prompt_from_description.py`. Skills can be learned by running the appropriate execute or execute_locomotion scripts with the generated task configuration as input, such as `python execute.py --task_config_path example_tasks/Change_Lamp_Direction/Change_Lam`.",
        "type": "comment"
    },
    "810": {
        "file_id": 46,
        "content": "p_Direction_The_robotic_arm_will_alter_the_lamps_light_direction_by_manipulating_the_lamps_head.yaml  \npython execute_locomotion.py --task_config_path example_tasks/task_Turn_right/Turn_right.yaml\n```\n### Pre-generated tasks\nIn `example_tasks` we include a number of generated tasks from RoboGen. We hope this could be useful for, e.g., language conditioned multi-task learning & transfer learning & low-level skill learning. We hope to keep updating the list! \n## Acknowledgements\n- The interface between OMPL and pybullet is based on [pybullet_ompl](https://github.com/lyfkyle/pybullet_ompl).\n- Part of the objaverse annotations are from [Scalable 3D Captioning with Pretrained Models](https://arxiv.org/abs/2306.07279)",
        "type": "code",
        "location": "/readme.md:127-136"
    },
    "811": {
        "file_id": 46,
        "content": "This code snippet is calling the `execute_locomotion.py` script with a task configuration file located at `example_tasks/task_Turn_right/Turn_right.yaml`. The script executes a pre-generated task related to turning right, possibly for learning and transfer purposes in RoboGen's environment.",
        "type": "comment"
    },
    "812": {
        "file_id": 47,
        "content": "/run.py",
        "type": "filepath"
    },
    "813": {
        "file_id": 47,
        "content": "This Python script uses argparse to parse arguments, defines dictionaries for temperature values and OpenAI model names, creates folders if necessary, generates task configurations, calls 'generate_distractor', saves meta-information in JSON files with timestamps, sets random seed, checks for locomotion tasks, and learns skills using RL algorithms. It executes \"execute_locomotion.py\" to store learned results.",
        "type": "summary"
    },
    "814": {
        "file_id": 47,
        "content": "from gpt_4.prompts.prompt_manipulation import generate_task as generate_task_manipulation\nfrom gpt_4.prompts.prompt_distractor import generate_distractor\nfrom gpt_4.prompts.prompt_locomotion import generate_task_locomotion\nfrom manipulation.partnet_category import partnet_categories\nimport json\nimport time, datetime\nimport numpy as np\nimport os\nimport yaml\nimport argparse \nparser = argparse.ArgumentParser()\nparser.add_argument('--train', type=int, default=1)\nparser.add_argument('--category', type=str, default=None)\nargs = parser.parse_args()\ntemperature_dict = {\n    \"task_generation\": 0.6,\n    \"reward\": 0.2,\n    \"yaml\": 0.3,\n    \"size\": 0.1,\n    \"joint\": 0,\n    \"spatial_relationship\": 0\n}\nmodel_dict = {\n    \"task_generation\": \"gpt-4\",\n    \"reward\": \"gpt-4\",\n    \"yaml\": \"gpt-4\",\n    \"size\": \"gpt-4\",\n    \"joint\": \"gpt-4\",\n    \"spatial_relationship\": \"gpt-4\"\n}\n### store information\n### generate task, return config path\nmeta_path = \"generated_tasks_release\"\nif not os.path.exists(\"data/{}\".format(meta_path)):\n    os.makedirs(\"data/{}\".format(meta_path))",
        "type": "code",
        "location": "/run.py:1-40"
    },
    "815": {
        "file_id": 47,
        "content": "This Python script imports various functions and classes, uses argparse to parse command line arguments, defines dictionaries for temperature values and OpenAI model names. It then creates a folder if it doesn't exist and appears to be involved in generating tasks or prompts for some AI system.",
        "type": "comment"
    },
    "816": {
        "file_id": 47,
        "content": "time_string = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d-%H-%M-%S')\nsave_folder = \"data/{}/meta-temperature-{}.json\".format(meta_path, time_string)\nwith open(save_folder, 'w') as f:\n    json.dump(temperature_dict, f, indent=4)\nsave_folder = \"data/{}/meta-model-{}.json\".format(meta_path, time_string)\nwith open(save_folder, 'w') as f:\n    json.dump(model_dict, f, indent=4)\nnp.random.seed(int(time.time()))\ngen_func = np.random.choice(['manipulation', 'locomotion'])\ngen_func = 'manipulation' if args.category is not None else gen_func\nif gen_func == 'manipulation':\n    if args.category is None:\n        object_category = partnet_categories[np.random.randint(len(partnet_categories))]\n    else:\n        object_category = args.category\n    all_task_config_paths = generate_task_manipulation(object_category, temperature_dict=temperature_dict, model_dict=model_dict, meta_path=meta_path)\n    for task_config_path in all_task_config_paths:\n        generate_distractor(task_config_path, temperature_dict=temperature_dict, model_dict=model_dict)",
        "type": "code",
        "location": "/run.py:41-59"
    },
    "817": {
        "file_id": 47,
        "content": "This code generates task configurations for manipulation tasks using the 'generate_task_manipulation' function, and then calls the 'generate_distractor' function on each configuration path. It also saves meta-information about temperature and model to JSON files with a timestamp in the file name for identification. The random seed is set based on the current timestamp to ensure reproducibility of the random choices made in the code.",
        "type": "comment"
    },
    "818": {
        "file_id": 47,
        "content": "elif gen_func == 'locomotion':\n    all_task_config_paths = generate_task_locomotion(temperature_dict=temperature_dict, model_dict=model_dict, meta_path=meta_path)\nif args.train:\n    for task_config_path in all_task_config_paths:\n        print(\"trying to learn skill: \", task_config_path)\n        try:\n            if gen_func == 'manipulation':\n                print(\"task_config_path: \", task_config_path)\n                with open(task_config_path, 'r') as f:\n                    task_config = yaml.safe_load(f)\n                solution_path = None\n                for obj in task_config:\n                    if \"solution_path\" in obj:\n                        solution_path = obj[\"solution_path\"]\n                        break\n                ## run RL once for each substep\n                os.system(\"python execute.py --task_config_path {}\".format(task_config_path))\n                ## or run RL multiple times for each substep and pick the best result for next substep\n                # os.system(\"python execute_long_horizon.py --task_config_path {}\".format(task_config_path))",
        "type": "code",
        "location": "/run.py:60-81"
    },
    "819": {
        "file_id": 47,
        "content": "This code checks if the function is for generating locomotion tasks. If so, it generates all task configurations and prints the path of each task configuration. It then tries to learn skills by running either a single or multiple instances of the RL algorithm depending on the command-line argument \"train\". The learned results are stored in the solution_path.",
        "type": "comment"
    },
    "820": {
        "file_id": 47,
        "content": "            else:\n                os.system(\"python execute_locomotion.py --task_config_path {} & \".format(task_config_path))\n        except Exception as e:\n            print(\"=\" * 20, \"an error occurred\", \"=\" * 20)\n            print(\"an error occurred: \", e)\n            print(\"=\" * 20, \"an error occurred\", \"=\" * 20)\n            print(\"failed to execute task: \", task_config_path)\n            continue",
        "type": "code",
        "location": "/run.py:82-90"
    },
    "821": {
        "file_id": 47,
        "content": "This code block runs the \"execute_locomotion.py\" script with the specified task configuration file, in case of failure it prints an error message and continues execution.",
        "type": "comment"
    }
}