{
    "200": {
        "file_id": 10,
        "content": "    generate_distractor(config_path, temperature_dict=temperature_dict, model_dict=model_dict)",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_from_description.py:248-248"
    },
    "201": {
        "file_id": 10,
        "content": "This function is calling the \"generate_distractor\" method, taking in two parameters: \"config_path\" and an optional \"temperature_dict\". It also has a second optional parameter called \"model_dict\". The purpose of this code may be to generate a distractor using the provided configuration path, possibly with customizable temperature and model settings.",
        "type": "comment"
    },
    "202": {
        "file_id": 11,
        "content": "/gpt_4/prompts/prompt_locomotion.py",
        "type": "filepath"
    },
    "203": {
        "file_id": 11,
        "content": "The code employs reinforcement learning for quadruped/humanoid locomotion, utilizing GPT to generate tasks and YAML format for task configuration. It saves data in specified directories and returns the generated task config path.",
        "type": "summary"
    },
    "204": {
        "file_id": 11,
        "content": "import os\nuser_contents = [\n\"\"\"\nYour goal is to propose some locomotion tasks for a quadruped/humanoid robot, and writing the corresponding reward functions for the quadruped to learn that specific locomotion skill in a simulator, using reinforcement learning.\nHere are some examples:\nFirst example:\nSkill: flip rightwards\nReward:\n```python\ndef _compute_reward(self):\n    # we first get some information of the quadruped/humanoid robot.\n    # COM_pos and COM_quat are the position and orientation (quaternion) of the center of mass of the quadruped/humanoid.\n    COM_pos, COM_quat = get_robot_pose(self)\n    # COM_vel, COM_ang are the velocity and angular velocity of the center of mass of the quadruped/humanoid.\n    COM_vel, COM_ang = get_robot_velocity(self)\n    # face_dir, side_dir, and up_dir are three axes of the rotation of the quadruped/humanoid.\n    # face direction points from the center of mass towards the face direction of the quadruped/humanoid.\n    # side direction points from the center of mass towards the side body direction of the quadruped/humanoid.",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_locomotion.py:1-22"
    },
    "205": {
        "file_id": 11,
        "content": "Importing necessary modules and defining the user_contents list, which contains example prompts for proposing locomotion tasks, reward functions for quadruped/humanoid robots in a simulator using reinforcement learning.",
        "type": "comment"
    },
    "206": {
        "file_id": 11,
        "content": "    # up direction points from the center of mass towards up, i.e., the negative direction of the gravity. \n    # gravity direction is [0, 0, -1].\n    # when initialized, the face of the robot is along the x axis, the side of the robot is along the y axis, and the up of the robot is along the z axis.\n    face_dir, side_dir, up_dir = get_robot_direction(self, COM_quat)\n    target_side = np.array([0, 1, 0]) # maintain initial side direction during flip\n    target_ang = np.array([50, 0, 0.0]) # spin around x axis to do the rightwards flip, since x is the face direction of the robot.\n    alpha_ang = 1.0\n    alpha_side = 1.0\n    r_ang    = - alpha_ang * np.linalg.norm(COM_ang - target_ang)\n    r_side   = - alpha_side * np.linalg.norm(side_dir - target_side)\n    r += r_ang + r_side\n    # there is a default energy term that penalizes the robot for consuming too much energy. This should be included for all skill.\n    r_energy = get_energy_reward(self)\n    return r + r_energy\n```\nsome more examples:\n{}\nCan you think of 3 more locomotion skills that a quadruped/humanoid can perform?",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_locomotion.py:23-46"
    },
    "207": {
        "file_id": 11,
        "content": "This code calculates the reward for a quadruped/humanoid locomotion skill. It initializes face_dir, side_dir, and up_dir using get_robot_direction function. The target_side and target_ang are set to maintain the initial side direction and spin around x axis during flip. Alpha_ang and alpha_side are defined as 1.0. r_ang and r_side are calculated as -alpha_ang*norm(COM_ang-target_ang) and -alpha_side*norm(side_dir-target_side), respectively. The code adds r_ang and r_side to r. It includes a default energy term that penalizes the robot for consuming too much energy, returned as r+r_energy.",
        "type": "comment"
    },
    "208": {
        "file_id": 11,
        "content": "For each skill,\nYour output format should be:\nSkill: <skill name>\nReward:\n```python\ndef _compute_reward(self):\n    # your code here\n    return r\n```\n\"\"\"\n]\ngood_exmaples = [\n\"\"\"\nSkill: jump backward\nReward:\n```python\ndef _compute_reward(self):\n    # we first get some information of the quadruped/humanoid.\n    # COM_pos and COM_quat are the position and orientation (quaternion) of the center of mass of the quadruped/humanoid.\n    COM_pos, COM_quat = get_robot_pose(self)\n    # COM_vel, COM_ang are the velocity and angular velocity of the center of mass of the quadruped/humanoid.\n    COM_vel, COM_ang = get_robot_velocity(self)\n    # face_dir, side_dir, and up_dir are three axes of the rotation of the quadruped/humanoid.\n    # face direction points from the center of mass towards the face direction of the quadruped/humanoid.\n    # side direction points from the center of mass towards the side body direction of the quadruped/humanoid.\n    # up direction points from the center of mass towards up, i.e., the negative direction of the gravity. ",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_locomotion.py:48-76"
    },
    "209": {
        "file_id": 11,
        "content": "This code computes a reward for a specific skill. It retrieves information about the robot's center of mass position and orientation, velocity, angular velocity, and rotation axes. The purpose of these values may be to evaluate how well the robot performed the skill.",
        "type": "comment"
    },
    "210": {
        "file_id": 11,
        "content": "    # gravity direction is [0, 0, -1].\n    # when initialized, the face of the robot is along the x axis, the side of the robot is along the y axis, and the up of the robot is along the z axis.\n    face_dir, side_dir, up_dir = get_robot_direction(self, COM_quat)\n    if self.time_step <= 30: # first a few steps the robot are jumping\n        target_height = 5.0\n    else: # then it should not jump\n        target_height = 0.0\n    target_v = np.array([-5.0, 0, 0.0]) # jump backwards\n    target_up = np.array([0, 0, 1]) # maintain up direction\n    target_face = np.array([1, 0, 0]) # maintain initial face direction\n    target_side = np.array([0, 1, 0]) # maintain initial side direction\n    target_ang = np.array([0, 0, 0.0]) # don't let the robot spin\n    alpha_vel = 5.0\n    alpha_ang = 1.0\n    alpha_face = 1.0\n    alpha_up = 1.0\n    alpha_side = 1.0\n    alpha_height = 10.0\n    r_vel    = - alpha_vel * np.linalg.norm(COM_vel - target_v)\n    r_ang    = - alpha_ang * np.linalg.norm(COM_ang - target_ang)\n    r_face   = - alpha_face * np.linalg.norm(face_dir - target_face)",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_locomotion.py:77-101"
    },
    "211": {
        "file_id": 11,
        "content": "The code initializes target velocities and directions for a robot, including jumping backward. For the first 30 time steps, it sets a target height of 5.0 units, then switches to a target height of 0.0 afterward. It defines alpha values and calculates distances from current state to targets for velocity, angular position, and face/up/side directions to compute the reward function.",
        "type": "comment"
    },
    "212": {
        "file_id": 11,
        "content": "    r_up     = - alpha_up * np.linalg.norm(up_dir - target_up)\n    r_side   = - alpha_side * np.linalg.norm(side_dir - target_side)\n    r_height = - alpha_height * np.linalg.norm(COM_pos[2] - target_height)\n    r = r_vel + r_ang + r_face + r_up + r_side + r_height\n    # there is a default energy term that penalizes the robot for consuming too much energy. This should be included for all skill.\n    r_energy = get_energy_reward(self)\n    return r + r_energy\n```\n\"\"\",\n\"\"\"\nSkill: walk forward\nReward:\n```python\ndef _compute_reward(self):\n    # we first get some information of the quadruped/humanoid.\n    # COM_pos and COM_quat are the position and orientation (quaternion) of the center of mass of the quadruped/humanoid.\n    COM_pos, COM_quat = get_robot_pose(self)\n    # COM_vel, COM_ang are the velocity and angular velocity of the center of mass of the quadruped/humanoid.\n    COM_vel, COM_ang = get_robot_velocity(self)\n    # face_dir, side_dir, and up_dir are three axes of the rotation of the quadruped/humanoid.",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_locomotion.py:102-124"
    },
    "213": {
        "file_id": 11,
        "content": "This code calculates the reward for a walking skill. It combines various penalties based on velocity, angle, rotation axes deviation from target values, and an energy term that discourages excessive energy consumption. The reward is then returned after including the energy term.",
        "type": "comment"
    },
    "214": {
        "file_id": 11,
        "content": "    # face direction points from the center of mass towards the face direction of the quadruped/humanoid.\n    # side direction points from the center of mass towards the side body direction of the quadruped/humanoid.\n    # up direction points from the center of mass towards up, i.e., the negative direction of the gravity. \n    # gravity direction is [0, 0, -1].\n    # when initialized, the face of the robot is along the x axis, the side of the robot is along the y axis, and the up of the robot is along the z axis.\n    face_dir, side_dir, up_dir = get_robot_direction(self, COM_quat)\n    # a skill can be catergorized by target velocity, target body height, target up/side/face direction, as well as target angular velocity of the quadruped/humanoid. \n    target_v = np.array([1.0, 0, 0]) # since the robot faces along x axis initially, for walking forward, the target velocity would just be [1, 0, 0]\n    target_height = self.COM_init_pos[2] # we want the robot to keep the original height when walkin, so it does not fall down.",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_locomotion.py:125-134"
    },
    "215": {
        "file_id": 11,
        "content": "This code initializes the face, side, and up directions for a robot based on its center of mass (COM) and uses these directions to set target velocities and heights. The target velocity is set as [1.0, 0, 0] since the robot faces along the x-axis initially, and it keeps the original height while walking to prevent falling down.",
        "type": "comment"
    },
    "216": {
        "file_id": 11,
        "content": "    target_face = np.array([1, 0, 0]) # the target_face keeps the robot facing forward.\n    target_side = np.array([0, 1, 0]) # for walking forward, the side direction does not really matter.\n    target_up = np.array([0, 0, 1]) # the target_up keeps the robot standing up.\n    target_ang = np.array([0, 0, 0]) # for walking forward, the angular velocity does not really matter.\n    # note in this example, the real goal can be specified using only 1 term, i.e., the target velocity being [1, 0, 0].\n    # howeever, to make the learning reliable, we need the auxiliary terms such as target_height, target_face, and target_up terms to keep the quadruped/humanoid stable during the RL exploration phase.\n    # you should try to keep these auxiliary terms for stability as well when desigining the reward.\n    # we use these coefficients to turn on/off and weight each term. For walking, we only control the target velocity, height, and face and up direction.\n    alpha_vel = 1.0\n    alpha_height = 1.0\n    alpha_face = 1.0",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_locomotion.py:135-147"
    },
    "217": {
        "file_id": 11,
        "content": "The code defines the target positions (face, side, up) and angular velocity for a robot, and uses coefficients to control each term. These auxiliary terms ensure stability during the RL exploration phase. The target velocity, height, face, and up direction are controlled for walking.",
        "type": "comment"
    },
    "218": {
        "file_id": 11,
        "content": "    alpha_side = 0.0\n    alpha_up = 1.0\n    alpha_ang = 0.0\n    r_vel    = - alpha_vel * np.linalg.norm(COM_vel - target_v)\n    r_height = - alpha_height * np.linalg.norm(COM_pos[2] - target_height)\n    r_face   = - alpha_face * np.linalg.norm(face_dir - target_face)\n    r_side    = - alpha_side * np.linalg.norm(side_dir - target_side)\n    r_up     = - alpha_up * np.linalg.norm(up_dir - target_up)\n    r_ang    = - alpha_ang * np.linalg.norm(COM_ang - target_ang)\n    r = r_vel + r_height + r_face + r_side + r_up + r_ang\n    # there is a default energy term that penalizes the robot for consuming too much energy. This should be included for all skill.\n    r_energy = get_energy_reward(self)\n    return r + r_energy\n``` \n\"\"\", \n]\nassistant_contents = []\nreward_file_header1 = \"\"\"\nfrom locomotion.sim import SimpleEnv\nimport numpy as np\nimport gym\nfrom locomotion.gpt_reward_api import *\nclass {}(SimpleEnv):\n\"\"\"\nreward_file_header2 = \"\"\"\n    def __init__(self, task_name, *args, **kwargs):\n        super().__init__(*args, **kwargs)",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_locomotion.py:148-181"
    },
    "219": {
        "file_id": 11,
        "content": "This code calculates a reward for a robot's movement based on its velocity, height, facing direction, sideways movement, upwards movement, and angular position relative to the target values. It also includes an energy term that penalizes the robot for consuming too much energy.",
        "type": "comment"
    },
    "220": {
        "file_id": 11,
        "content": "        self.task_name = task_name\n        self.detected_position = {}\n\"\"\"\nreward_file_end = \"\"\"\ngym.register(\n    id='gym-{}-v0',\n    entry_point={},\n)\n\"\"\"\nimport time\nimport datetime\nimport numpy as np\nimport copy\nfrom gpt_4.query import query\nimport yaml\nimport json\ndef generate_task_locomotion(model_dict, temperature_dict, meta_path='generated_tasks_locomotion'):\n    system = \"You are a helpful assistant.\"\n    ts = time.time()\n    time_string = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n    save_path = \"data/{}/{}\".format(meta_path, \"locomotion_\" + time_string)\n    print(\"=\" * 30)\n    print(\"querying GPT to imagine the tasks\")\n    print(\"=\" * 30)\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n    if not os.path.exists(save_path + \"/gpt_response\"):\n        os.makedirs(save_path + \"/gpt_response\")\n    sampled_good_examples = np.random.choice(good_exmaples, 2, replace=False)\n    sampled_good_examples = \"\\n\".join(sampled_good_examples)\n    new_user_contents = []\n    copied_user_contents = copy.deepcopy(user_contents)",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_locomotion.py:182-222"
    },
    "221": {
        "file_id": 11,
        "content": "This code generates locomotion tasks using a GPT model. It saves the generated tasks in a specified directory, creates subdirectories if they do not exist, and samples 2 good examples from a list of examples.",
        "type": "comment"
    },
    "222": {
        "file_id": 11,
        "content": "    new_user_contents.append(copied_user_contents[0].format(sampled_good_examples))\n    task_save_path = os.path.join(save_path, \"gpt_response/task_generation.json\")\n    response = query(system, new_user_contents, assistant_contents, save_path=task_save_path, \n                     model=model_dict['task_generation'], temperature=temperature_dict[\"task_generation\"])\n    response = response.split(\"\\n\")\n    tasks = []\n    rewards = []\n    for l_idx, line in enumerate(response):\n        line = line.lower()\n        if \"skill:\" in line.lower():\n            tasks.append(response[l_idx])\n            reward = []\n            start_idx = l_idx + 1\n            for l_idx_2 in range(start_idx, len(response)):\n                if \"```python\" in response[l_idx_2].lower():\n                    start_idx = l_idx_2 + 1\n                    break\n            for l_idx_2 in range(start_idx, len(response)):\n                if response[l_idx_2].startswith(\"```\"):\n                    break\n                reward.append(response[l_idx_2])",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_locomotion.py:223-246"
    },
    "223": {
        "file_id": 11,
        "content": "This code is parsing and extracting tasks from a generated response, along with their corresponding rewards. It starts by appending formatted contents to new_user_contents, then queries a model to generate a task generation response. The response is split into lines, and tasks are identified using the marker \"skill:\". Rewards are extracted until reaching the next code block marked with \"```python\", which serves as the end of each task definition.",
        "type": "comment"
    },
    "224": {
        "file_id": 11,
        "content": "            reward[0] = \"    \" + reward[0]\n            for idx in range(1, len(reward)):\n                reward[idx] = \"        \" + reward[idx]\n            reward = \"\\n\".join(reward)\n            rewards.append(reward)\n    generated_task_configs = []\n    for task, reward in zip(tasks, rewards):\n        print(\"task is: \", task)\n        task_description = task.split(\": \")[1]\n        description = f\"{task_description}\".replace(\" \", \"_\").replace(\".\", \"\").replace(\",\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n        solution_path = os.path.join(save_path, \"task_{}\".format(description))\n        if not os.path.exists(solution_path):\n            os.makedirs(solution_path)\n        reward_file_name = os.path.join(solution_path, f\"{description}.py\")\n        header = reward_file_header1.format(description)\n        end = reward_file_end.format(description, description)   \n        file_content =  header + reward_file_header2 + reward + end\n        with open(reward_file_name, \"w\") as f:\n            f.write(file_content)\n        config_path = save_path",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_locomotion.py:248-274"
    },
    "225": {
        "file_id": 11,
        "content": "Code adds rewards to each task, formats them into a file, and appends it to the solution path.",
        "type": "comment"
    },
    "226": {
        "file_id": 11,
        "content": "        save_name =  description + '.yaml'\n        task_config_path = os.path.join(config_path, save_name)\n        parsed_yaml = []\n        parsed_yaml.append(dict(solution_path=solution_path))\n        with open(task_config_path, 'w') as f:\n            yaml.dump(parsed_yaml, f, indent=4)\n        with open(os.path.join(solution_path, \"config.yaml\"), 'w') as f:\n            yaml.dump(parsed_yaml, f, indent=4)\n        generated_task_configs.append(task_config_path)\n    return generated_task_configs",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_locomotion.py:275-286"
    },
    "227": {
        "file_id": 11,
        "content": "This code saves a task configuration file in YAML format. It takes a description and solution path as inputs, creates a save name, constructs the full paths for the task and solution files, writes the parsed YAML data to both files, and adds the generated task config path to a list before returning it.",
        "type": "comment"
    },
    "228": {
        "file_id": 12,
        "content": "/gpt_4/prompts/prompt_manipulation.py",
        "type": "filepath"
    },
    "229": {
        "file_id": 12,
        "content": "This code enables Franka Panda arm to perform household manipulation tasks by generating task descriptions, handling delays and extracting information for successful task execution.",
        "type": "summary"
    },
    "230": {
        "file_id": 12,
        "content": "import numpy as np\nimport copy\nimport time, datetime\nimport os\nimport json\nfrom objaverse_utils.utils import partnet_mobility_dict\nfrom gpt_4.prompts.utils import build_task_given_text, parse_task_response\nfrom gpt_4.query import query\ntask_user_contents = \"\"\"\nI will give you an articulated object, with its articulation tree and semantics. Your goal is to imagine some tasks that a robotic arm can perform with this articulated object in household scenarios. You can think of the robotic arm as a Franka Panda robot. The task will be built in a simulator for the robot to learn it. \nFocus on manipulation or interaction with the object itself. Sometimes the object will have functions, e.g., a microwave can be used to heat food, in these cases, feel free to include other objects that are needed for the task. \nPlease do not think of tasks that try to assemble or disassemble the object. Do not think of tasks that aim to clean the object or check its functionality. \nFor each task you imagined, please write in the following format: ",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation.py:1-16"
    },
    "231": {
        "file_id": 12,
        "content": "This code imports necessary libraries and defines the task prompt for generating robot tasks using a simulator. The user will provide an articulated object with its articulation tree and semantics, and the goal is to imagine manipulation or interaction tasks for a robotic arm (Franka Panda) in household scenarios, excluding assembly/disassembly and cleaning tasks.",
        "type": "comment"
    },
    "232": {
        "file_id": 12,
        "content": "Task name: the name of the task.\nDescription: some basic descriptions of the tasks. \nAdditional Objects: Additional objects other than the provided articulated object required for completing the task. \nLinks: Links of the articulated objects that are required to perform the task. \n- Link 1: reasons why this link is needed for the task\n- Link 2: reasons why this link is needed for the task\n- …\nJoints: Joints of the articulated objects that are required to perform the task. \n- Joint 1: reasons why this joint is needed for the task\n- Joint 2: reasons why this joint is needed for the task\n- …\nExample Input: \n```Oven articulation tree\nlinks: \nbase\nlink_0\nlink_1\nlink_2\nlink_3\nlink_4\nlink_5\nlink_6\nlink_7\njoints: \njoint_name: joint_0 joint_type: revolute parent_link: link_7 child_link: link_0\njoint_name: joint_1 joint_type: continuous parent_link: link_7 child_link: link_1\njoint_name: joint_2 joint_type: continuous parent_link: link_7 child_link: link_2\njoint_name: joint_3 joint_type: continuous parent_link: link_7 child_link: link_3",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation.py:17-48"
    },
    "233": {
        "file_id": 12,
        "content": "The code defines a task structure with properties such as the task name, description, required additional objects, links, joints, and an example input (oven articulation tree). Each link and joint has reasons for their necessity in completing the task.",
        "type": "comment"
    },
    "234": {
        "file_id": 12,
        "content": "joint_name: joint_4 joint_type: continuous parent_link: link_7 child_link: link_4\njoint_name: joint_5 joint_type: continuous parent_link: link_7 child_link: link_5\njoint_name: joint_6 joint_type: continuous parent_link: link_7 child_link: link_6\njoint_name: joint_7 joint_type: fixed parent_link: base child_link: link_7\n```\n```Oven semantics\nlink_0 hinge door\nlink_1 hinge knob\nlink_2 hinge knob\nlink_3 hinge knob\nlink_4 hinge knob\nlink_5 hinge knob\nlink_6 hinge knob\nlink_7 heavy oven_body\n```\nExample output:\nTask Name: Open Oven Door\nDescription: The robotic arm will open the oven door.\nAdditional Objects: None\nLinks:\n- link_0: from the semantics, this is the door of the oven. The robot needs to approach this door in order to open it. \nJoints: \n- joint_0: from the articulation tree, this is the revolute joint that connects link_0. Therefore, the robot needs to actuate this joint for opening the door.\nTask Name: Adjust Oven Temperature\nDescription: The robotic arm will turn one of the oven's hinge knobs to set a desired temperature.",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation.py:49-78"
    },
    "235": {
        "file_id": 12,
        "content": "The code represents a list of joints and links for a robotic arm, with each line detailing a specific joint's properties (name, type, parent link, child link) and the corresponding link's description (semantics). These details are crucial for understanding the arm's structure and functionality.",
        "type": "comment"
    },
    "236": {
        "file_id": 12,
        "content": "Additional Objects: None\nLinks:\n- link_1: the robot needs to approach link_1, which is assumed to be the temperature knob, to rotate it to set the temperature.\nJoints:\n- joint_1: joint_1 connects link_1 from the articulation tree. The robot needs to actuate it to rotate link_1 to the desired temperature.\nTask Name: Heat a hamburger Inside Oven \nDescription: The robot arm places a hamburger inside the oven, and sets the oven temperature to be appropriate for heating the hamburger.\nAdditional Objects: hamburger\nLinks:\n- link_0: link_0 is the oven door from the semantics. The robot needs to open the door in order to put the hamburger inside the oven.\nlink_1: the robot needs to approach link_1, which is the temperature knob, to rotate it to set the desired temperature.\nJoints:\n- joint_0: from the articulation tree, this is the revolute joint that connects link_0 (the door). Therefore, the robot needs to actuate this joint for opening the door.\n- joint_1: from the articulation tree, joint_1 connects lin",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation.py:79-94"
    },
    "237": {
        "file_id": 12,
        "content": "Code describes a robot task: heating a hamburger inside an oven. The robot opens the door, places the hamburger, and sets the desired temperature using a temperature knob. The code includes objects, links, and joints involved in the process.",
        "type": "comment"
    },
    "238": {
        "file_id": 12,
        "content": "k_1, which is the temperature knob. The robot needs to actuate it to rotate link_1 to the desired temperature.\nTask Name: Set Oven Timer\nDescription: The robot arm turns a timer knob to set cooking time for the food.\nAdditional Objects: None.\nLinks: \n- link_2: link_2 is assumed to be the knob for controlling the cooking time. The robot needs to approach link_2 to set the cooking time.\nJoints:\n- joint_2: from the articulation tree, joint_2 connects link_2. The robot needs to actuate joint_2 to rotate link_2 to the desired position, setting the oven timer.\nCan you do the same for the following object:\n\"\"\"\n# TODO: add another example where the ambiguous description is changed to be a precise description of the object. \ndef generate_task(object_category=None, object_path=None, existing_response=None, temperature_dict=None, \n                  model_dict=None, meta_path=\"generated_tasks\"):\n    # send the object articulation tree, semantics file and get task descriptions, invovled objects and joints\n    # randomly sample an object for generation. ",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation.py:94-113"
    },
    "239": {
        "file_id": 12,
        "content": "The code snippet contains a function generate_task that generates task descriptions, involved objects, and joints based on an articulation tree and semantics file. The objective is to create precise object descriptions for robot manipulation tasks. It randomly samples objects from the given categories or paths for generation.",
        "type": "comment"
    },
    "240": {
        "file_id": 12,
        "content": "    object_cetegories = list(partnet_mobility_dict.keys())\n    if object_category is None:\n        object_category = object_cetegories[np.random.randint(len(object_cetegories))]\n    if object_path is None:\n        possible_object_ids = partnet_mobility_dict[object_category]\n        object_path = possible_object_ids[np.random.randint(len(possible_object_ids))]\n    articulation_tree_path = f\"data/dataset/{object_path}/link_and_joint.txt\"\n    with open(articulation_tree_path, 'r') as f:\n        articulation_tree = f.readlines()\n    semantics = f\"data/dataset/{object_path}/semantics.txt\"\n    with open(semantics, 'r') as f:\n        semantics = f.readlines()\n    task_user_contents_filled = copy.deepcopy(task_user_contents)\n    articulation_tree_filled = \"\"\"\n```{} articulation tree\n{}\n```\"\"\".format(object_category, \"\".join(articulation_tree))\n    semantics_filled = \"\"\"\n```{} semantics\n{}\n```\"\"\".format(object_category, \"\".join(semantics))\n    task_user_contents_filled = task_user_contents_filled + articulation_tree_filled + semantics_filled",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation.py:115-139"
    },
    "241": {
        "file_id": 12,
        "content": "This code selects an object category and object ID from a predefined dictionary, reads the articulation tree and semantics for the selected object from files, and then fills in the task_user_contents with these data. This is done to generate a complete task description by combining the chosen object's information with the original task_user_contents. The code is part of a larger program that likely involves generating tasks or instructions for some kind of robot simulation or control system.",
        "type": "comment"
    },
    "242": {
        "file_id": 12,
        "content": "    if existing_response is None:\n        system = \"You are a helpful assistant.\"\n        ts = time.time()\n        time_string = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n        save_folder = \"data/{}/{}_{}_{}\".format(meta_path, object_category, object_path, time_string)\n        if not os.path.exists(save_folder + \"/gpt_response\"):\n            os.makedirs(save_folder + \"/gpt_response\")\n        save_path = \"{}/gpt_response/task_generation.json\".format(save_folder)\n        print(\"=\" * 50)\n        print(\"=\" * 20, \"generating task\", \"=\" * 20)\n        print(\"=\" * 50)\n        task_response = query(system, [task_user_contents_filled], [], save_path=save_path, debug=False, \n                              temperature=temperature_dict['task_generation'],\n                              model=model_dict['task_generation'])\n    else:\n        with open(existing_response, 'r') as f:\n            data = json.load(f)\n        task_response = data[\"res\"]\n        print(task_response)\n    ### generate task yaml config",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation.py:141-165"
    },
    "243": {
        "file_id": 12,
        "content": "This code checks if a response exists and generates or retrieves it. If no response exists, it creates a folder for the task, queries GPT-4 for a task generation, and saves the response in JSON format. If a response exists, it reads and prints the existing response.",
        "type": "comment"
    },
    "244": {
        "file_id": 12,
        "content": "    task_names, task_descriptions, additional_objects, links, joints = parse_task_response(task_response)\n    task_number = len(task_names)\n    print(\"task number: \", task_number)\n    all_config_paths = []\n    for task_idx in range(task_number):\n        if existing_response is None:\n            time.sleep(20)\n        task_name = task_names[task_idx]\n        task_description = task_descriptions[task_idx]\n        additional_object = additional_objects[task_idx]\n        involved_links = links[task_idx]\n        involved_joints = joints[task_idx]\n        config_path = build_task_given_text(object_category, task_name, task_description, additional_object, involved_links, involved_joints, \n                          articulation_tree_filled, semantics_filled, object_path, save_folder, temperature_dict, model_dict=model_dict)\n        all_config_paths.append(config_path)\n    return all_config_paths",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation.py:166-184"
    },
    "245": {
        "file_id": 12,
        "content": "This function extracts task information from a response, creates configurations for each task using the build_task_given_text function, and stores the paths of all created configurations in a list. It also handles potential delays due to rate limits or API issues with time.sleep.",
        "type": "comment"
    },
    "246": {
        "file_id": 13,
        "content": "/gpt_4/prompts/prompt_manipulation_reward_primitive.py",
        "type": "filepath"
    },
    "247": {
        "file_id": 13,
        "content": "This code allows robots to complete household tasks by assigning actions and rewards, dividing manipulation tasks, using primitive grasping, and preventing repetitions with proximity/angle differences. It also calculates task rewards, defines grasping/releasing functions, and suggests action spaces in robotics environments using Gym.",
        "type": "summary"
    },
    "248": {
        "file_id": 13,
        "content": "import copy\nfrom gpt_4.query import query\nimport os\nuser_contents = [\n\"\"\"\nA robotic arm is trying to solve some household object manipulation tasks to learn corresponding skills in a simulator.\nWe will provide with you the task description, the initial scene configurations of the task, which contains the objects in the task and certain information about them. \nYour goal is to decompose the task into executable sub-steps for the robot, and for each substep, you should either call a primitive action that the robot can execute, or design a reward function for the robot to learn, to complete the substep. \nFor each substep, you should also write a function that checks whether the substep has been successfully completed. \nCommon substeps include moving towards a location, grasping an object, and interacting with the joint of an articulated object.\nAn example task:\nTask Name: Set oven temperature\nDescription: The robotic arm will turn the knob of an oven to set a desired temperature.\nInitial config:\n```yaml",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation_reward_primitive.py:1-19"
    },
    "249": {
        "file_id": 13,
        "content": "The code provides a description of a task where a robotic arm needs to learn household object manipulation skills in a simulator. The user is asked to decompose the given tasks into executable sub-steps, assign primitive actions or reward functions for each substep, and write functions to check if each substep has been completed successfully.",
        "type": "comment"
    },
    "250": {
        "file_id": 13,
        "content": "-   use_table: false\n-   center: (1, 0, 0) # when an object is not on the table, the center specifies its location in the world coordinate. \n    lang: a freestanding oven \n    name: oven\n    on_table: false\n    path: oven.urdf\n    size: 0.85\n    type: urdf\n```\nI will also give you the articulation tree and semantics file of the articulated object in the task. Such information will be useful for writing the reward function/the primitive actions, for example, when the reward requires accessing the joint value of a joint in the articulated object, or the position of a link in the articulated object, or when the primitive needs to access a name of the object.\n```Oven articulation tree:\nlinks: \nbase\nlink_0\nlink_1\nlink_2\nlink_3\nlink_4\njoints: \njoint_name: joint_0 joint_type: continuous parent_link: link_4 child_link: link_0\njoint_name: joint_1 joint_type: continuous parent_link: link_4 child_link: link_1\njoint_name: joint_2 joint_type: continuous parent_link: link_4 child_link: link_2\njoint_name: joint_3 joint_type: continuous parent_link: link_4 child_link: link_3",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation_reward_primitive.py:20-44"
    },
    "251": {
        "file_id": 13,
        "content": "This code defines the properties of an object, likely for a robotics task. The \"oven\" is a freestanding object with its URDF path specified and a size of 0.85. Its location is in the world coordinate system when not on a table. The reward primitive may require access to joint values or link positions within the articulation tree defined for the oven.",
        "type": "comment"
    },
    "252": {
        "file_id": 13,
        "content": "joint_name: joint_4 joint_type: fixed parent_link: base child_link: link_4\n```\n```Oven semantics\nlink_0 hinge knob\nlink_1 hinge knob\nlink_2 hinge knob\nlink_3 hinge knob\nlink_4 heavy oven_body\n```\nI will also give you the links and joints of the articulated object that will be used for completing the task:\nLinks:\nlink_0: We know from the semantics that link_0 is a hinge knob. It is assumed to be the knob that controls the temperature of the oven. The robot needs to actuate this knob to set the temperature of the oven.\nJoints:\njoint_0: from the articulation tree, joint_0 connects link_0 and is a continuous joint. Therefore, the robot needs to actuate joint_0 to turn link_0, which is the knob.\nFor each substep, you should decide whether the substep can be achieved by using the provided list of primitives. If not, you should then write a reward function for the robot to learn to perform this substep. \nIf you choose to write a reward function for the substep, you should also specify the action space of the robot when learning this reward function. ",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation_reward_primitive.py:45-66"
    },
    "253": {
        "file_id": 13,
        "content": "This code defines a joint named \"joint_4\" with type \"fixed\", connecting the \"base\" parent link to the \"link_4\" child link. The articulated object's semantics outline four hinge knob links (link_0 to link_3) and one heavy oven_body link (link_4). The objective is for the robot to learn to manipulate the knob (link_0) to control the oven temperature by actuating joint_0, a continuous joint connecting link_0.",
        "type": "comment"
    },
    "254": {
        "file_id": 13,
        "content": "There are 2 options for the action space: \"delta-translation\", where the action is the delta translation of the robot end-effector, suited for local movements; and \"normalized-direct-translation\", where the action specifies the target location the robot should move to, suited for moving to a target location.\nFor each substep, you should also write a condition that checks whether the substep has been successfully completed.\nHere is a list of primitives the robot can do. The robot is equipped with a suction gripper, which makes it easy for the robot to grasp an object or a link on an object. \ngrasp_object(self, object_name): the robot arm will grasp the object specified by the argument object name.\ngrasp_object_link(self, object_name, link_name): some object like an articulated object is composed of multiple links. The robot will grasp a link with link_name on the object with object_name. \nrelease_grasp(self): the robot will release the grasped object.\nNote that all primitives will return a tu",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation_reward_primitive.py:67-74"
    },
    "255": {
        "file_id": 13,
        "content": "The code defines two action spaces: \"delta-translation\" for local movements and \"normalized-direct-translation\" for target location. It suggests adding a condition to check if each substep is successful, and lists three robot primitives: grasp_object, grasp_object_link, and release_grasp. All primitives return a tuple. The robot has a suction gripper for grasping objects or links on objects.",
        "type": "comment"
    },
    "256": {
        "file_id": 13,
        "content": "ple (rgbs, final_state) which represents the rgb images of the execution process and the final state of the execution process. \nYou should always call the primitive in the following format:\nrgbs, final_state = some_primitive_function(self, arg1, ..., argn)\nHere is a list of helper functions that you can use for designing the reward function or the success condition:\nget_position(self, object_name): get the position of center of mass of object with object_name.\nget_orientation(self, object_name): get the orientation of an object with object_name.\nget_joint_state(self, object_name, joint_name): get the joint angle value of a joint in an object.\nget_joint_limit(self, object_name, joint_name): get the lower and upper joint angle limit of a joint in an object, returned as a 2-element tuple.\nget_link_state(self, object_name, link_name): get the position of the center of mass of the link of an object.\nget_eef_pos(self): returns the position, orientation of the robot end-effector as a list.\nget_bounding",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation_reward_primitive.py:74-85"
    },
    "257": {
        "file_id": 13,
        "content": "This function is designed to accept a list of RGB images and the final state of an execution process. It must be called using the specified format. The code provides helper functions like get_position, get_orientation, get_joint_state, get_joint_limit, get_link_state, and get_eef_pos for designing reward functions or success conditions.",
        "type": "comment"
    },
    "258": {
        "file_id": 13,
        "content": "_box(self, object_name): get the axis-aligned bounding box of an object. It returns the min and max xyz coordinate of the bounding box.\nget_bounding_box_link(self, object_name, link_name): get the axis-aligned bounding box of the link of an object. It returns the min and max xyz coordinate of the bounding box.\nin_bbox(self, pos, bbox_min, bbox_max): check if pos is within the bounding box with the lowest corner at bbox_min and the highest corner at bbox_max. \ncheck_grasped(self, object_name, link_name): return true if an object or a link of the object is grasped. link_name can be none, in which case it will check whether the object is grasped.\nget_initial_pos_orient(self, obj): get the initial position and orientation of an object at the beginning of the task.\nget_initial_joint_angle(self, obj_name, joint_name): get the initial joint angle of an object at the beginning of the task.\nYou can assume that for objects, the lower joint limit corresponds to their natural state, e.g., a box is closed with the lid joint being 0, and a lever is unpushed when the joint angle is 0.",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation_reward_primitive.py:85-92"
    },
    "259": {
        "file_id": 13,
        "content": "The code provides various utility functions for manipulating objects, such as obtaining the bounding box of an object or its link, checking if a position is within a bounding box, and determining if an object or a specific link of an object is grasped. The functions also allow retrieving the initial position, orientation, and joint angle of an object at the beginning of the task.",
        "type": "comment"
    },
    "260": {
        "file_id": 13,
        "content": "For the above task \"Set oven temperature\", it can be decomposed into the following substeps, primitives, and reward functions:\nsubstep 1: grasp the temperature knob\n```primitive\n\trgbs, final_state = grasp_object_link(self, \"oven\", \"link_0\") \n    success = check_grasped(self, \"oven\", \"link_0\")\n```\nsubstep 2: turn the temperature knob to set a desired temperature\n```reward\ndef _compute_reward(self):\n    # This reward encourages the end-effector to stay near the knob to grasp it.\n    eef_pos = get_eef_pos(self)[0]\n    knob_pos = get_link_state(self, \"oven\", \"link_0\")\n    reward_near = -np.linalg.norm(eef_pos - knob_pos)\n    joint_angle = get_joint_state(self, \"oven\", \"joint_0\") \n    joint_limit_low, joint_limit_high = get_joint_limit(self, \"oven\", \"joint_0\")\n    desired_temperature = joint_limit_low + (joint_limit_high - joint_limit_low)  / 3 # We assume the target desired temperature is one third of the joint angle. It can also be 1/3, or other values between joint_limit_low and joint_limit_high. \n    #",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation_reward_primitive.py:94-115"
    },
    "261": {
        "file_id": 13,
        "content": "This code decomposes the task \"Set oven temperature\" into two substeps and provides primitives for each substep. Substep 1 is \"grasp the temperature knob,\" with a primitive that uses grasp_object_link function to achieve it. Substep 2 is \"turn the temperature knob to set a desired temperature,\" which has a reward function _compute_reward defining the reward for this action, encouraging the end-effector to stay near the knob while turning it. The desired temperature is assumed to be one third of the joint angle range.",
        "type": "comment"
    },
    "262": {
        "file_id": 13,
        "content": " The reward is the negative distance between the current joint angle and the joint angle of the desired temperature.\n    diff = np.abs(joint_angle - desired_temperature)\n    reward_joint =  -diff\n    reward = reward_near + 5 * reward_joint\n    success = diff < 0.1 * (joint_limit_high - joint_limit_low)\n    return reward, success\n```\n```action space\ndelta-translation\n```\nI will give some more examples of decomposing the task. Reply yes if you understand the goal.\n\"\"\",\n\"\"\"\nAnother example:\nTask Name: Fetch item from refrigerator\nDescription: The robotic arm will open a refrigerator door reach inside to grab an item, place it on the table, and then close the door\nInitial config:\n```yaml\n-   use_table: true \n-   center: (1.2, 0, 0)\n    lang: a common two-door refrigerator\n    name: Refrigerator\n    on_table: false \n    path: refrigerator.urdf\n    size: 1.8\n    type: urdf\n-   center: (1.2, 0, 0.5) \n    lang: a can of soda\n    name: Item\n    on_table: false \n    path: soda_can.obj\n    size: 0.2\n    type: mesh\n```\n```Refrigerator articulation tree",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation_reward_primitive.py:115-154"
    },
    "263": {
        "file_id": 13,
        "content": "The code calculates the reward for a robotic arm reaching a desired temperature by comparing the current joint angle to the desired temperature and subtracting them. The reward is then multiplied by a factor of 5 before being added to another reward variable. A success condition is also set if the difference between the two angles is less than a certain threshold. Finally, the reward and success are returned.",
        "type": "comment"
    },
    "264": {
        "file_id": 13,
        "content": "links: \nbase\nlink_0\nlink_1\nlink_2\njoints: \njoint_name: joint_0 joint_type: fixed parent_link: base child_link: link_0\njoint_name: joint_1 joint_type: revolute parent_link: link_0 child_link: link_1\njoint_name: joint_2 joint_type: revolute parent_link: link_0 child_link: link_2\n```\n```Refrigerator semantics\nlink_0 heavy refrigerator_body\nlink_1 hinge door\nlink_2 hinge door\n```\nLinks:\nlink_1: This link is one of the refrigerator doors, which the robot neesd to reach for the item inside.\nJoints:\njoint_1: This joint connects link_1, representing one of the doors. The robot needs to actuate this joint to open the door, reach for the item, and close the door.\nThis task can be decomposed as follows:\nsubstep 1: grasp the refrigerator door\n```primitive\n    rgbs, final_state = grasp_object_link(self, \"Refrigerator\", \"link_1\")  \n    success = check_grasped(self, \"Refrigerator\", \"link_1\")\n```\nsubstep 2: open the refrigerator door\n```reward\ndef _compute_reward(self):\n    # this reward encourages the end-effector to stay near door to grasp it.",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation_reward_primitive.py:155-189"
    },
    "265": {
        "file_id": 13,
        "content": "This code segment describes a robot task involving a refrigerator. The robot needs to reach for an item inside the refrigerator by grasping and opening the door. Substep 1 involves grasping the refrigerator door using the \"grasp_object_link\" primitive, while substep 2 encourages staying near the door to grasp it with the reward function \"_compute_reward\". The goal is to open the door to reach for the item inside the refrigerator.",
        "type": "comment"
    },
    "266": {
        "file_id": 13,
        "content": "    eef_pos = get_eef_pos(self)[0]\n    door_pos = get_link_state(self, \"Refrigerator\", \"link_1\")\n    reward_near = -np.linalg.norm(eef_pos - door_pos)\n    # Get the joint state of the door. We know from the semantics and the articulation tree that joint_1 connects link_1 and is the joint that controls the rotation of the door.\n    joint_angle = get_joint_state(self, \"Refrigerator\", \"joint_1\") \n    # The reward is the negative distance between the current joint angle and the joint angle when the door is fully open (upper limit).\n    joint_limit_low, joint_limit_high = get_joint_limit(self, \"Refrigerator\", \"joint_1\")\n    diff = np.abs(joint_angle - joint_limit_high)\n    reward_joint =  -diff\n    reward = reward_near + 5 * reward_joint\n    success = diff < 0.35 * (joint_limit_high - joint_limit_low) # for opening, we think 65 percent is enough\n    return reward, success\n```\n```action space\ndelta-translation\n```\nIn the last substep the robot already grasps the door, thus only local movements are needed to open it. ",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation_reward_primitive.py:190-210"
    },
    "267": {
        "file_id": 13,
        "content": "This code calculates the reward and success for opening a door. It first gets the EEF position, door position, and joint angle of the door. Then it computes the negative distance between the EEF position and door position as 'reward_near', and the difference between the current joint angle and upper limit as 'reward_joint'. The reward is the sum of these two values. If the difference in joint angle is less than 0.35 times the range of joint limits, the success is true. This code assumes that the robot has already grasped the door and only needs local movements to open it.",
        "type": "comment"
    },
    "268": {
        "file_id": 13,
        "content": "substep 3: grasp the item\n```primitive\n    rgbs, final_state = grasp_object(self, \"Item\")\n    success = check_grasped(self, \"Item\")\n```\nsubstep 4: move the item out of the refrigerator\n```reward\ndef _compute_reward(self):\n    # Get the current item position\n    item_pos = get_position(self, \"Item\")\n    # The first reward encourages the end-effector to stay near the item\n    eef_pos = get_eef_pos(self)[0]\n    reward_near = -np.linalg.norm(eef_pos - item_pos)\n    # The reward is to encourage the robot to grasp the item and move the item to be on the table. \n    # The goal is not to just move the soda can to be at a random location out of the refrigerator. Instead, we need to place it somewhere on the table. \n    # This is important for moving an object out of a container style of task.\n    table_bbox_low, table_bbox_high = get_bounding_box(self, \"init_table\") # the table is referred to as \"init_table\" in the simulator. \n    table_bbox_range = table_bbox_high - table_bbox_low\n    # target location is to put the item at a random location on the table",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation_reward_primitive.py:212-234"
    },
    "269": {
        "file_id": 13,
        "content": "This code is related to a robotics task where the goal is to grasp an item, move it out of the refrigerator, and place it on the table. It consists of four substeps: 1) approach the refrigerator, 2) open the door, 3) grasp the item, and 4) move the item out of the refrigerator. The code calculates rewards to encourage the robot to stay near the item, grasp it, and place it on the table rather than moving it randomly out of the refrigerator. It uses position data from the end-effector (robot's hand), the item, and the table in the simulation.",
        "type": "comment"
    },
    "270": {
        "file_id": 13,
        "content": "    target_location = np.zeros(3)\n    target_location[0] = table_bbox_low[0] + 0.2 * table_bbox_range[0] # 0.2 is a random chosen number, any number in [0, 1] should work\n    target_location[1] = table_bbox_low[1] + 0.3 * table_bbox_range[1] # 0.3 is a random chosen number, any number in [0, 1] should work\n    target_location[2] = table_bbox_high[2] + 0.05 # target height is slightly above the table\n    diff = np.linalg.norm(item_pos - target_location)\n    reward_distance = -diff\n    reward = reward_near + 5 * reward_distance\n    success = diff < 0.06\n    return reward, success\n```\n```action space\nnormalized-direct-translation\n```\nSince this substep requires moving the item to a target location, we use the normalized-direct-translation.\nsubstep 5: grasp the refrigerator door again\n```primitive\n    rgbs, final_state = grasp_object_link(self, \"Refrigerator\", \"link_1\")\n    success = check_grasped(self, \"Refrigerator\", \"link_1\") \n```\nsubstep 6: close the refrigerator door\n```reward\ndef _compute_reward(self):\n    # this reward encourages the end-effector to stay near door",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation_reward_primitive.py:235-263"
    },
    "271": {
        "file_id": 13,
        "content": "This code calculates the reward and success for reaching a target location (substep 1) by generating a random number in [0, 1] to adjust coordinates of the target. It also handles grasping the refrigerator door again (substep 5), encouraging the end-effector to stay near the door during closing the refrigerator door (substep 6).",
        "type": "comment"
    },
    "272": {
        "file_id": 13,
        "content": "    eef_pos = get_eef_pos(self)[0]\n    door_pos = get_link_state(self, \"Refrigerator\", \"link_1\")\n    reward_near = -np.linalg.norm(eef_pos - door_pos)\n    # Get the joint state of the door. \n    joint_angle = get_joint_state(self, \"Refrigerator\", \"joint_1\") \n    # The reward encourages the robot to make joint angle of the door to be the lower limit to clost it.\n    joint_limit_low, joint_limit_high = get_joint_limit(self, \"Refrigerator\", \"joint_1\")\n    diff = np.abs(joint_limit_low - joint_angle)\n    reward_joint =  -diff\n    reward = reward_near + 5 * reward_joint\n    success = diff < 0.1 * (joint_limit_high - joint_limit_low) # for closing, we think 10 percent is enough     \n    return reward, success\n```\n```action space\ndelta-translation\n```\nI will provide more examples in the following messages. Please reply yes if you understand the goal.\n\"\"\",\n\"\"\"\nHere is another example:\nTask Name:  Put a toy car inside a box\nDescription: The robotic arm will open a box, grasp the toy car and put it inside the box.\nInitial config:",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation_reward_primitive.py:264-294"
    },
    "273": {
        "file_id": 13,
        "content": "The code calculates a reward for the robot to encourage it to close a door by considering both the distance between the end-effector position and the door's position, as well as the joint angle of the door. The closer the door is to being closed and the lower the joint angle is, the higher the reward will be. A success threshold is also defined to check if the robot has successfully closed the door by verifying that the difference between the joint angle and its limit is below a certain threshold (10% of the range between the joint limits). The action space is delta-translation, indicating the robotic arm will move in small increments towards the target position.",
        "type": "comment"
    },
    "274": {
        "file_id": 13,
        "content": "```yaml\n-  use_table: True \n-   center: (0.2, 0.3, 0)\n    on_table: True\n    lang: a box\n    name: box\n    size: 0.25\n    type: urdf\n-   center: (0.1, 0.6, 0)\n    on_table: True\n    lang: a toy car\n    name: toy_car\n    size: 0.1\n    type: mesh\n```\n```box articulation tree\nlinks: \nbase\nlink_0\nlink_1\nlink_2\njoints: \njoint_name: joint_0 joint_type: revolute parent_link: link_2 child_link: link_0\njoint_name: joint_1 joint_type: revolute parent_link: link_2 child_link: link_1\njoint_name: joint_2 joint_type: fixed parent_link: base child_link: link_2\n```\n```box semantics\nlink_0 hinge rotation_lid\nlink_1 hinge rotation_lid\nlink_2 free box_body\n```\nLinks:\nlink_0: To fully open the box, the robot needs to open both box lids. We know from the semantics that link_0 is one of the lids.\nlink_1: To fully open the box, the robot needs to open both box lids. We know from the semantics that link_1 is another lid.\nJoints:\njoint_0: from the articulation tree, joint_0 connects link_0 and is a hinge joint. Thus, the robot needs to actuate joint_0 to open link_0, which is the lid of the box.",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation_reward_primitive.py:295-334"
    },
    "275": {
        "file_id": 13,
        "content": "This code defines a box with articulation tree and semantics, consisting of links link_0, link_1, and link_2. Link_0 and link_1 are both lids that need to be opened for the box to be fully open, while joint_0 is a hinge joint connecting link_0 and requiring actuation to open it.",
        "type": "comment"
    },
    "276": {
        "file_id": 13,
        "content": "joint_1: from the articulation tree, joint_1 connects link_1 and is a hinge joint. Thus, the robot needs to actuate joint_1 to open link_1, which is the lid of the box.\nThis task can be decomposed as follows:\nsubstep 1: grasp the first lid of the box\n```primitive\n\t# The semantics shows that link_0 and link_1 are the lid links. \n\trgbs, final_state = grasp_object_link(self, \"box\", \"link_0\")  \n    success = check_grasped(self, \"box\", \"link_0\")\n```\nsubstep 2: open the first lid of the box\n```reward\ndef _compute_reward(self):\n    # This reward encourages the end-effector to stay near the lid to grasp it.\n    eef_pos = get_eef_pos(self)[0]\n    lid_pos = get_link_state(self, \"box\", \"link_0\")\n    reward_near = -np.linalg.norm(eef_pos - lid_pos)\n    # Get the joint state of the first lid. The semantics and the articulation tree show that joint_0 connects link_0 and is the joint that controls the rotation of the first lid link_0.\n    joint_angle = get_joint_state(self, \"box\", \"joint_0\") \n    # The reward is the ",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation_reward_primitive.py:335-356"
    },
    "277": {
        "file_id": 13,
        "content": "This code decomposes a task into two substeps: grasping the first lid of the box and then opening it. The reward function encourages the end-effector to stay near the lid to grasp it, and calculates the joint state of the first lid.",
        "type": "comment"
    },
    "278": {
        "file_id": 13,
        "content": "negative distance between the current joint angle and the joint angle when the lid is fully open (upper limit).\n    joint_limit_low, joint_limit_high = get_joint_limit(self, \"box\", \"joint_0\")\n    diff = np.abs(joint_angle - joint_limit_high)\n    reward_joint =  -diff\n    reward = reward_near + 5 * reward_joint\n    success = diff < 0.35 * (joint_limit_high - joint_limit_low)\n    return reward, success\n```\n```action space\ndelta-translation\n```\nsubstep 3: grasp the second lid of the box\n```primitive\n\t# We know from the semantics that link_0 and link_1 are the lid links. \n\trgbs, final_state = grasp_object_link(self, \"box\", \"link_1\")  \n    success = check_grasped(self, \"box\", \"link_1\")\n```\nsubstep 4: open the second lid of the box\n```reward\ndef _compute_reward(self):\n    # This reward encourages the end-effector to stay near the lid to grasp it.\n    eef_pos = get_eef_pos(self)[0]\n    lid_pos = get_link_state(self, \"box\", \"link_1\")\n    reward_near = -np.linalg.norm(eef_pos - lid_pos)\n    # Get the joint state of the second lid. ",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation_reward_primitive.py:356-386"
    },
    "279": {
        "file_id": 13,
        "content": "The code calculates the negative distance between the current joint angle and the upper limit (when the lid is fully open) to compute a reward for opening the second lid of the box. It also checks if the difference is less than 0.35 times the range of the joint limits, indicating success in opening the lid. Additionally, it encourages the end-effector to stay near the lid for grasping purposes.",
        "type": "comment"
    },
    "280": {
        "file_id": 13,
        "content": "    joint_angle = get_joint_state(self, \"box\", \"joint_1\") \n    # The reward is the negative distance between the current joint angle and the joint angle when the lid is fully open (upper limit).\n    joint_limit_low, joint_limit_high = get_joint_limit(self, \"box\", \"joint_1\")\n    diff = np.abs(joint_angle - joint_limit_high)\n    reward_joint =  -diff\n    reward = reward_near + 5 * reward_joint\n    success = diff < 0.35 * (joint_limit_high - joint_limit_low)\n    return reward, success\n```\n```action space\ndelta-translation\n```\nsubstep 5: grasp the toy car\n```primitive\n\trgbs, final_state = grasp_object(self, \"toy_car\")\n    success = check_grasped(self, \"toy_car\")\n```\nsubstep 6: put the toy car into the box\n```reward\ndef _compute_reward(self):\n    # This reward encourages the end-effector to stay near the car to grasp it.\n    car_position = get_position(self, \"toy_car\")\n    eef_pos = get_eef_pos(self)[0]\n    reward_near = -np.linalg.norm(eef_pos - car_position)\n    # main reward is 1 if the car is inside the box. From the semantics we know that link2 is the box body",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation_reward_primitive.py:387-416"
    },
    "281": {
        "file_id": 13,
        "content": "This code calculates the reward for a robot manipulation task involving a box and a toy car. The reward is determined by the distance between the current joint angle of the box and its upper limit, as well as the proximity of the robot's end-effector to the toy car. This code also checks if the lid of the box is fully open and calculates the success of grasping the toy car and placing it inside the box.",
        "type": "comment"
    },
    "282": {
        "file_id": 13,
        "content": "    box_bbox_low, box_bbox_high = get_bounding_box_link(self, \"box\", \"link_2\")\n    reward_in = int(in_bbox(self, car_position, box_bbox_low, box_bbox_high))\n    # another reward is to encourage the robot to move the car to be near the box\n    reward_reaching = - np.linalg.norm(car_position - (box_bbox_low + box_bbox_high) / 2)\n    # The task is considered to be successful if the car is inside the box bounding box\n    success = reward_in\n    # We give more weight to reward_in, which is the major goal of the task.\n    reward = 5 * reward_in + reward_reaching + reward_near\n    return reward, success\n```\n```action space\nnormalized-direct-translation\n```\nSince this substep requires moving the item to a target location, we use the normalized-direct-translation.\nPlease decompose the following task into substeps. For each substep, write a primitive/a reward function, write the success checking function, and the action space if the reward is used. \nThe primitives you can call:\ngrasp_object(self, object_name): the robot arm will grasp the object specified by the argument object name.",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation_reward_primitive.py:417-439"
    },
    "283": {
        "file_id": 13,
        "content": "The code calculates two rewards: reward_in and reward_reaching. It checks if the car is inside the box bounding box (success) and combines these rewards to form the final reward. The action space used for this task is normalized-direct-translation, as it involves moving an item to a target location.",
        "type": "comment"
    },
    "284": {
        "file_id": 13,
        "content": "grasp_object_link(self, object_name, link_name): some object like an articulated object is composed of multiple links. The robot will grasp a link with link_name on the object with object_name. \nrelease_grasp(self): the robot will release the grasped object.\nNote that all primitives will return a tuple (rgbs, final_state) which represents the rgb images of the execution process and the final state of the execution process. \nYou should always call the primitive in the following format:\nrgbs, final_state = some_primitive_function(self, arg1, ..., argn)\nThe APIs you can use for writing the reward function/success checking function:\nget_position(self, object_name): get the position of center of mass of object with object_name.\nget_orientation(self, object_name): get the orientation of an object with object_name.\nget_joint_state(self, object_name, joint_name): get the joint angle value of a joint in an object.\nget_joint_limit(self, object_name, joint_name): get the lower and upper joint angle limit of a joint in an object, returned as a 2-element tuple.",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation_reward_primitive.py:440-450"
    },
    "285": {
        "file_id": 13,
        "content": "This code defines two primitive functions: \"grasp_object_link\" and \"release_grasp\". These functions allow a robot to grasp an object link and then release the grasped object. The primitives return RGB images of the execution process and the final state. The provided API includes functions for obtaining position, orientation, joint angle values, and joint angle limits for object manipulation.",
        "type": "comment"
    },
    "286": {
        "file_id": 13,
        "content": "get_link_state(self, object_name, link_name): get the position of the center of mass of the link of an object.\nget_eef_pos(self): returns the position, orientation of the robot end-effector as a list.\nget_bounding_box(self, object_name): get the axis-aligned bounding box of an object. It returns the min and max xyz coordinate of the bounding box.\nget_bounding_box_link(self, object_name, link_name): get the axis-aligned bounding box of the link of an object. It returns the min and max xyz coordinate of the bounding box.\nin_bbox(self, pos, bbox_min, bbox_max): check if pos is within the bounding box with the lowest corner at bbox_min and the highest corner at bbox_max. \ncheck_grasped(self, object_name, link_name): return true if an object or a link of the object is grasped. link_name can be none, in which case it will check whether the object is grasped.\nget_initial_pos_orient(self, obj): get the initial position and orientation of an object at the beginning of the task.\nget_initial_joint_angle",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation_reward_primitive.py:451-458"
    },
    "287": {
        "file_id": 13,
        "content": "This code contains various methods for object manipulation, such as getting the position of an object's center of mass, obtaining the end-effector position and orientation, retrieving bounding box information for objects or their links, checking if a given position is within a bounding box, determining if an object or link is grasped, and retrieving initial position and orientation at the start of a task.",
        "type": "comment"
    },
    "288": {
        "file_id": 13,
        "content": "(self, obj_name, joint_name): get the initial joint angle of an object at the beginning of the task.\nThe action space you can use for learning with the reward: delta-translation is better suited for small movements, and normalized-direct-translation is better suited for directly specifying the target location of the robot end-effector. \nYou can assume that for objects, the lower joint limit corresponds to their natural state, e.g., a box is closed with the lid joint being 0, and a lever is unpushed when the joint angle is 0.\n\"\"\"\n]\nassistant_contents = [\n\"\"\"\nYes, I understand the goal. Please proceed with the next example.\n\"\"\",\n\"\"\"\nYes, I understand the goal. Please proceed with the next example.\n\"\"\"\n]\nreward_file_header1 = \"\"\"\nfrom manipulation.sim import SimpleEnv\nimport numpy as np\nfrom manipulation.gpt_reward_api import *\nfrom manipulation.gpt_primitive_api import *\nimport gym\nclass {}(SimpleEnv):\n\"\"\"\nreward_file_header2 = \"\"\"\n    def __init__(self, task_name, *args, **kwargs):\n        super().__init__(*args, **kwargs)",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation_reward_primitive.py:458-489"
    },
    "289": {
        "file_id": 13,
        "content": "This code is part of a robotics learning environment, where it defines a class that retrieves the initial joint angle of an object at the beginning of a task. It suggests using different action spaces for learning with rewards depending on the type of movement being made. The code also assumes that objects have natural states corresponding to their lower joint limits.",
        "type": "comment"
    },
    "290": {
        "file_id": 13,
        "content": "        self.task_name = task_name\n        self.detected_position = {}\n\"\"\"\nreward_file_end = \"\"\"\ngym.register(\n    id='gym-{}-v0',\n    entry_point={},\n)\n\"\"\"\nprimitive_file_header1 = \"\"\"\nfrom manipulation.sim import SimpleEnv\nimport numpy as np\nfrom manipulation.gpt_primitive_api import *\nfrom manipulation.gpt_reward_api import *\nimport gym\nclass {}(SimpleEnv):\n\"\"\"\nprimitive_file_header2 = \"\"\"\n    def __init__(self, task_name, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.task_name = task_name\n        self.detected_position = {}\n    def execute(self):\n\"\"\"\nprimitive_file_end = \"\"\"\n        return rgbs, final_state, success\ngym.register(\n    id='{}-v0',\n    entry_point={},\n)\n\"\"\"\ndef decompose_and_generate_reward_or_primitive(task_name, task_description, initial_config, articulation_tree, semantics, \n                              involved_links, involved_joints, object_id, yaml_config_path, save_path, \n                              temperature=0.4, model='gpt-4'):\n    query_task = \"\"\"\nTask name: {}\nDescription: {}",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation_reward_primitive.py:490-535"
    },
    "291": {
        "file_id": 13,
        "content": "This code is generating a reward or primitive function for a robotics task. It uses Gym, a reinforcement learning library, to define and register an environment with the specified task name. The code creates classes for both reward and primitive functions, and includes initialization and execution methods. The `decompose_and_generate_reward_or_primitive` function takes various inputs such as task description, initial configuration, articulation tree, semantics, involved links, joints, object ID, and more to generate the appropriate environment for the robotics task.",
        "type": "comment"
    },
    "292": {
        "file_id": 13,
        "content": "Initial config:\n```yaml\n{}\n```\n{}\n{}\nLinks:\n{}\nJoints:\n{}\n\"\"\".format(task_name, task_description, initial_config, articulation_tree, semantics, involved_links, involved_joints)\n    filled_user_contents = copy.deepcopy(user_contents)\n    filled_user_contents[-1] = filled_user_contents[-1] + query_task\n    system = \"You are a helpful assistant.\"\n    reward_response = query(system, filled_user_contents, assistant_contents, save_path=save_path, debug=False, \n                            temperature=temperature, model=model)\n    res = reward_response.split(\"\\n\")\n    substeps = []\n    substep_types = []\n    reward_or_primitives = []\n    action_spaces = []\n    num_lines = len(res)\n    for l_idx, line in enumerate(res):\n        line = line.lower()\n        if line.startswith(\"substep\"):\n            substep_name = line.split(\":\")[1]\n            substeps.append(substep_name)\n            py_start_idx, py_end_idx = l_idx, l_idx\n            for l_idx_2 in range(l_idx + 1, num_lines):\n                ### this is a reward\n                if res[l_idx_2].lower().startswith(\"```reward\"):",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation_reward_primitive.py:536-574"
    },
    "293": {
        "file_id": 13,
        "content": "This code appears to be involved in task manipulation and reward primitives generation. It begins by defining an initial configuration, links, joints, and other relevant details. The code then performs some operations on the user contents and system inputs. Afterwards, it queries the GPT model for a response, which is likely related to the task manipulation or reward primitive generation. The response is split into separate lines and processed further to identify substep names, parse rewards, and generate action spaces based on the output from the GPT model. The code appears to be part of a larger system that involves a task description, configuration, articulation tree, semantics, involved links, and involved joints.",
        "type": "comment"
    },
    "294": {
        "file_id": 13,
        "content": "                    substep_types.append(\"reward\")\n                    py_start_idx = l_idx_2 + 1\n                    for l_idx_3 in range(l_idx_2 + 1, num_lines):\n                        if \"```\" in res[l_idx_3]:\n                            py_end_idx = l_idx_3\n                            break\n                if res[l_idx_2].lower().startswith(\"```primitive\"):\n                    substep_types.append(\"primitive\")\n                    action_spaces.append(\"None\")\n                    py_start_idx = l_idx_2 + 1\n                    for l_idx_3 in range(l_idx_2 + 1, num_lines):\n                        if \"```\" in res[l_idx_3]:\n                            py_end_idx = l_idx_3\n                            break\n                    break\n                if res[l_idx_2].lower().startswith(\"```action space\"):\n                    action_space = res[l_idx_2 + 1]\n                    action_spaces.append(action_space)\n                    break\n            reward_or_primitive_lines = res[py_start_idx:py_end_idx]\n            reward_or_primitive_lines = [line.lstrip() for line in reward_or_primitive_lines]",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation_reward_primitive.py:575-598"
    },
    "295": {
        "file_id": 13,
        "content": "This code identifies different types of sub-steps within a prompt: \"reward\", \"primitive\", and \"action space\". It extracts lines between \"```\" markdown fences and checks if the line starts with certain keywords to determine the sub-step type. The extracted lines are then processed further based on their type.",
        "type": "comment"
    },
    "296": {
        "file_id": 13,
        "content": "            if substep_types[-1] == 'reward':\n                reward_or_primitive_lines[0] = \"    \" + reward_or_primitive_lines[0]\n                for idx in range(1, len(reward_or_primitive_lines)):\n                    reward_or_primitive_lines[idx] = \"        \" + reward_or_primitive_lines[idx]\n            else:\n                for idx in range(0, len(reward_or_primitive_lines)):\n                    reward_or_primitive_lines[idx] = \"        \" + reward_or_primitive_lines[idx]\n            reward_or_primitive = \"\\n\".join(reward_or_primitive_lines) + \"\\n\"\n            reward_or_primitives.append(reward_or_primitive)\n    task_name = task_name.replace(\" \", \"_\")\n    parent_folder = os.path.dirname(os.path.dirname(save_path))\n    task_save_path = os.path.join(parent_folder, \"task_{}\".format(task_name))\n    if not os.path.exists(task_save_path):\n        os.makedirs(task_save_path)\n    print(\"substep: \", substeps)\n    print(\"substep types: \", substep_types)\n    print(\"reward or primitives: \", reward_or_primitives)",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation_reward_primitive.py:599-618"
    },
    "297": {
        "file_id": 13,
        "content": "This code is formatting a list of reward or primitive lines based on the last substep type. If the last substep type is 'reward', it indents the first line and leaves others unchanged. Otherwise, it indents all lines. Then it joins the formatted lines, appends to the reward_or_primitives list, saves task information, and prints relevant details for debugging or logging purposes.",
        "type": "comment"
    },
    "298": {
        "file_id": 13,
        "content": "    print(\"action spaces: \", action_spaces)\n    with open(os.path.join(task_save_path, \"substeps.txt\"), \"w\") as f:\n        f.write(\"\\n\".join(substeps))\n    with open(os.path.join(task_save_path, \"substep_types.txt\"), \"w\") as f:\n        f.write(\"\\n\".join(substep_types))\n    with open(os.path.join(task_save_path, \"action_spaces.txt\"), \"w\") as f:\n        f.write(\"\\n\".join(action_spaces))\n    with open(os.path.join(task_save_path, \"config_path.txt\"), \"w\") as f:\n        f.write(yaml_config_path)\n    for idx, (substep, type, reward_or_primitive) in enumerate(zip(substeps, substep_types, reward_or_primitives)):\n        substep = substep.lstrip().replace(\" \", \"_\")\n        substep = substep.replace(\"'\", \"\")\n        file_name = os.path.join(task_save_path, f\"{substep}.py\")\n        if type == 'reward':\n            header = reward_file_header1.format(substep)\n            end = reward_file_end.format(substep, substep)\n            file_content =  header + reward_file_header2 + reward_or_primitive + end\n            with open(file_name, \"w\") as f:",
        "type": "code",
        "location": "/gpt_4/prompts/prompt_manipulation_reward_primitive.py:619-639"
    },
    "299": {
        "file_id": 13,
        "content": "Writes the action spaces, substeps, substep types, and configuration path to separate files. Then, for each reward or primitive in the provided list, it creates a file with the corresponding header and footer formatted by the substep name and content, and writes the reward/primitive to the file.",
        "type": "comment"
    }
}