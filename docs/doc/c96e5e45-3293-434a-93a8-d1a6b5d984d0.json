{
    "summary": "This code utilizes Google Bard API and defines functions for image verification tasks, downloads images, generates descriptions, checks GPT responses containing \"yes\", and verifies object-action pairs.",
    "details": [
        {
            "comment": "This code imports necessary libraries, loads a pre-trained model, and sets up an API session with Google Bard. It defines a function \"bard_verify\" which takes an image as input, and utilizes the Bard API for verification tasks using the provided token and session configuration. The model is loaded on the GPU if available; otherwise, it uses the CPU.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/bard_verify.py\":0-26",
            "content": "import os\nimport requests\nimport objaverse\nfrom PIL import Image\nfrom gpt_4.query import query\nimport torch\nimport numpy as np\nfrom lavis.models import load_model_and_preprocess\nimport os\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel, vis_processors, _ = load_model_and_preprocess(name=\"blip2_t5\", model_type=\"pretrain_flant5xl\", is_eval=True, device=device)\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\ndef bard_verify(image):\n    from bardapi import Bard\n    token = \"\" # replace with your token\n    session = requests.Session()\n    session.headers = {\n                \"Host\": \"bard.google.com\",\n                \"X-Same-Domain\": \"1\",\n                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\",\n                \"Content-Type\": \"application/x-www-form-urlencoded;charset=UTF-8\",\n                \"Origin\": \"https://bard.google.com\",\n                \"Referer\": \"https://bard.google.com/\",\n            }\n    session.cookies.set(\"__Secure-1PSID\", token) "
        },
        {
            "comment": "This code snippet includes functions for querying Bard, generating Blip2 captions, and verifying an object's presence in Objaverse. It also preprocesses images and finds the image with the largest size from a list of thumbnail URLs.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/bard_verify.py\":27-54",
            "content": "    bard = Bard(token=token, session=session)\n    query_string = \"\"\"I will show you an image. Please describe the content of the image. \"\"\"\n    print(\"===================== querying bard: ==========================\")\n    print(query_string)\n    res = bard.ask_about_image(query_string, image)\n    description = res['content']\n    print(\"bard description: \", description)\n    print(\"===============\")\n    return description\ndef blip2_caption(image):\n    # preprocess the image\n    # vis_processors stores image transforms for \"train\" and \"eval\" (validation / testing / inference)\n    image = vis_processors[\"eval\"](image).unsqueeze(0).to(device)\n    # generate caption\n    res = model.generate({\"image\": image})\n    return res[0]\ndef verify_objaverse_object(object_name, uid, task_name=None, task_description=None, use_bard=False, use_blip2=True):\n    annotations = objaverse.load_annotations([uid])[uid]\n    thumbnail_urls = annotations['thumbnails'][\"images\"]\n    max_size = -1000\n    max_url = -1\n    for dict in thumbnail_urls:"
        },
        {
            "comment": "Code snippet downloads the image from a URL, checks if it exists and is not empty, generates descriptions using BARD and BLIP-2 models, and adds the image to the data folder. It also handles exceptions and creates directories if needed.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/bard_verify.py\":55-86",
            "content": "        width = dict[\"width\"]\n        if width > max_size:\n            max_size = width\n            max_url = dict[\"url\"]\n    if max_url == -1: # TODO: in this case, we should render the object using blender to get the image.\n        return False\n    # download the image from the url\n    try: \n        raw_image = Image.open(requests.get(max_url, stream=True).raw).convert('RGB')\n    except:\n        return False\n    if not os.path.exists('objaverse_utils/data/images'):\n        os.makedirs('objaverse_utils/data/images')\n    raw_image.save(\"objaverse_utils/data/images/{}.jpeg\".format(uid))\n    bard_image = open(\"objaverse_utils/data/images/{}.jpeg\".format(uid), \"rb\").read()\n    descriptions = []\n    if use_bard:\n        bard_description = bard_verify(bard_image)\n        descriptions.append(bard_description)\n    if use_blip2:\n        blip2_description = blip2_caption(raw_image)\n        descriptions.append(blip2_description)\n    gpt_results = []\n    for description in descriptions:\n        if description:\n            system = \"You are a helpful assistant.\""
        },
        {
            "comment": "This code is constructing a query string that describes the robot's task, object, and language annotation for retrieval from a database. The code also checks if a 'data/debug' directory exists and if not, it continues with the process.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/bard_verify.py\":87-102",
            "content": "            query_string = \"\"\"\n            A robotic arm is trying to solve a task to learn a manipulation skill in a simulator.\n        We are trying to find the best objects to load into the simulator to build this task for the robot to learn the skill.\n        The task the robot is trying to learn is: {}. \n        A more detailed description of the task is: {}.\n        As noted, to build the task in the simulator, we need to find this object: {}.\n        We are retrieving the object from an existing database, which provides some language annotations for the object.\n        With the given lanugage annotation, please think if the object can be used in the simulator as {} for learning the task {}.\n        This is the language annotation:\n        {}\n        Please reply first with your reasoning, and then a single line with \"**yes**\" or \"**no**\" to indicate whether this object can be used.\n        \"\"\".format(task_name, task_description, object_name, object_name, task_name, description)\n            if not os.path.exists('data/debug'):"
        },
        {
            "comment": "This code checks if the GPT response contains \"yes\" and stores the result in a list. It then returns True if all results are \"True\", indicating successful verification for each object-action pair. The main function uses the verify_object_action function with specific parameters.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/bard_verify.py\":103-120",
            "content": "                os.makedirs('data/debug')\n            res = query(system, [query_string], [], save_path='data/debug/verify.json', temperature=0)\n            responses = res.split(\"\\n\")\n            useable = False\n            for l_idx, line in enumerate(responses):\n                if \"yes\" in line.lower():\n                    useable = True\n                    break\n            gpt_results.append(useable)\n    return np.alltrue(gpt_results)\nif __name__ == \"__main__\":\n    uid = \"adbd797050f5429daee730a3aad04ee3\"\n    verify_objaverse_object(\"a hamburger\", uid, \"Heat up a hamburger in microwave\", \"The robot arm places a hamburger inside the microwave, closes the door, and sets the microwave timer to heat the soup for an appropriate amount of time.\")"
        }
    ]
}