{
    "summary": "This code generates YAML configurations for Franka Panda robotic arm scenes, uses URDF to model refrigerators, parses responses, and saves relevant information as JSON for spatial relationship tasks in RoboGen.",
    "details": [
        {
            "comment": "Code imports necessary libraries and defines a yaml configuration format for describing an initial scene configuration for a given task, specifically for a mobile Franka panda robotic arm. The configuration includes information about the presence of a table, its location, and other relevant details to build the task in a simulator.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/utils.py\":0-14",
            "content": "import copy\nimport os\nimport yaml\nfrom gpt_4.prompts.prompt_manipulation_reward_primitive import decompose_and_generate_reward_or_primitive\nfrom gpt_4.prompts.prompt_set_joint_angle import query_joint_angle\nfrom gpt_4.prompts.prompt_spatial_relationship import query_spatial_relationship\nfrom gpt_4.query import query\nfrom gpt_4.adjust_size import adjust_size_v2\ntask_yaml_config_prompt = \"\"\"\nI need you to describe the initial scene configuration for a given task in the following format, using a yaml file. This yaml file will help build the task in a simulator. The task is for a mobile Franka panda robotic arm to learn a manipulation skill in the simulator. The Franka panda arm is mounted on a floor, at location (1, 1, 0). It can move freely on the floor. The z axis is the gravity axis. \nThe format is as follows:\n```yaml \n- use_table: whether the task requires using a table. This should be decided based on common sense. If a table is used, its location will be fixed at (0, 0, 0). The height of the ta"
        },
        {
            "comment": "For each object in the task, specify its type as 'mesh', provide a name for referencing in the simulator, describe its size using one scale value, define the mesh's language description, input the object's path, and indicate if it should be placed on a table based on common sense and task requirements.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/utils.py\":14-21",
            "content": "ble will be 0.6m. Usually, if the objects invovled in the task are usually placed on a table (not directly on the ground), then the task requires using a table.\n# for each object involved in the task, we need to specify the following fields for it.\n- type: mesh\n  name: name of the object, so it can be referred to in the simulator\n  size: describe the scale of the object mesh using 1 number in meters. The scale should match real everyday objects. E.g., an apple is of scale 0.08m. You can think of the scale to be the longest dimension of the object. \n  lang: this should be a language description of the mesh. The language should be a concise description of the obejct, such that the language description can be used to search an existing database of objects to find the object.\n  path: this can be a string showing the path to the mesh of the object. \n  on_table: whether the object needs to be placed on the table (if there is a table needed for the task). This should be based on common sense and the requirement of the task. E.g., a microwave is usually placed on the table."
        },
        {
            "comment": "The code describes the location and movability of objects within a task context. The center represents the object's position in either world or table coordinates, depending on task requirements. The 'movable' flag determines if the object can be moved by the robot based on task specifications.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/utils.py\":22-26",
            "content": "  center: the location of the object center. If there isn't a table needed for the task or the object does not need to be on the table, this center should be expressed in the world coordinate system. If there is a table in the task and the object needs to be placed on the table, this center should be expressed in terms of the table coordinate, where (0, 0, 0) is the lower corner of the table, and (1, 1, 1) is the higher corner of the table. In either case, you should try to specify a location such that there is no collision between objects.\n  movable: if the object is movable or not in the simulator due to robot actions. This option should be falsed for most tasks; it should be true only if the task specifically requires the robot to move the object. This value can also be missing, which means the object is not movable.\n```\nAn example input includes the task names, task descriptions, and objects involved in the task. I will also provide with you the articulation tree and semantics of the articulated object. "
        },
        {
            "comment": "This code generates a yaml configuration for the task based on the provided task name, description, and objects involved. It replaces generic/placeholder objects with concrete examples in the lang field.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/utils.py\":27-49",
            "content": "This can be useful for knowing what parts are already in the articulated object, and thus you do not need to repeat those parts as separate objects in the yaml file.\nYour task includes two parts:\n1. Output the yaml configuration of the task.\n2. Sometimes, the task description / objects involved will refer to generic/placeholder objects, e.g., to place an \"item\" into the drawer, and to heat \"food\" in the microwave. In the generated yaml config, you should change these placeholder objects to be concrete objects in the lang field, e.g., change \"item\" to be a toy or a pencil, and \"food\" to be a hamburger, a bowl of soup, etc. \nExample input:\nTask Name: Insert Bread Slice \nDescription: The robotic arm will insert a bread slice into the toaster.\nObjects involved: Toaster, bread slice. Only the objects specified here should be included in the yaml file.\n```Toaster articulation tree\nlinks: \nbase\nlink_0\nlink_1\nlink_2\nlink_3\nlink_4\nlink_5\njoints: \njoint_name: joint_0 joint_type: continuous parent_link: link_5 child_link: link_0"
        },
        {
            "comment": "This code defines the joint structure and semantics of a toaster, specifying its parts and their connections. The output provides information on how to incorporate this toaster into a scene or simulation, including table placement, positioning, and size.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/utils.py\":50-74",
            "content": "joint_name: joint_1 joint_type: prismatic parent_link: link_5 child_link: link_1\njoint_name: joint_2 joint_type: prismatic parent_link: link_5 child_link: link_2\njoint_name: joint_3 joint_type: prismatic parent_link: link_5 child_link: link_3\njoint_name: joint_4 joint_type: prismatic parent_link: link_5 child_link: link_4\njoint_name: joint_5 joint_type: fixed parent_link: base child_link: link_5\n```\n```Toaster semantics\nlink_0 hinge knob\nlink_1 slider slider\nlink_2 slider button\nlink_3 slider button\nlink_4 slider button\nlink_5 free toaster_body\n```\nAn example output:\n```yaml\n- use_table: True ### Toaster and bread are usually put on a table. \n- type: mesh\n  name: \"Toaster\"\n  on_table: True # Toasters are usually put on a table.\n  center: (0.1, 0.1, 0) # Remember that when an object is placed on the table, the center is expressed in the table coordinate, where (0, 0, 0) is the lower corner and (1, 1, 1) is the higher corner of the table. Here we put the toaster near the lower corner of the table.  \n  size: 0.35 # the size of a toaster is roughly 0.35m"
        },
        {
            "comment": "Code represents an object, a toaster and its components. It describes the language (e.g., \"a common toaster\"), path of toaster file (e.g., \"toaster.urdf\"), type of component (\"bread slice\"), name of component (\"bread slice\"), whether it's on the table or not (True), center coordinates in table coordinates, size of bread slice, language of component (e.g., \"a slice of bread\"), and path of component file (e.g., \"bread_slice.obj\").",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/utils.py\":75-104",
            "content": "  lang: \"a common toaster\"\n  path: \"toaster.urdf\"\n- type: mesh\n  name: \"bread slice\"\n  on_table: True # Bread is usually placed on the table as well. \n  center: (0.8, 0.7, 0) # Remember that when an object is placed on the table, the center is expressed in the table coordinate, where (0, 0, 0) is the lower corner and (1, 1, 1) is the higher corner of the table. Here we put the bread slice near the higher corner of the table.  \n  size: 0.1 # common size of a bread slice \n  lang: \"a slice of bread\"\n  Path: \"bread_slice.obj\"\n```\nAnother example input:\nTask Name: Removing Lid From Pot\nDescription: The robotic arm will remove the lid from the pot.\nObjects involved: KitchenPot. Only the objects specified here should be included in the yaml file.\n```KitchenPot articulation tree\nlinks: \nbase\nlink_0\nlink_1\njoints: \njoint_name: joint_0 joint_type: prismatic parent_link: link_1 child_link: link_0\njoint_name: joint_1 joint_type: fixed parent_link: base child_link: link_1\n```\n```KitchenPot semantics\nlink_0 slider lid\nlink_1 free pot_body"
        },
        {
            "comment": "In this code snippet, a kitchen pot is defined with properties such as use_table, type, mesh, name, on_table, center, size, lang, and path. The kitchen pot is usually placed on the table and has a random location on the table.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/utils.py\":105-131",
            "content": "```\nOutput:\n```yaml\n- use_table: True # A kitchen pot is usually placed on the table.\n- type: mesh\n  name: \"KitchenPot\"\n  on_table: True # kitchen pots are usually placed on a table. \n  center: (0.3, 0.6, 0) # Remember that when an object is placed on the table, the center is expressed in the table coordinate, where (0, 0, 0) is the lower corner and (1, 1, 1) is the higher corner of the table. Here we put the kitchen pot just at a random location on the table.  \n  size: 0.28 # the size of a common kitchen pot is roughly 0.28m\n  lang: \"a common kitchen pot\"\n  path: \"kitchen_pot.urdf\"\n```\nNote in this example, the kitchen pot already has a lid from the semantics file. Therefore, you do not need to include a separate lid in the yaml file.\nOne more example input:\nTask Name: Push the chair.\nDescription: The robotic arm will push and move the chair to a target location.\nObjects involved: A chair. Only the objects here should be included in the yaml file.\n```Chair articulation tree\nlinks: \nbase\nlink_0\nlink_1\njoints: "
        },
        {
            "comment": "The code represents the semantics of a chair, specifying its type as \"Chair\" and stating it's not on a table or use_table is False. It also mentions the mesh name as \"Chair\", that it's on the ground, and has a center point at (1.0, 0, 0). The chair size is specified as 1.2, and it's movable for tasks requiring the robot to push it.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/utils.py\":132-157",
            "content": "joint_name: joint_0 joint_type: revolute parent_link: link_1 child_link: link_0\njoint_name: joint_1 joint_type: fixed parent_link: base child_link: link_1\n```\n```Chair semantics\nlink_0 hinge seat\nlink_1 free leg\n```\nOutput:\n```yaml\n- use_table: False # A chair is usually just on the ground\n- type: mesh\n  name: \"Chair\"\n  on_table: False # An oven is usually just placed on the floor.\n  center: (1.0, 0, 0) # Remember that when not on a table, the center is expressed in the world coordinate. Since the robot is at (1, 1, 0) and the table is at (0, 0, 0), we place the oven at (1.8, 2, 0) to avoid collision with the table and the robot.\n  size: 1.2 # the size of an oven is roughly 0.9m\n  lang: \"a standard chair\"\n  path: \"chair.urdf\"\n  movable: True # here the task requires the robot to push the chair, so the chair has to be moveable.\n```\nNote in the above example we set the chair to be moveable so the robot can push it for executing the task.\nAnother example:\nTask Name: Put an item into the box drawer\nDescription: The robot will open the drawer of the box, and put an item into it."
        },
        {
            "comment": "Code generates a box with articulation tree and semantics, as well as an item object for placement in the drawer of the box. The box is defined using the urdf format, while the item is defined in mesh format. The resulting objects are specified in yaml format, including their names, types, sizes, positions, and other relevant information.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/utils.py\":158-201",
            "content": "Objects involved: A box with drawer, an item to be placed in the drawer. \n```Box articulation tree\nlinks: \nbase\nlink_0\nlink_1\nlink_2\njoints: \njoint_name: joint_0 joint_type: revolute parent_link: link_2 child_link: link_0\njoint_name: joint_1 joint_type: prismatic parent_link: link_2 child_link: link_1\njoint_name: joint_2 joint_type: fixed parent_link: base child_link: link_2\n```\n```Box semantics\nlink_0 hinge rotation_lid\nlink_1 slider drawer\nlink_2 free box_body\n```\nOutput:\n```yaml\n-   use_table: true\n-   center: (0.5, 0.5, 0)\n    lang: \"a wooden box\"\n    name: \"Box\"\n    on_table: true\n    path: \"box.urdf\"\n    size: 0.3\n    type: urdf\n-   path: \"item.obj\"\n    center: (0.2, 0.4, 0)\n    lang: \"A toy\" # Note here, we changed the generic/placeholder \"item\" object to be a more concrete object: a toy. \n    name: \"Item\"\n    on_table: true\n    size: 0.05\n    type: mesh\n```\nOne more example:\nTask Name: Fetch item from refrigerator\nDescription: The robot will open the refrigerator door, and fetch an item from the refrigerator.\nObjects involved: A refrigerator, an item to be fetched from the refrigerator."
        },
        {
            "comment": "This code defines a refrigerator model in URDF format with three links (base, door hinge, and door) and joints for articulation. The semantics describe the refrigerator as heavy and having two doors with hinges. The output YAML file includes information such as the use of a table, the center position, language, name, on-table status, path, reward asset path, size, and type.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/utils.py\":203-233",
            "content": "```Refirgerator articulation tree\nlinks: \nbase\nlink_0\nlink_1\nlink_2\njoints: \njoint_name: joint_0 joint_type: fixed parent_link: base child_link: link_0\njoint_name: joint_1 joint_type: revolute parent_link: link_0 child_link: link_1\njoint_name: joint_2 joint_type: revolute parent_link: link_0 child_link: link_2\n```\n```Refrigerator semantics\nlink_0 heavy refrigerator_body\nlink_1 hinge door\nlink_2 hinge door\n```\nOutput:\n```yaml\n-   use_table: true # the fetched item should be placed on the table, after it's moved out of the refrigerator.\n-   center: (1.0, 0.2, 0) # Remember that when not on a table, the center is expressed in the world coordinate. Since the robot is at (1, 1, 0) and the table is at (0, 0, 0), we place the oven at (1.8, 2, 0) to avoid collision with the table and the robot.\n    lang: a common two-door refrigerator\n    name: Refrigerator\n    on_table: false # the refrigerator is usually placed on the floor.\n    path: refrigerator.urdf\n    reward_asset_path: '10612'\n    size: 1.8\n    type: urdf\n-   center: (1.0, 0.2, 0.5) # the soda can is initially placed inside the refrigerator."
        },
        {
            "comment": "This code snippet is parsing a response to get a YAML file for a given task description. It takes the response, task description, save path, and optional parameters for temperature and model. The code finds the \"```yaml\" line and extracts the YAML content following it. This information could be used to generate a YAML configuration file for a specific task involving objects.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/utils.py\":234-258",
            "content": "    lang: a can of soda\n    name: Item\n    on_table: false # the item is initially placed inside the refrigerator\n    path: soda_can.obj\n    size: 0.2\n    type: mesh\n```\nRules: \n- You do not need to include the robot in the yaml file.\n- The yaml file should only include the objects listed in \"Objects involved\".\n- Sometimes, the task description / objects involved will refer to generic/placeholder objects, e.g., to place an \"item\" into the drawer, and to heat \"food\" in the microwave. In the generated yaml config, you should change these placeholder objects to be concrete objects in the lang field, e.g., change \"item\" to be a toy or a pencil, and \"food\" to be a hamburger, a bowl of soup, etc. \nCan you do this for the following task:\nTask Name: {}\nDescription: {}\nObjects involved: {}\n\"\"\"\ndef parse_response_to_get_yaml(response, task_description, save_path, temperature=0.2, model='gpt-4'):\n    yaml_string = []\n    for l_idx, line in enumerate(response):\n        if \"```yaml\" in line:\n            for l_idx_2 in range(l_idx + 1, len(response)):"
        },
        {
            "comment": "This code is parsing a task response, extracting task names, descriptions, and additional objects. It preprocesses the task description by replacing spaces, periods, commas, etc., and generates a save name for the YAML file. The function returns parsed size data and the save name after querying GPT to adjust object sizes.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/utils.py\":259-286",
            "content": "                if response[l_idx_2].lstrip().startswith(\"```\"):\n                    break\n                yaml_string.append(response[l_idx_2])\n            yaml_string = '\\n'.join(yaml_string)\n            description = f\"{task_description}\".replace(\" \", \"_\").replace(\".\", \"\").replace(\",\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n            save_name =  description + '.yaml'\n            print(\"=\" * 30)\n            print(\"querying GPT to adjust the size of the objects\")\n            print(\"=\" * 30)\n            parsed_size_yaml = adjust_size_v2(description, yaml_string, save_path, temperature, model=model)\n            return parsed_size_yaml, save_name\ndef parse_task_response(task_response):\n    task_names = []\n    task_descriptions = []\n    additional_objects = []\n    links = []\n    joints = []\n    task_response = task_response.split(\"\\n\")\n    for l_idx, line in enumerate(task_response):\n        if line.lower().startswith(\"task name:\"):\n            task_name = line.split(\":\")[1].strip()\n            task_name = task_name.replace(\"/\", \" or \").replace(\".\", \"\").replace(\"'\", \"\").replace('\"', \"\")"
        },
        {
            "comment": "This code extracts task details from a given response, including task name and description, additional objects involved, links, and joints. It appends these extracted details into respective lists for further use.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/utils.py\":287-303",
            "content": "            task_names.append(task_name)\n            task_description = task_response[l_idx+1].split(\":\")[1].strip()\n            task_description = task_description.replace(\"/\", \" or \").replace(\".\", \"\").replace(\"'\", \"\").replace('\"', \"\").replace(\")\", \".\").replace(\"(\", \".\")\n            task_descriptions.append(task_description)\n            additional_objects.append(task_response[l_idx+2].split(\":\")[1].strip())\n            involved_links = \"\"\n            for link_idx in range(l_idx+4, len(task_response)):\n                if task_response[link_idx].lower().startswith(\"joints:\"):\n                    break\n                else:\n                    # involved_links.append(task_response[link_idx].split(\":\")[0][2:])\n                    involved_links += (task_response[link_idx][2:])\n            links.append(involved_links)\n            involved_joints = \"\"\n            for joint_idx in range(link_idx+1, len(task_response)):\n                if not task_response[joint_idx].lower().startswith(\"- \"):\n                    break"
        },
        {
            "comment": "Code takes a string input, splits it, and appends the substring to a list. If an additional object is not given, it uses the first argument as the task object.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/utils.py\":304-327",
            "content": "                else:\n                    # involved_joints.append(task_response[joint_idx].split(\":\")[0][2:])\n                    involved_joints += (task_response[joint_idx][2:])\n            joints.append(involved_joints)\n    return task_names, task_descriptions, additional_objects, links, joints\ndef build_task_given_text(object_category, task_name, task_description, additional_object, involved_links, involved_joints, \n                          articulation_tree_filled, semantics_filled, object_path, save_folder, temperature_dict, model_dict=None):\n    if model_dict is None:\n        model_dict = {\n            \"task_generation\": \"gpt-4\",\n            \"reward\": \"gpt-4\",\n            \"yaml\": \"gpt-4\",\n            \"size\": \"gpt-4\",\n            \"joint\": \"gpt-4\",\n            \"spatial_relationship\": \"gpt-4\"\n        }\n    task_yaml_config_prompt_filled = copy.deepcopy(task_yaml_config_prompt)\n    if additional_object.lower() == \"none\":\n        task_object = object_category\n    else:\n        task_object = \"{}, {}\".format(object_category, additional_object)"
        },
        {
            "comment": "This code generates a task YAML configuration by utilizing GPT-4. It combines prompt filling, articulation tree, and semantics data. The system uses the response from GPT-4 to create a JSON file (task_yaml_config). The code then extracts relevant information, formats it into a new JSON file for size, and saves it. This process is designed to generate task YAML configurations for a simulator.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/utils.py\":328-342",
            "content": "    task_yaml_config_prompt_filled = task_yaml_config_prompt_filled.format(task_name, task_description, task_object)\n    task_yaml_config_prompt_filled += articulation_tree_filled + semantics_filled\n    system = \"You are a helpful assistant.\"\n    save_path = os.path.join(save_folder, \"gpt_response/task_yaml_config_{}.json\".format(task_name))\n    print(\"=\" * 50)\n    print(\"=\" * 20, \"generating task yaml config\", \"=\" * 20)\n    print(\"=\" * 50)\n    task_yaml_response = query(system, [task_yaml_config_prompt_filled], [], save_path=save_path, debug=False, \n                            temperature=temperature_dict[\"yaml\"], model=model_dict[\"yaml\"])\n    # NOTE: parse the yaml file and generate the task in the simulator.\n    description = f\"{task_name}_{task_description}\".replace(\" \", \"_\").replace(\".\", \"\").replace(\",\", \"\")\n    task_yaml_response = task_yaml_response.split(\"\\n\")\n    size_save_path = os.path.join(save_folder, \"gpt_response/size_{}.json\".format(task_name))\n    parsed_yaml, save_name = parse_response_to_get_yaml(task_yaml_response, description, save_path=size_save_path, "
        },
        {
            "comment": "This code is creating a configuration file for an articulated object using information from the input. It sets the object type to 'urdf' and adds a reward asset path for reward generation. The generated file is saved at the specified save folder, and a deep copy of the parsed_yaml is made for further processing. Finally, the code decompose the configuration, generates a reward, and saves it in a separate JSON file at the save folder.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/utils.py\":343-365",
            "content": "                                                        temperature=temperature_dict[\"size\"], model=model_dict[\"size\"])\n    # NOTE: post-process such that articulated object is urdf.\n    # NOTE: post-process to include the reward asset path for reward generation. \n    for obj in parsed_yaml:\n        if \"name\" in obj and obj['name'] == object_category:\n            obj['type'] = 'urdf'\n            obj['reward_asset_path'] = object_path\n    # config_path = \"gpt_4/data/parsed_configs_semantic_articulated/{}-{}\".format(object_category, time_string)\n    config_path = save_folder\n    with open(os.path.join(config_path, save_name), 'w') as f:\n        yaml.dump(parsed_yaml, f, indent=4)\n    input_to_reward_config = copy.deepcopy(parsed_yaml)\n    for obj in input_to_reward_config:\n        if \"reward_asset_path\" in obj:\n            input_to_reward_config.remove(obj)\n    initial_config = yaml.safe_dump(parsed_yaml)\n    ### decompose and generate reward\n    yaml_file_path = os.path.join(config_path, save_name)\n    reward_save_path = os.path.join(save_folder, \"gpt_response/reward_{}.json\".format(task_name))"
        },
        {
            "comment": "The code prints headers for different sections and generates a reward or primitive solution, a joint angle file path, and reads substeps from a file.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/utils.py\":366-383",
            "content": "    print(\"=\" * 50)\n    print(\"=\" * 20, \"generating reward\", \"=\" * 20)\n    print(\"=\" * 50)\n    solution_path = decompose_and_generate_reward_or_primitive(task_name, task_description, initial_config, \n                                                                articulation_tree_filled, semantics_filled, \n                                                                involved_links, involved_joints, object_path, \n                                                                yaml_file_path, save_path=reward_save_path,\n                                                                temperature=temperature_dict[\"reward\"],\n                                                                model=model_dict[\"reward\"])\n    ### generate joint angle\n    save_path = os.path.join(save_folder, \"gpt_response/joint_angle_{}.json\".format(task_name))\n    substep_file_path = os.path.join(solution_path, \"substeps.txt\")\n    with open(substep_file_path, 'r') as f:\n        substeps = f.readlines()\n    print(\"=\" * 50)\n    print(\"=\" * 20, \"generating initial joint angle\", \"=\" * 20)"
        },
        {
            "comment": "The code queries joint angle values, sets object name in joint angle dictionary, finds involved objects from config file, creates the save path for spatial relationships JSON file, and then proceeds to query spatial relationships.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/utils.py\":384-401",
            "content": "    print(\"=\" * 50)\n    joint_angle_values = query_joint_angle(task_name, task_description, articulation_tree_filled, semantics_filled, \n                                            involved_links, involved_joints, substeps, save_path=save_path, \n                                            temperature=temperature_dict['joint'], model=model_dict[\"joint\"])\n    joint_angle_values[\"set_joint_angle_object_name\"] = object_category\n    involved_objects = []\n    config = yaml.safe_load(initial_config)\n    for obj in config:\n        if \"name\" in obj:\n            involved_objects.append(obj[\"name\"])\n    involved_objects = \", \".join(involved_objects)\n    save_path = os.path.join(save_folder, \"gpt_response/spatial_relationships_{}.json\".format(task_name))\n    print(\"=\" * 50)\n    print(\"=\" * 20, \"generating initial spatial relationship\", \"=\" * 20)\n    print(\"=\" * 50)\n    spatial_relationships = query_spatial_relationship(task_name, task_description, involved_objects, articulation_tree_filled, semantics_filled, \n   "
        },
        {
            "comment": "This code saves a configuration file for a spatial relationship task in RoboGen. It appends various information to the config list, including involved links and joints, substeps, solution path, joint angle values, spatial relationships, task name, and task description. Finally, it writes the configuration to two YAML files at specified paths.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/utils.py\":401-413",
            "content": "                                         involved_links, involved_joints, substeps, save_path=save_path, \n                                            temperature=temperature_dict['spatial_relationship'], model=model_dict[\"spatial_relationship\"])\n    config.append(dict(solution_path=solution_path))\n    config.append(joint_angle_values)\n    config.append(dict(spatial_relationships=spatial_relationships))\n    config.append(dict(task_name=task_name, task_description=task_description))\n    with open(os.path.join(config_path, save_name), 'w') as f:\n        yaml.dump(config, f, indent=4)\n    with open(os.path.join(solution_path, \"config.yaml\"), 'w') as f:\n        yaml.dump(config, f, indent=4)\n    return os.path.join(config_path, save_name)"
        }
    ]
}