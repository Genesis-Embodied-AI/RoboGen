{
    "summary": "The code initializes variables, performs approach actions, and resets state as needed for grasping. It also activates suction and deactivates it after approaching the object. The simulation estimates object normals, aligns the robot's arm, adjusts target positions, plans motion with collision avoidance, saves data, considers dynamics for accuracy, handles collisions, and restores environment state if necessary.",
    "details": [
        {
            "comment": "The code is initializing the necessary functions and variables for grasping an object. The \"get_save_path\" function returns the path where primitive files will be saved, and \"release_grasp\" deactivates suction and saves the environment state for 20 simulation steps in separate files. Finally, \"grasp_object\" saves the initial environment state before grasping an object with the specified name.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_primitive_api.py\":0-33",
            "content": "import pybullet as p\nimport os\nimport numpy as np\nimport open3d as o3d\nfrom manipulation.motion_planning_utils import motion_planning\nfrom manipulation.grasping_utils import get_pc_and_normal, align_gripper_z_with_normal, align_gripper_x_with_normal\nfrom manipulation.gpt_reward_api import get_link_pc, get_bounding_box, get_link_id_from_name\nfrom manipulation.utils import save_env, load_env\nMOTION_PLANNING_TRY_TIMES=100\ndef get_save_path(simulator):\n    return simulator.primitive_save_path\ndef release_grasp(simulator):\n    simulator.deactivate_suction()\n    save_path = get_save_path(simulator)\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n    rgbs = []\n    states = []\n    for t in range(20):\n        p.stepSimulation()\n        rgbs.append(simulator.render()[0])\n        state_save_path = os.path.join(save_path, \"state_{}.pkl\".format(t))\n        save_env(simulator, state_save_path)\n        states.append(state_save_path)\n    return rgbs, states\ndef grasp_object(simulator, object_name):\n    ori_state = save_env(simulator, None)"
        },
        {
            "comment": "Code snippet checks if the target object is already grasped, and if not, performs an approach action. If the object is grasped, it saves 10 renders with their corresponding states. It returns a list of renders and a list of state save paths.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_primitive_api.py\":34-58",
            "content": "    p.stepSimulation()\n    object_name = object_name.lower()\n    save_path = get_save_path(simulator)\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n    # if the target object is already grasped.  \n    points = p.getContactPoints(bodyA=simulator.robot.body, linkIndexA=simulator.suction_id, physicsClientId=simulator.id)\n    if points:\n        for point in points:\n            obj_id, contact_link = point[2], point[4]\n            if obj_id == simulator.urdf_ids[object_name]:\n                simulator.activate_suction()\n                rgbs = []\n                states = []\n                for t in range(10):\n                    p.stepSimulation()\n                    rgbs.append(simulator.render()[0])\n                    state_save_path = os.path.join(save_path, \"state_{}.pkl\".format(t))\n                    save_env(simulator, state_save_path)\n                    states.append(state_save_path)\n                return rgbs, states\n    rgbs, states = approach_object(simulator, object_name)\n    base_t = len(rgbs)"
        },
        {
            "comment": "The code checks if the base time is greater than 1. If true, it activates suction for 10 timesteps and saves the states at each step. If false, it directly resets the state. The grasp_object_link function saves the initial state, steps simulation, converts object name to lowercase, creates save directory if not exists, checks if target link is already grasped, and returns points if any.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_primitive_api.py\":59-86",
            "content": "    if base_t > 1:\n        for t in range(10):\n            simulator.activate_suction()\n            p.stepSimulation()\n            rgbs.append(simulator.render()[0])\n            state_save_path = os.path.join(save_path, \"state_{}.pkl\".format(t + base_t))\n            save_env(simulator, state_save_path)\n            states.append(state_save_path)\n    else:\n        # directy reset the state\n        load_env(simulator, state=ori_state)\n    return rgbs, states\ndef grasp_object_link(simulator, object_name, link_name):\n    ori_state = save_env(simulator, None)\n    p.stepSimulation()\n    object_name = object_name.lower()\n    save_path = get_save_path(simulator)\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n    # if the target object link is already grasped.  \n    points = p.getContactPoints(bodyA=simulator.robot.body, linkIndexA=simulator.suction_id, physicsClientId=simulator.id)\n    if points:\n        for point in points:\n            obj_id, contact_link = point[2], point[4]\n            if obj"
        },
        {
            "comment": "This code checks if the suction is activated and collects RGB images and state data for 10 steps. If there are existing RGB images, it adds more, and saves the states at each step.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_primitive_api.py\":86-109",
            "content": "_id == simulator.urdf_ids[object_name] and contact_link == get_link_id_from_name(simulator, object_name, link_name):\n                simulator.activate_suction()\n                rgbs = []\n                states = []\n                for t in range(10):\n                    p.stepSimulation()\n                    rgbs.append(simulator.render()[0])\n                    state_save_path = os.path.join(save_path, \"state_{}.pkl\".format(t))\n                    save_env(simulator, state_save_path)\n                    states.append(state_save_path)\n                return rgbs, states\n    rgbs, states = approach_object_link(simulator, object_name, link_name)\n    base_t = len(rgbs)\n    if base_t > 1:\n        simulator.activate_suction()\n        for t in range(10):\n            p.stepSimulation()\n            rgbs.append(simulator.render()[0])\n            state_save_path = os.path.join(save_path, \"state_{}.pkl\".format(t + base_t))\n            save_env(simulator, state_save_path)\n            states.append(state_save_path)\n    else:"
        },
        {
            "comment": "Resets the state and saves it, then deactivates suction and approaches an object by releasing the gripper after saving states for 20 steps.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_primitive_api.py\":110-140",
            "content": "        # directy reset the state\n        load_env(simulator, state=ori_state)\n    return rgbs, states\ndef approach_object(simulator, object_name, dynamics=False):\n    save_path = get_save_path(simulator)\n    ori_state = save_env(simulator, None)\n    simulator.deactivate_suction()\n    release_rgbs = []\n    release_states = []\n    release_steps = 20\n    for t in range(release_steps):\n        p.stepSimulation()\n        rgb, depth = simulator.render()\n        release_rgbs.append(rgb)\n        state_save_path = os.path.join(save_path, \"state_{}.pkl\".format(t))\n        save_env(simulator, state_save_path)\n        release_states.append(state_save_path)\n    object_name = object_name.lower()\n    it = 0\n    object_name = object_name.lower()\n    object_pc, object_normal = get_pc_and_normal(simulator, object_name)\n    low, high = get_bounding_box(simulator, object_name)\n    com = (low + high) / 2\n    current_joint_angles = simulator.robot.get_joint_angles(indices=simulator.robot.right_arm_joint_indices)\n    while True:\n        random_point = object_pc[np.random.randint(0, object_pc.shape[0])]"
        },
        {
            "comment": "The code randomly selects a normal from an object's normal distribution and checks if it points outwards. It then sets the robot arm joint angles for two cases (normal and negative normal) and calculates target positions and orientations depending on the robot type. This allows the robot to manipulate objects with different robots (panda, sawyer, ur5, fetch).",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_primitive_api.py\":141-160",
            "content": "        random_normal = object_normal[np.random.randint(0, object_normal.shape[0])]\n        ### adjust the normal such that it points outwards the object.\n        line = com - random_point\n        if np.dot(line, random_normal) > 0:\n            random_normal = -random_normal\n        for normal in [random_normal, -random_normal]:\n            simulator.robot.set_joint_angles(simulator.robot.right_arm_joint_indices, current_joint_angles)\n            target_pos = random_point\n            real_target_pos = target_pos + normal * 0\n            if simulator.robot_name in [\"panda\", \"sawyer\"]:\n                target_orientation = align_gripper_z_with_normal(-normal).as_quat()\n                mp_target_pos = target_pos + normal * 0.03\n            elif simulator.robot_name in ['ur5', 'fetch']:\n                target_orientation = align_gripper_x_with_normal(-normal).as_quat()\n                if simulator.robot_name == 'ur5':\n                    mp_target_pos = target_pos + normal * 0.07\n                elif simulator.robot_name == 'fetch':"
        },
        {
            "comment": "This code snippet calculates a target position and removes the \"robot\" object from the list of all objects. It then defines obstacles using the remaining objects, performs motion planning by calling the \"motion_planning\" function with the calculated target position, and checks for successful results. If successful, it creates directories if they don't exist, stores release RGB values, and iterates through path coordinates to set joint angles or control the robot's arm using motor gains and forces.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_primitive_api.py\":161-180",
            "content": "                    mp_target_pos = target_pos + normal * 0.07\n            all_objects = list(simulator.urdf_ids.keys())\n            all_objects.remove(\"robot\")\n            obstacles = [simulator.urdf_ids[x] for x in all_objects]\n            allow_collision_links = []\n            res, path = motion_planning(simulator, mp_target_pos, target_orientation, obstacles=obstacles, allow_collision_links=allow_collision_links)\n            if res:\n                if not os.path.exists(save_path):\n                    os.makedirs(save_path)\n                rgbs = release_rgbs\n                intermediate_states = release_states\n                for idx, q in enumerate(path):\n                    if not dynamics:\n                        simulator.robot.set_joint_angles(simulator.robot.right_arm_joint_indices, q)\n                        p.stepSimulation()\n                    else:\n                        for _ in range(3):\n                            simulator.robot.control(simulator.robot.right_arm_joint_indices, q, simulator.robot.motor_gains, forces=5 * 240.)"
        },
        {
            "comment": "Code snippet performs robot arm IK manipulation using PyBullet simulation. It renders the robot's state, saves intermediate states, sets joint motor control for positioning and controls the right arm joint indices to achieve a specific target orientation.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_primitive_api.py\":181-196",
            "content": "                            p.stepSimulation()\n                    rgb, depth = simulator.render()\n                    rgbs.append(rgb)\n                    save_state_path = os.path.join(save_path,  \"state_{}.pkl\".format(idx + release_steps))\n                    save_env(simulator, save_state_path)\n                    intermediate_states.append(save_state_path)\n                base_idx = len(intermediate_states)\n                for t in range(20):\n                    ik_indices = [_ for _ in range(len(simulator.robot.right_arm_joint_indices))]\n                    ik_joints = simulator.robot.ik(simulator.robot.right_end_effector, \n                                                    real_target_pos, target_orientation, \n                                                    ik_indices=ik_indices)\n                    p.setJointMotorControlArray(simulator.robot.body, jointIndices=simulator.robot.right_arm_joint_indices, \n                                                controlMode=p.POSITION_CONTROL, targetPositions=ik_joints,"
        },
        {
            "comment": "This code snippet initializes forces for the robot's right arm joints, steps the simulation, renders the scene, appends RGB values to a list, saves intermediate simulation states, and checks for collisions between the suction and rigid objects. If a collision occurs, it handles the contact by storing the relevant information in variables.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_primitive_api.py\":197-213",
            "content": "                                                forces=[5*240] * len(simulator.robot.right_arm_joint_indices), physicsClientId=simulator.id)\n                    p.stepSimulation()\n                    rgb, depth = simulator.render()\n                    rgbs.append(rgb)\n                    save_state_path = os.path.join(save_path,  \"state_{}.pkl\".format(base_idx + t))\n                    save_env(simulator, save_state_path)\n                    intermediate_states.append(save_state_path)\n                    # TODO: check if there is already a collision. if so, break.\n                    collision = False\n                    points = p.getContactPoints(bodyA=simulator.robot.body, linkIndexA=simulator.suction_id, physicsClientId=simulator.id)\n                    if points:\n                        # Handle contact between suction with a rigid object.\n                        for point in points:\n                            obj_id, contact_link, contact_position_on_obj = point[2], point[4], point[6]\n                            if obj_id == simulator.urdf_ids['plane'] or obj_id == simulator.robot.body:"
        },
        {
            "comment": "The code attempts to execute a primitive motion planning, and if it fails, it restores the environment state and saves it before returning initial RGBs and state files. If the primitive execution succeeds, it stores the intermediate states and RGBs for future use. The function approach_object_link deactivates suction, executes a release motion plan for 20 simulation steps, and then steps the simulation.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_primitive_api.py\":214-241",
            "content": "                                pass\n                            else:\n                                collision = True\n                                break\n                    if collision:\n                        break\n                return rgbs, intermediate_states\n            it += 1\n            if it > MOTION_PLANNING_TRY_TIMES:\n                print(\"failed to execute the primitive\")\n                load_env(simulator, state=ori_state)\n                save_env(simulator, os.path.join(save_path,  \"state_{}.pkl\".format(0)))\n                rgbs = [simulator.render()[0]]\n                state_files = [os.path.join(save_path,  \"state_{}.pkl\".format(0))]\n                return rgbs, state_files\ndef approach_object_link(simulator, object_name, link_name, dynamics=False):\n    save_path = get_save_path(simulator)\n    ori_state = save_env(simulator, None)\n    simulator.deactivate_suction()\n    release_rgbs = []\n    release_states = []\n    release_steps = 20\n    for t in range(release_steps):\n        p.stepSimulation()"
        },
        {
            "comment": "This code snippet is responsible for rendering a simulator, saving its state at each timestep, and calculating object normals for manipulation tasks. It retrieves the link and object points cloud, estimates their normals, and aligns them to ensure they point outwards from the object. The normal adjustment is important for successful manipulation tasks.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_primitive_api.py\":242-267",
            "content": "        rgb, depth = simulator.render()\n        release_rgbs.append(rgb)\n        state_save_path = os.path.join(save_path, \"state_{}.pkl\".format(t))\n        save_env(simulator, state_save_path)\n        release_states.append(state_save_path)\n    object_name = object_name.lower()\n    it = 0\n    object_name = object_name.lower()\n    link_pc = get_link_pc(simulator, object_name, link_name) \n    object_pc = link_pc\n    pcd = o3d.geometry.PointCloud() \n    pcd.points = o3d.utility.Vector3dVector(object_pc)\n    pcd.estimate_normals()\n    object_normal = np.asarray(pcd.normals)\n    current_joint_angles = simulator.robot.get_joint_angles(indices=simulator.robot.right_arm_joint_indices)\n    while True:\n        object_name = object_name.lower()\n        target_pos = link_pc[np.random.randint(0, link_pc.shape[0])]\n        nearest_point_idx = np.argmin(np.linalg.norm(object_pc - target_pos.reshape(1, 3), axis=1))\n        align_normal = object_normal[nearest_point_idx]\n        ### adjust the normal such that it points outwards the object."
        },
        {
            "comment": "This code calculates the center of a bounding box using get_bounding_box() and aligns the robot's arm based on the object's normal direction. It adjusts the target position and orientation for different robots, sets joint angles, and adds a debug line to visualize the process. The code checks the robot type (panda, sawyer, ur5, or fetch) and aligns the gripper accordingly.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_primitive_api.py\":268-286",
            "content": "        low, high = get_bounding_box(simulator, object_name)\n        com = (low + high) / 2\n        line = com - target_pos\n        if np.dot(line, align_normal) > 0:\n            align_normal = -align_normal\n        for normal in [align_normal, -align_normal]:\n            simulator.robot.set_joint_angles(simulator.robot.right_arm_joint_indices, current_joint_angles)\n            real_target_pos = target_pos + normal * 0\n            debug_id = p.addUserDebugLine(target_pos, target_pos + normal, [1, 0, 0], 5)\n            if simulator.robot_name in [\"panda\", \"sawyer\"]:\n                target_orientation = align_gripper_z_with_normal(-normal).as_quat()\n                mp_target_pos = target_pos + normal * 0.03\n            elif simulator.robot_name in ['ur5', 'fetch']:\n                target_orientation = align_gripper_x_with_normal(-normal).as_quat()\n                if simulator.robot_name == 'ur5':\n                    mp_target_pos = target_pos + normal * 0.07\n                elif simulator.robot_name == 'fetch':"
        },
        {
            "comment": "This code adjusts the target position, removes unnecessary objects from simulation, plans motion with collision avoidance, and executes the planned motion on the robot's right arm. If dynamics are considered, it repeats the control action three times for better accuracy. Finally, if the path is successful, it saves RGB data and intermediate states at each step in a specified folder.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_primitive_api.py\":287-306",
            "content": "                    mp_target_pos = target_pos + normal * 0.07\n            all_objects = list(simulator.urdf_ids.keys())\n            all_objects.remove(\"robot\")\n            obstacles = [simulator.urdf_ids[x] for x in all_objects]\n            allow_collision_links = []\n            res, path = motion_planning(simulator, mp_target_pos, target_orientation, obstacles=obstacles, allow_collision_links=allow_collision_links)\n            p.removeUserDebugItem(debug_id)\n            if res:\n                if not os.path.exists(save_path):\n                    os.makedirs(save_path)\n                rgbs = release_rgbs\n                intermediate_states = release_states\n                for idx, q in enumerate(path):\n                    if not dynamics:\n                        simulator.robot.set_joint_angles(simulator.robot.right_arm_joint_indices, q)\n                    else:\n                        for _ in range(3):\n                            simulator.robot.control(simulator.robot.right_arm_joint_indices, q, simulator.robot.motor_gains, forces=5 * 240.)"
        },
        {
            "comment": "The code snippet is part of a simulation where the robot arm's joint angles are updated based on inverse kinematics calculations. RGB and depth images are obtained from rendering, intermediate states are saved, and the right arm is controlled using joint motor control for 20 steps after motion planning.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_primitive_api.py\":307-325",
            "content": "                            p.stepSimulation()\n                    p.stepSimulation()\n                    rgb, depth = simulator.render()\n                    rgbs.append(rgb)\n                    save_state_path = os.path.join(save_path,  \"state_{}.pkl\".format(idx + release_steps))\n                    save_env(simulator, save_state_path)\n                    intermediate_states.append(save_state_path)\n                base_idx = len(intermediate_states)\n                for t in range(20):\n                    print(\"post motion planing step: \", t)\n                    print(\"rgb image length: \", len(rgbs))\n                    ik_indices = [_ for _ in range(len(simulator.robot.right_arm_joint_indices))]\n                    ik_joints = simulator.robot.ik(simulator.robot.right_end_effector, \n                                                    real_target_pos, target_orientation, \n                                                    ik_indices=ik_indices)\n                    p.setJointMotorControlArray(simulator.robot.body, jointIndices=simulator.robot.right_arm_joint_indices, "
        },
        {
            "comment": "Code snippet checks for collisions between the robot's suction and a rigid object. It retrieves contact points, and if any are found, it proceeds to handle the contact by iterating over them. The function saves the current state of the simulation into a file before checking for collisions. If collisions occur, the code breaks out of the loop.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_primitive_api.py\":326-341",
            "content": "                                                controlMode=p.POSITION_CONTROL, targetPositions=ik_joints,\n                                                forces=[5*240] * len(simulator.robot.right_arm_joint_indices), physicsClientId=simulator.id)\n                    p.stepSimulation()\n                    rgb, depth = simulator.render()\n                    rgbs.append(rgb)\n                    save_state_path = os.path.join(save_path,  \"state_{}.pkl\".format(base_idx + t))\n                    save_env(simulator, save_state_path)\n                    intermediate_states.append(save_state_path)\n                    # TODO check here if there is a collision. if so, break\n                    collision = False\n                    points = p.getContactPoints(bodyA=simulator.robot.body, linkIndexA=simulator.suction_id, physicsClientId=simulator.id)\n                    if points:\n                        # Handle contact between suction with a rigid object.\n                        for point in points:\n                            obj_id, contact_link, contact_position_on_obj = point[2], point[4], point[6]"
        },
        {
            "comment": "This code checks for collisions during motion planning and retries if necessary. If a certain object is identified, no collision check is performed. The code also handles failure by restoring the environment state and saving it before returning default values.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_primitive_api.py\":343-360",
            "content": "                            if obj_id == simulator.urdf_ids['plane'] or obj_id == simulator.robot.body:\n                                pass\n                            else:\n                                collision = True\n                                break\n                    if collision:\n                        break\n                return rgbs, intermediate_states \n            it += 1\n            if it > MOTION_PLANNING_TRY_TIMES:\n                print(\"failed to execute the primitive\")\n                load_env(simulator, state=ori_state)\n                save_env(simulator, os.path.join(save_path,  \"state_{}.pkl\".format(0)))\n                rgbs = [simulator.render()[0]]\n                state_files = [os.path.join(save_path,  \"state_{}.pkl\".format(0))]\n                return rgbs, state_files"
        }
    ]
}