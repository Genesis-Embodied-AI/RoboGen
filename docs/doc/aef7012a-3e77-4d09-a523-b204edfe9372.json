{
    "summary": "The code employs reinforcement learning for quadruped/humanoid locomotion, utilizing GPT to generate tasks and YAML format for task configuration. It saves data in specified directories and returns the generated task config path.",
    "details": [
        {
            "comment": "Importing necessary modules and defining the user_contents list, which contains example prompts for proposing locomotion tasks, reward functions for quadruped/humanoid robots in a simulator using reinforcement learning.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/prompt_locomotion.py\":0-21",
            "content": "import os\nuser_contents = [\n\"\"\"\nYour goal is to propose some locomotion tasks for a quadruped/humanoid robot, and writing the corresponding reward functions for the quadruped to learn that specific locomotion skill in a simulator, using reinforcement learning.\nHere are some examples:\nFirst example:\nSkill: flip rightwards\nReward:\n```python\ndef _compute_reward(self):\n    # we first get some information of the quadruped/humanoid robot.\n    # COM_pos and COM_quat are the position and orientation (quaternion) of the center of mass of the quadruped/humanoid.\n    COM_pos, COM_quat = get_robot_pose(self)\n    # COM_vel, COM_ang are the velocity and angular velocity of the center of mass of the quadruped/humanoid.\n    COM_vel, COM_ang = get_robot_velocity(self)\n    # face_dir, side_dir, and up_dir are three axes of the rotation of the quadruped/humanoid.\n    # face direction points from the center of mass towards the face direction of the quadruped/humanoid.\n    # side direction points from the center of mass towards the side body direction of the quadruped/humanoid."
        },
        {
            "comment": "This code calculates the reward for a quadruped/humanoid locomotion skill. It initializes face_dir, side_dir, and up_dir using get_robot_direction function. The target_side and target_ang are set to maintain the initial side direction and spin around x axis during flip. Alpha_ang and alpha_side are defined as 1.0. r_ang and r_side are calculated as -alpha_ang*norm(COM_ang-target_ang) and -alpha_side*norm(side_dir-target_side), respectively. The code adds r_ang and r_side to r. It includes a default energy term that penalizes the robot for consuming too much energy, returned as r+r_energy.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/prompt_locomotion.py\":22-45",
            "content": "    # up direction points from the center of mass towards up, i.e., the negative direction of the gravity. \n    # gravity direction is [0, 0, -1].\n    # when initialized, the face of the robot is along the x axis, the side of the robot is along the y axis, and the up of the robot is along the z axis.\n    face_dir, side_dir, up_dir = get_robot_direction(self, COM_quat)\n    target_side = np.array([0, 1, 0]) # maintain initial side direction during flip\n    target_ang = np.array([50, 0, 0.0]) # spin around x axis to do the rightwards flip, since x is the face direction of the robot.\n    alpha_ang = 1.0\n    alpha_side = 1.0\n    r_ang    = - alpha_ang * np.linalg.norm(COM_ang - target_ang)\n    r_side   = - alpha_side * np.linalg.norm(side_dir - target_side)\n    r += r_ang + r_side\n    # there is a default energy term that penalizes the robot for consuming too much energy. This should be included for all skill.\n    r_energy = get_energy_reward(self)\n    return r + r_energy\n```\nsome more examples:\n{}\nCan you think of 3 more locomotion skills that a quadruped/humanoid can perform?"
        },
        {
            "comment": "This code computes a reward for a specific skill. It retrieves information about the robot's center of mass position and orientation, velocity, angular velocity, and rotation axes. The purpose of these values may be to evaluate how well the robot performed the skill.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/prompt_locomotion.py\":47-75",
            "content": "For each skill,\nYour output format should be:\nSkill: <skill name>\nReward:\n```python\ndef _compute_reward(self):\n    # your code here\n    return r\n```\n\"\"\"\n]\ngood_exmaples = [\n\"\"\"\nSkill: jump backward\nReward:\n```python\ndef _compute_reward(self):\n    # we first get some information of the quadruped/humanoid.\n    # COM_pos and COM_quat are the position and orientation (quaternion) of the center of mass of the quadruped/humanoid.\n    COM_pos, COM_quat = get_robot_pose(self)\n    # COM_vel, COM_ang are the velocity and angular velocity of the center of mass of the quadruped/humanoid.\n    COM_vel, COM_ang = get_robot_velocity(self)\n    # face_dir, side_dir, and up_dir are three axes of the rotation of the quadruped/humanoid.\n    # face direction points from the center of mass towards the face direction of the quadruped/humanoid.\n    # side direction points from the center of mass towards the side body direction of the quadruped/humanoid.\n    # up direction points from the center of mass towards up, i.e., the negative direction of the gravity. "
        },
        {
            "comment": "The code initializes target velocities and directions for a robot, including jumping backward. For the first 30 time steps, it sets a target height of 5.0 units, then switches to a target height of 0.0 afterward. It defines alpha values and calculates distances from current state to targets for velocity, angular position, and face/up/side directions to compute the reward function.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/prompt_locomotion.py\":76-100",
            "content": "    # gravity direction is [0, 0, -1].\n    # when initialized, the face of the robot is along the x axis, the side of the robot is along the y axis, and the up of the robot is along the z axis.\n    face_dir, side_dir, up_dir = get_robot_direction(self, COM_quat)\n    if self.time_step <= 30: # first a few steps the robot are jumping\n        target_height = 5.0\n    else: # then it should not jump\n        target_height = 0.0\n    target_v = np.array([-5.0, 0, 0.0]) # jump backwards\n    target_up = np.array([0, 0, 1]) # maintain up direction\n    target_face = np.array([1, 0, 0]) # maintain initial face direction\n    target_side = np.array([0, 1, 0]) # maintain initial side direction\n    target_ang = np.array([0, 0, 0.0]) # don't let the robot spin\n    alpha_vel = 5.0\n    alpha_ang = 1.0\n    alpha_face = 1.0\n    alpha_up = 1.0\n    alpha_side = 1.0\n    alpha_height = 10.0\n    r_vel    = - alpha_vel * np.linalg.norm(COM_vel - target_v)\n    r_ang    = - alpha_ang * np.linalg.norm(COM_ang - target_ang)\n    r_face   = - alpha_face * np.linalg.norm(face_dir - target_face)"
        },
        {
            "comment": "This code calculates the reward for a walking skill. It combines various penalties based on velocity, angle, rotation axes deviation from target values, and an energy term that discourages excessive energy consumption. The reward is then returned after including the energy term.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/prompt_locomotion.py\":101-123",
            "content": "    r_up     = - alpha_up * np.linalg.norm(up_dir - target_up)\n    r_side   = - alpha_side * np.linalg.norm(side_dir - target_side)\n    r_height = - alpha_height * np.linalg.norm(COM_pos[2] - target_height)\n    r = r_vel + r_ang + r_face + r_up + r_side + r_height\n    # there is a default energy term that penalizes the robot for consuming too much energy. This should be included for all skill.\n    r_energy = get_energy_reward(self)\n    return r + r_energy\n```\n\"\"\",\n\"\"\"\nSkill: walk forward\nReward:\n```python\ndef _compute_reward(self):\n    # we first get some information of the quadruped/humanoid.\n    # COM_pos and COM_quat are the position and orientation (quaternion) of the center of mass of the quadruped/humanoid.\n    COM_pos, COM_quat = get_robot_pose(self)\n    # COM_vel, COM_ang are the velocity and angular velocity of the center of mass of the quadruped/humanoid.\n    COM_vel, COM_ang = get_robot_velocity(self)\n    # face_dir, side_dir, and up_dir are three axes of the rotation of the quadruped/humanoid."
        },
        {
            "comment": "This code initializes the face, side, and up directions for a robot based on its center of mass (COM) and uses these directions to set target velocities and heights. The target velocity is set as [1.0, 0, 0] since the robot faces along the x-axis initially, and it keeps the original height while walking to prevent falling down.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/prompt_locomotion.py\":124-133",
            "content": "    # face direction points from the center of mass towards the face direction of the quadruped/humanoid.\n    # side direction points from the center of mass towards the side body direction of the quadruped/humanoid.\n    # up direction points from the center of mass towards up, i.e., the negative direction of the gravity. \n    # gravity direction is [0, 0, -1].\n    # when initialized, the face of the robot is along the x axis, the side of the robot is along the y axis, and the up of the robot is along the z axis.\n    face_dir, side_dir, up_dir = get_robot_direction(self, COM_quat)\n    # a skill can be catergorized by target velocity, target body height, target up/side/face direction, as well as target angular velocity of the quadruped/humanoid. \n    target_v = np.array([1.0, 0, 0]) # since the robot faces along x axis initially, for walking forward, the target velocity would just be [1, 0, 0]\n    target_height = self.COM_init_pos[2] # we want the robot to keep the original height when walkin, so it does not fall down."
        },
        {
            "comment": "The code defines the target positions (face, side, up) and angular velocity for a robot, and uses coefficients to control each term. These auxiliary terms ensure stability during the RL exploration phase. The target velocity, height, face, and up direction are controlled for walking.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/prompt_locomotion.py\":134-146",
            "content": "    target_face = np.array([1, 0, 0]) # the target_face keeps the robot facing forward.\n    target_side = np.array([0, 1, 0]) # for walking forward, the side direction does not really matter.\n    target_up = np.array([0, 0, 1]) # the target_up keeps the robot standing up.\n    target_ang = np.array([0, 0, 0]) # for walking forward, the angular velocity does not really matter.\n    # note in this example, the real goal can be specified using only 1 term, i.e., the target velocity being [1, 0, 0].\n    # howeever, to make the learning reliable, we need the auxiliary terms such as target_height, target_face, and target_up terms to keep the quadruped/humanoid stable during the RL exploration phase.\n    # you should try to keep these auxiliary terms for stability as well when desigining the reward.\n    # we use these coefficients to turn on/off and weight each term. For walking, we only control the target velocity, height, and face and up direction.\n    alpha_vel = 1.0\n    alpha_height = 1.0\n    alpha_face = 1.0"
        },
        {
            "comment": "This code calculates a reward for a robot's movement based on its velocity, height, facing direction, sideways movement, upwards movement, and angular position relative to the target values. It also includes an energy term that penalizes the robot for consuming too much energy.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/prompt_locomotion.py\":147-180",
            "content": "    alpha_side = 0.0\n    alpha_up = 1.0\n    alpha_ang = 0.0\n    r_vel    = - alpha_vel * np.linalg.norm(COM_vel - target_v)\n    r_height = - alpha_height * np.linalg.norm(COM_pos[2] - target_height)\n    r_face   = - alpha_face * np.linalg.norm(face_dir - target_face)\n    r_side    = - alpha_side * np.linalg.norm(side_dir - target_side)\n    r_up     = - alpha_up * np.linalg.norm(up_dir - target_up)\n    r_ang    = - alpha_ang * np.linalg.norm(COM_ang - target_ang)\n    r = r_vel + r_height + r_face + r_side + r_up + r_ang\n    # there is a default energy term that penalizes the robot for consuming too much energy. This should be included for all skill.\n    r_energy = get_energy_reward(self)\n    return r + r_energy\n``` \n\"\"\", \n]\nassistant_contents = []\nreward_file_header1 = \"\"\"\nfrom locomotion.sim import SimpleEnv\nimport numpy as np\nimport gym\nfrom locomotion.gpt_reward_api import *\nclass {}(SimpleEnv):\n\"\"\"\nreward_file_header2 = \"\"\"\n    def __init__(self, task_name, *args, **kwargs):\n        super().__init__(*args, **kwargs)"
        },
        {
            "comment": "This code generates locomotion tasks using a GPT model. It saves the generated tasks in a specified directory, creates subdirectories if they do not exist, and samples 2 good examples from a list of examples.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/prompt_locomotion.py\":181-221",
            "content": "        self.task_name = task_name\n        self.detected_position = {}\n\"\"\"\nreward_file_end = \"\"\"\ngym.register(\n    id='gym-{}-v0',\n    entry_point={},\n)\n\"\"\"\nimport time\nimport datetime\nimport numpy as np\nimport copy\nfrom gpt_4.query import query\nimport yaml\nimport json\ndef generate_task_locomotion(model_dict, temperature_dict, meta_path='generated_tasks_locomotion'):\n    system = \"You are a helpful assistant.\"\n    ts = time.time()\n    time_string = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n    save_path = \"data/{}/{}\".format(meta_path, \"locomotion_\" + time_string)\n    print(\"=\" * 30)\n    print(\"querying GPT to imagine the tasks\")\n    print(\"=\" * 30)\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n    if not os.path.exists(save_path + \"/gpt_response\"):\n        os.makedirs(save_path + \"/gpt_response\")\n    sampled_good_examples = np.random.choice(good_exmaples, 2, replace=False)\n    sampled_good_examples = \"\\n\".join(sampled_good_examples)\n    new_user_contents = []\n    copied_user_contents = copy.deepcopy(user_contents)"
        },
        {
            "comment": "This code is parsing and extracting tasks from a generated response, along with their corresponding rewards. It starts by appending formatted contents to new_user_contents, then queries a model to generate a task generation response. The response is split into lines, and tasks are identified using the marker \"skill:\". Rewards are extracted until reaching the next code block marked with \"```python\", which serves as the end of each task definition.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/prompt_locomotion.py\":222-245",
            "content": "    new_user_contents.append(copied_user_contents[0].format(sampled_good_examples))\n    task_save_path = os.path.join(save_path, \"gpt_response/task_generation.json\")\n    response = query(system, new_user_contents, assistant_contents, save_path=task_save_path, \n                     model=model_dict['task_generation'], temperature=temperature_dict[\"task_generation\"])\n    response = response.split(\"\\n\")\n    tasks = []\n    rewards = []\n    for l_idx, line in enumerate(response):\n        line = line.lower()\n        if \"skill:\" in line.lower():\n            tasks.append(response[l_idx])\n            reward = []\n            start_idx = l_idx + 1\n            for l_idx_2 in range(start_idx, len(response)):\n                if \"```python\" in response[l_idx_2].lower():\n                    start_idx = l_idx_2 + 1\n                    break\n            for l_idx_2 in range(start_idx, len(response)):\n                if response[l_idx_2].startswith(\"```\"):\n                    break\n                reward.append(response[l_idx_2])"
        },
        {
            "comment": "Code adds rewards to each task, formats them into a file, and appends it to the solution path.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/prompt_locomotion.py\":247-273",
            "content": "            reward[0] = \"    \" + reward[0]\n            for idx in range(1, len(reward)):\n                reward[idx] = \"        \" + reward[idx]\n            reward = \"\\n\".join(reward)\n            rewards.append(reward)\n    generated_task_configs = []\n    for task, reward in zip(tasks, rewards):\n        print(\"task is: \", task)\n        task_description = task.split(\": \")[1]\n        description = f\"{task_description}\".replace(\" \", \"_\").replace(\".\", \"\").replace(\",\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n        solution_path = os.path.join(save_path, \"task_{}\".format(description))\n        if not os.path.exists(solution_path):\n            os.makedirs(solution_path)\n        reward_file_name = os.path.join(solution_path, f\"{description}.py\")\n        header = reward_file_header1.format(description)\n        end = reward_file_end.format(description, description)   \n        file_content =  header + reward_file_header2 + reward + end\n        with open(reward_file_name, \"w\") as f:\n            f.write(file_content)\n        config_path = save_path"
        },
        {
            "comment": "This code saves a task configuration file in YAML format. It takes a description and solution path as inputs, creates a save name, constructs the full paths for the task and solution files, writes the parsed YAML data to both files, and adds the generated task config path to a list before returning it.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/gpt_4/prompts/prompt_locomotion.py\":274-285",
            "content": "        save_name =  description + '.yaml'\n        task_config_path = os.path.join(config_path, save_name)\n        parsed_yaml = []\n        parsed_yaml.append(dict(solution_path=solution_path))\n        with open(task_config_path, 'w') as f:\n            yaml.dump(parsed_yaml, f, indent=4)\n        with open(os.path.join(solution_path, \"config.yaml\"), 'w') as f:\n            yaml.dump(parsed_yaml, f, indent=4)\n        generated_task_configs.append(task_config_path)\n    return generated_task_configs"
        }
    ]
}