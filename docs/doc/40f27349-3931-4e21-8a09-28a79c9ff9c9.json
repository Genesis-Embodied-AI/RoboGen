{
    "summary": "The CEMOptimizer class optimizes cost functions using the CEM algorithm, and sets up a reinforcement learning algorithm in assistive gym environments. It saves states as pickle files with highest protocol, converts numpy arrays to gifs, disconnects from physics clients.",
    "details": [
        {
            "comment": "The code defines a class called \"CEMOptimizer\" with an initializer that takes in parameters like cost_function, solution_dim, plan_n_segs, max_iters, population_size, num_elites, upper_bound, lower_bound and epsilon. The class aims to optimize a given cost function in a multi-dimensional space using a CEM (Covariance Matrix Adaptation Evolution Strategy) algorithm with specified parameters.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/execute_locomotion.py\":0-24",
            "content": "import time\nimport datetime\nimport copy\nimport os, sys, shutil, argparse\nimport pickle\nimport numpy as np\nimport scipy.stats as stats\nfrom tqdm import tqdm\nfrom cem_policy.parallel_worker import ParallelRolloutWorker\nimport os\nimport pybullet as p\nfrom cem_policy.utils import *\nclass CEMOptimizer(object):\n    def __init__(self, cost_function, solution_dim, plan_n_segs, max_iters, population_size, num_elites,\n                 upper_bound=None, lower_bound=None, epsilon=0.05):\n        \"\"\"\n        :param cost_function: Takes input one or multiple data points in R^{sol_dim}\\\n        :param solution_dim: The dimensionality of the problem space\n        :param max_iters: The maximum number of iterations to perform during optimization\n        :param population_size: The number of candidate solutions to be sampled at every iteration\n        :param num_elites: The number of top solutions that will be used to obtain the distribution\n                            at the next iteration.\n        :param upper_bound: An array of upper bounds for the sampled data points"
        },
        {
            "comment": "This code defines a class for optimizing a cost function using genetic algorithm. It takes parameters like lower and upper bounds, epsilon (minimum variance), maximum iterations, population size, number of elites, and cost function as inputs. The obtain_solution method is used to optimize the cost function with the provided initial candidate distribution in the current state of the environment.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/execute_locomotion.py\":25-43",
            "content": "        :param lower_bound: An array of lower bounds for the sampled data points\n        :param epsilon: A minimum variance. If the maximum variance drops below epsilon, optimization is stopped.\n        \"\"\"\n        super().__init__()\n        self.solution_dim, self.max_iters, self.population_size, self.num_elites = \\\n            solution_dim, max_iters, population_size, num_elites\n        self.plan_n_segs = plan_n_segs\n        self.ub, self.lb = upper_bound.reshape([1, solution_dim]), lower_bound.reshape([1, solution_dim])\n        self.epsilon = epsilon\n        if num_elites > population_size:\n            raise ValueError(\"Number of elites must be at most the population size.\")\n        self.cost_function = cost_function\n    def obtain_solution(self, cur_state, init_mean=None, init_var=None):\n        \"\"\" Optimizes the cost function using the provided initial candidate distribution\n        :param cur_state: Full state of the current environment such that the environment can always be reset to this state"
        },
        {
            "comment": "This function initializes the candidate distribution mean and variance, then iterates through a CEM process to find solutions with lower costs. It clips samples within bounds, tiles them into multiple segments, calculates costs using cost_function, selects elites based on cost, and continues until maximum iterations or epsilon threshold is met.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/execute_locomotion.py\":44-63",
            "content": "        :param init_mean: (np.ndarray) The mean of the initial candidate distribution.\n        :param init_var: (np.ndarray) The variance of the initial candidate distribution.\n        :return:\n        \"\"\"\n        mean = (self.ub + self.lb) / 2. if init_mean is None else init_mean\n        var = (self.ub - self.lb) / 4. if init_var is None else init_var\n        t = 0\n        X = stats.norm(loc=np.zeros_like(mean), scale=np.ones_like(mean))\n        while (t < self.max_iters):  # and np.max(var) > self.epsilon:\n            print(\"inside CEM, iteration {}\".format(t))\n            samples = X.rvs(size=[self.population_size, self.solution_dim]) * np.sqrt(var) + mean\n            samples = np.clip(samples, self.lb, self.ub)\n            full_samples = np.tile(samples, [1, self.plan_n_segs])\n            costs_ = self.cost_function(cur_state, full_samples)\n            costs = [_[0] for _ in costs_]\n            print(np.mean(costs), np.min(costs))\n            sort_costs = np.argsort(costs)\n            elites = samples[sort_costs][:self.num_elites]"
        },
        {
            "comment": "This code calculates the mean of elites and updates variance. It returns the mean solution and variance, tiled to match the plan's number of segments. The CEMPolicy class initializes environment variables, use_mpc flag, plan horizon, plan segments, action buffer, rollout worker, and action space bounds.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/execute_locomotion.py\":64-86",
            "content": "            mean = np.mean(elites, axis=0)\n            var *= 0.2\n            t += 1\n        sol, solvar = mean, var\n        sol = np.tile(sol, self.plan_n_segs)\n        solvar = np.tile(solvar, [1, self.plan_n_segs])\n        return sol\nclass CEMPolicy(object):\n    \"\"\" Use the ground truth dynamics to optimize a trajectory of actions. \"\"\"\n    def __init__(self, env, env_class, env_kwargs, use_mpc, plan_horizon, plan_n_segs, max_iters, population_size, num_elites):\n        self.env, self.env_class, self.env_kwargs = env, env_class, env_kwargs\n        self.use_mpc = use_mpc\n        self.plan_horizon, self.action_dim = plan_horizon, len(env.action_space.sample())\n        self.plan_n_segs = plan_n_segs\n        self.action_buffer = []\n        self.prev_sol = None\n        self.rollout_worker = ParallelRolloutWorker(env_class, env_kwargs, plan_horizon, self.action_dim)\n        lower_bound = np.tile(env.action_space.low[None], [int(self.plan_horizon / self.plan_n_segs), 1]).flatten()\n        upper_bound = np.tile(env.action_space.high[None], [int(self.plan_horizon / self.plan_n_segs), 1]).flatten()"
        },
        {
            "comment": "This code initializes an optimizer for generating locomotion actions, resets the previous solution, and obtains a new solution from the optimizer when necessary. The optimizer uses a cost function, population size, number of elites, and lower and upper bounds to generate solutions.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/execute_locomotion.py\":87-108",
            "content": "        self.optimizer = CEMOptimizer(self.rollout_worker.cost_function,\n                                      int(self.plan_horizon * self.action_dim / self.plan_n_segs),\n                                      self.plan_n_segs,\n                                      max_iters=max_iters,\n                                      population_size=population_size,\n                                      num_elites=num_elites,\n                                      lower_bound=lower_bound,\n                                      upper_bound=upper_bound, )\n    def reset(self):\n        self.prev_sol = None\n    def get_action(self, state):\n        if len(self.action_buffer) > 0 and not self.use_mpc:\n            action, self.action_buffer = self.action_buffer[0], self.action_buffer[1:]\n            return action\n        env_state = save_env(self.env)\n        soln = self.optimizer.obtain_solution(env_state, self.prev_sol).reshape([-1, self.action_dim])\n        if self.use_mpc:\n            self.prev_sol = np.vstack([np.copy(soln)[1:, :], np.zeros([1, self.action_dim])]).flatten()"
        },
        {
            "comment": "This code is part of a reinforcement learning algorithm for assistive gym environments. It includes the initialization and planning process for a specific environment, as well as arguments for seed, rendering, and task configuration path. The code allows for training on different environments using various reinforcement learning algorithms.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/execute_locomotion.py\":109-132",
            "content": "        else:\n            self.prev_sol = None\n            self.action_buffer = soln[1:]  # self.action_buffer is only needed for the non-mpc case.\n        load_env(self.env, state=env_state)  # Recover the environment\n        print(\"cem finished planning!\")\n        return soln[0]\nif __name__ == '__main__':\n    import importlib\n    import yaml\n    parser = argparse.ArgumentParser(description='RL for Assistive Gym')\n    parser.add_argument('--env', default='open_the_dishwasher_door-v0',\n                        help='Environment to train on (default: open_the_dishwasher_door-v0)')\n    parser.add_argument('--algo', default='sac',\n                        help='Reinforcement learning algorithm')\n    parser.add_argument('--task_config_path', type=str, default=None)\n    parser.add_argument('--seed', type=int, default=1,\n                        help='Random seed (default: 1)')\n    parser.add_argument('--render', type=int, default=0,\n                        help='whether to use rendering (default: 0)')\n    args = parser.parse_args()"
        },
        {
            "comment": "This code loads and sets up a task for a robot using specified configuration. It imports the required module based on the provided solution path, creates an environment instance with the given configuration, and creates a copy of the configuration to use for CEM (Covariance Matrix Adaptation Evolution Strategy) configuration.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/execute_locomotion.py\":134-165",
            "content": "    time_string = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d-%H-%M-%S')\n    robot_name = np.random.choice(['anymal', \"a1\"])\n    horizon = 40\n    config={\n        'gui': args.render,\n        'task': None,\n        'robot_name': robot_name,\n        'frameskip': 10,\n        'frameskip_save': 2,\n        'horizon': horizon,\n    }\n    default_cem_kwargs = {\n        'use_mpc': False,\n        'plan_horizon': horizon,\n        'plan_n_segs': int(horizon/5),\n        'max_iters': 5,\n        'population_size': 6000,\n        'num_elites': 1,\n    }\n    # change this to be the specified task class\n    task_config = yaml.safe_load(open(args.task_config_path, 'r'))\n    solution_path = task_config[0]['solution_path']\n    task_name = solution_path.split(\"/\")[-1][5:]\n    module = importlib.import_module(\"{}.{}\".format(solution_path.replace(\"/\", \".\"), task_name))\n    config[\"task_name\"] = task_name\n    env_class = getattr(module, task_name)\n    env = env_class(**config)\n    cem_config = copy.deepcopy(config)\n    cem_config['gui'] = False"
        },
        {
            "comment": "This code initializes a CEMPolicy object for a given environment, then runs the policy to execute locomotion actions. It collects RGBS and states data at each step, saving it in lists. The code also calculates the total return from rewards. At the end, it creates a save path based on time, robot name, and total return, ensuring that the path doesn't already exist before using it for saving.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/execute_locomotion.py\":167-198",
            "content": "    policy = CEMPolicy(env,\n                        env_class,\n                        cem_config,\n                        use_mpc=default_cem_kwargs['use_mpc'],\n                        plan_horizon=default_cem_kwargs['plan_horizon'],\n                        plan_n_segs=default_cem_kwargs['plan_n_segs'],\n                        max_iters=default_cem_kwargs['max_iters'],\n                        population_size=default_cem_kwargs['population_size'],\n                        num_elites=default_cem_kwargs['num_elites'])\n    # Run policy\n    all_rbgs = []\n    all_states = []\n    all_return = []\n    obs = env.reset()\n    rgbs = []\n    states = []\n    ret = 0\n    done = False\n    for idx in range(env.horizon):\n        print(\"step {}\".format(idx))\n        action = policy.get_action(obs)\n        obs, reward, done, _, rgbs_, states_ = env.step_(action)\n        ret += reward\n        rgbs += rgbs_\n        states += states_\n    save_path=f\"{solution_path}/cem/{time_string}_{robot_name}_{ret:.3f}\"\n    if not os.path.exists(save_path):"
        },
        {
            "comment": "Creates directory for save_path, converts numpy array to gif using 'save_numpy_as_gif', saves states as pickle file with highest protocol, disconnects the environment from physics client.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/execute_locomotion.py\":199-202",
            "content": "        os.makedirs(save_path)\n    save_numpy_as_gif(np.array(rgbs), f\"{save_path}/result.mp4\", fps=60)\n    pickle.dump(states, open(f\"{save_path}/result.pkl\", 'wb'), pickle.HIGHEST_PROTOCOL)\n    p.disconnect(env.id)"
        }
    ]
}