{
    "summary": "The code features functions for loading OBJ files, handling GIFs, and downloading 3D objects with rendering and randomization options. It generates URDF files, performs physics simulation using PyBullet, and saves states for RoboGen.",
    "details": [
        {
            "comment": "This code imports various libraries and defines a function for normalizing the vertices of an OBJ file. It reads the OBJ file, extracts vertex coordinates, centers them, and scales them to have values between -1 and 1. This could be used in computer graphics or robotics applications where object normalization is required.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":0-37",
            "content": "import os\nimport yaml\nimport numpy as np\nfrom PIL import Image\nfrom moviepy.editor import ImageSequenceClip\nimport os.path as osp\nimport pybullet as p\nimport os\nimport json\nimport multiprocessing\nfrom multiprocessing import Process\nimport multiprocessing.pool\nimport pickle\nimport copy\nimport importlib\nfrom PIL import Image, ImageSequence\nimport multiprocessing\nimport objaverse\nimport trimesh\nfrom objaverse_utils.utils import text_to_uid_dict, partnet_mobility_dict, sapaien_cannot_vhacd_part_dict\ndefault_config = {\n    \"gui\": False,\n    \"use_suction\": True,\n    \"rotation_mode\": 'delta-axis-angle-local',\n}\ndef normalize_obj(obj_file_path):\n    vertices = []\n    with open(osp.join(obj_file_path), 'r') as f:\n        lines = f.readlines()\n        for line in lines:\n            if line.startswith(\"v \"):\n                vertices.append([float(x) for x in line.split()[1:]])\n    vertices = np.array(vertices).reshape(-1, 3)\n    vertices = vertices - np.mean(vertices, axis=0) # center to zero\n    vertices = vertices / np.max(np.linalg.norm(vertices, axis=1)) # normalize to -1, 1"
        },
        {
            "comment": "This code function appears to be part of a larger program that manages 3D object data. The main function, \"down_load_single_object\", takes an object name and possibly a list of UIDs (unique identifiers). If no UID list is provided, it searches for the UID related to the object name. It then iterates through each UID, checks if the corresponding save path exists, and seems to be related to loading 3D object data from those paths. The code snippet also includes operations involving vertex manipulation such as reading files, writing normalized vertices to a new file, and printing out save paths for debugging purposes.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":39-62",
            "content": "    with open(osp.join(obj_file_path.replace(\".obj\", \"_normalized.obj\")), 'w') as f:\n        vertex_idx = 0\n        for line in lines:\n            if line.startswith(\"v \"):\n                line = \"v \" + \" \".join([str(x) for x in vertices[vertex_idx]]) + \"\\n\"\n                vertex_idx += 1\n            f.write(line)\ndef down_load_single_object(name, uids=None, candidate_num=5, vhacd=True, debug=False, task_name=None, task_description=None):\n    if uids is None:\n        if name in text_to_uid_dict:\n            uids = text_to_uid_dict[name]\n        else:\n            from objaverse_utils.find_uid_utils import find_uid\n            uids = find_uid(name, candidate_num=candidate_num, debug=debug, task_name=task_name, task_description=task_description)\n            if uids is None:\n                return False\n    processes = multiprocessing.cpu_count()\n    for uid in uids:\n        save_path = osp.join(\"objaverse_utils/data/obj\", \"{}\".format(uid))\n        print(\"save_path is: \", save_path)\n        if not osp.exists(save_path):"
        },
        {
            "comment": "Creates directory, checks if URDF file exists, loads object, exports mesh, normalizes size, and runs vhacd if enabled.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":63-93",
            "content": "            os.makedirs(save_path)\n        if osp.exists(save_path + \"/material.urdf\"):\n            continue\n        objects = objaverse.load_objects(\n            uids=[uid],\n            download_processes=processes\n        )\n        test_obj = (objects[uid])\n        scene = trimesh.load(test_obj)\n        try:\n            trimesh.exchange.export.export_mesh(\n                scene, osp.join(save_path, \"material.obj\")\n            )\n        except:\n            print(\"cannot export obj for uid: \", uid)\n            uids.remove(uid)\n            if uid in text_to_uid_dict[name]:\n                text_to_uid_dict[name].remove(uid)\n            continue\n        # we need to further parse the obj to normalize the size to be within -1, 1\n        if not osp.exists(osp.join(save_path, \"material_normalized.obj\")):\n            normalize_obj(osp.join(save_path, \"material.obj\"))\n        # we also need to parse the obj to vhacd\n        if vhacd:\n            if not osp.exists(osp.join(save_path, \"material_normalized_vhacd.obj\")):\n                run_vhacd(save_path)"
        },
        {
            "comment": "This function downloads and parses a 3D object from a given YAML configuration file. It first reads the YAML file, then iterates over each object in the file to find the relevant one with type 'mesh'. If the object is found, it attempts to download the object, specifying the number of candidates to download (default: 10) and whether to use V-HACD algorithm for meshing. The function returns True if successful in downloading and parsing the mesh.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":95-120",
            "content": "        # for pybullet, we have to additionally parse it to urdf\n        obj_to_urdf(save_path, scale=1, vhacd=vhacd) \n    return True\ndef download_and_parse_objavarse_obj_from_yaml_config(config_path, candidate_num=10, vhacd=True):\n    config = None\n    while config is None:\n        with open(config_path, 'r') as file:\n            config = yaml.safe_load(file)\n    task_name = None\n    task_description = None\n    for obj in config:\n        if 'task_name' in obj.keys():\n            task_name = obj['task_name']\n            task_description = obj['task_description']\n            break\n    for obj in config:\n        if 'type' in obj.keys() and obj['type'] == 'mesh' and 'uid' not in obj.keys():\n            print(\"{} trying to download object: {} {}\".format(\"=\" * 20, obj['lang'], \"=\" * 20))\n            success = down_load_single_object(obj[\"lang\"], candidate_num=candidate_num, vhacd=vhacd, \n                                              task_name=task_name, task_description=task_description)\n            if not success:"
        },
        {
            "comment": "This code defines functions for loading GIF images, building the environment, and potentially returning the environment class. It handles configuring the task based on a given task configuration file and can save the configuration to a specified file. The code also includes options for rendering, randomizing, and specifying the action space and object ID.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":121-146",
            "content": "                print(\"failed to find suitable object to download {} quit building this task\".format(obj[\"lang\"]))\n                return False\n            obj['uid'] = text_to_uid_dict[obj[\"lang\"]]\n            obj['all_uid'] = text_to_uid_dict[obj[\"lang\"] + \"_all\"]\n            with open(config_path, 'w') as f:\n                yaml.dump(config, f, indent=4)\n    return True\ndef load_gif(gif_path):\n    img = Image.open(gif_path)\n    # Extract each frame from the GIF and convert to RGB\n    frames = [frame.convert('RGB') for frame in ImageSequence.Iterator(img)]\n    # Convert each frame to a numpy array\n    frames_arrays = [np.array(frame) for frame in frames]\n    return frames_arrays\ndef build_up_env(task_config, solution_path, task_name, restore_state_file, return_env_class=False, \n                    action_space='delta-translation', render=False, randomize=False, \n                    obj_id=0,\n                ):\n    save_config = copy.deepcopy(default_config)\n    save_config['config_path'] = task_config\n    save_config['task_name'] = task_name"
        },
        {
            "comment": "The code imports and initializes a specific environment class for a given task, using the saved configuration. It returns the environment, save_config, and optionally the environment class itself. The NonDaemonPool is used to create non-daemon processes in multiprocessing.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":147-177",
            "content": "    save_config['restore_state_file'] = restore_state_file\n    save_config['translation_mode'] = action_space\n    save_config['gui'] = render\n    save_config['randomize'] = randomize\n    save_config['obj_id'] = obj_id\n    ### you might want to restore to a specific state\n    module = importlib.import_module(\"{}.{}\".format(solution_path.replace(\"/\", \".\"), task_name))\n    env_class = getattr(module, task_name)\n    env = env_class(**save_config)\n    if not return_env_class:\n        return env, save_config\n    else:\n        return env, save_config, env_class\nclass NonDaemonPool(multiprocessing.pool.Pool):\n    def Process(self, *args, **kwds):\n        proc = super(NonDaemonPool, self).Process(*args, **kwds)\n        class NonDaemonProcess(proc.__class__):\n            \"\"\"Monkey-patch process to ensure it is never daemonized\"\"\"\n            @property\n            def daemon(self):\n                return False\n            @daemon.setter\n            def daemon(self, val):\n                pass\n        proc.__class__ = NonDaemonProcess"
        },
        {
            "comment": "The function `save_numpy_as_gif` creates a GIF file from a numpy array containing images. It ensures the file has a .gif extension, scales and resizes the clip, and requires moviepy library (current GitHub version) for creating the GIF.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":178-211",
            "content": "        return proc\ndef save_numpy_as_gif(array, filename, fps=20, scale=1.0):\n    \"\"\"Creates a gif given a stack of images using moviepy\n    Notes\n    -----\n    works with current Github version of moviepy (not the pip version)\n    https://github.com/Zulko/moviepy/commit/d4c9c37bc88261d8ed8b5d9b7c317d13b2cdf62e\n    Usage\n    -----\n    >>> X = randn(100, 64, 64)\n    >>> gif('test.gif', X)\n    Parameters\n    ----------\n    filename : string\n        The filename of the gif to write to\n    array : array_like\n        A numpy array that contains a sequence of images\n    fps : int\n        frames per second (default: 10)\n    scale : float\n        how much to rescale each image by (default: 1.0)\n    \"\"\"\n    # ensure that the file has the .gif extension\n    fname, _ = os.path.splitext(filename)\n    filename = fname + '.gif'\n    # copy into the color dimension if the images are black and white\n    if array.ndim == 3:\n        array = array[..., np.newaxis] * np.ones(3)\n    # make the moviepy clip\n    clip = ImageSequenceClip(list(array), fps=fps).resize(scale)"
        },
        {
            "comment": "The code snippet reads an OBJ file and writes a GIF, creates a GUI window for the generated image, and returns the clip. It also generates an ROS URDF model for the object based on the input OBJ file path and scale factor. The code checks if there is a .png file in the folder, then it creates a material section for the URDF based on whether a .png file exists or not.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":212-247",
            "content": "    clip.write_gif(filename, fps=fps)\n    return clip\ndef obj_to_urdf(obj_file_path, scale=1, vhacd=True, normalized=True, obj_name='material'):\n    header = \"\"\"<?xml version=\"1.0\" ?>\n<robot name=\"cube.urdf\">\n  <link name=\"baseLink\">\n    <contact>\n      <lateral_friction value=\"1.0\"/>\n      <rolling_friction value=\"0.0\"/>\n      <contact_cfm value=\"0.0\"/>\n      <contact_erp value=\"1.0\"/>\n    </contact>\n    <inertial>\n      <origin rpy=\"0 0 0\" xyz=\"0.0 0.02 0.0\"/>\n       <mass value=\".1\"/>\n       <inertia ixx=\"1\" ixy=\"0\" ixz=\"0\" iyy=\"1\" iyz=\"0\" izz=\"1\"/>\n    </inertial>\n\"\"\"\n    all_files = os.listdir(obj_file_path)\n    png_file = None\n    for x in all_files:\n        if x.endswith(\".png\"):\n            png_file = x\n            break\n    if png_file is not None:\n        material = \"\"\"\n         <material name=\"texture\">\n        <texture filename=\"{}\"/>\n      </material>\"\"\".format(osp.join(obj_file_path, png_file))        \n    else:\n        material = \"\"\"\n        <material name=\"yellow\">\n            <color rgba=\"1 1 0.4 1\"/>"
        },
        {
            "comment": "Code generates a URDF (Universal Robot Description Format) file for a robot's manipulation component. It creates visual and collision models, handles normalized and non-normalized versions of the model, and combines them into a single URDF file using provided parameters. The code utilizes file paths and scales to generate appropriate XML structure for the models.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":248-280",
            "content": "        </material>\n        \"\"\"\n    obj_file = \"{}.obj\".format(obj_name) if not normalized else \"{}_normalized.obj\".format(obj_name)\n    visual = \"\"\"\n    <visual>\n      <origin rpy=\"0 0 0\" xyz=\"0 0 0\"/>\n      <geometry>\n        <mesh filename=\"{}\" scale=\"{} {} {}\"/>\n      </geometry>\n      {}\n    </visual>\n    \"\"\".format(osp.join(obj_file_path, obj_file), scale, scale, scale, material)\n    if normalized:\n        collision_file = '{}_normalized_vhacd.obj'.format(obj_name) if vhacd else \"{}_normalized.obj\".format(obj_name)\n    else:\n        collision_file = '{}_vhacd.obj'.format(obj_name) if vhacd else \"{}.obj\".format(obj_name)\n    collision = \"\"\"\n    <collision>\n      <origin rpy=\"0 0 0\" xyz=\"0 0 0\"/>\n      <geometry>\n             <mesh filename=\"{}\" scale=\"{} {} {}\"/>\n      </geometry>\n    </collision>\n  </link>\n  </robot>\n  \"\"\".format(osp.join(obj_file_path, collision_file), scale, scale, scale)\n    urdf =  \"\".join([header, visual, collision])\n    with open(osp.join(obj_file_path, \"{}.urdf\".format(obj_name)), 'w') as f:"
        },
        {
            "comment": "The code contains functions for running VHACD (Voronoi-based Hierarchical Algorithm for Collaborative Decomposition) on OBJ files, reading center data from strings, and connecting to the PyBullet physics engine. The run_vhacd function normalizes and runs VHACD on an OBJ file, while parse_center parses a string representing coordinates as a center point. The run_vhacd_with_timeout function connects to the PyBullet engine and runs VHACD with a timeout.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":281-306",
            "content": "        f.write(urdf)\ndef run_vhacd(input_obj_file_path, normalized=True, obj_name=\"material\"):\n    p.connect(p.DIRECT)\n    if normalized:\n        name_in = os.path.join(input_obj_file_path, \"{}_normalized.obj\".format(obj_name))\n        name_out = os.path.join(input_obj_file_path, \"{}_normalized_vhacd.obj\".format(obj_name))\n        name_log = os.path.join(input_obj_file_path, \"log.txt\")\n    else:\n        name_in = os.path.join(input_obj_file_path, \"{}.obj\".format(obj_name))\n        name_out = os.path.join(input_obj_file_path, \"{}_vhacd.obj\".format(obj_name))\n        name_log = os.path.join(input_obj_file_path, \"log.txt\")\n    p.vhacd(name_in, name_out, name_log)\ndef parse_center(center):   \n    if center.startswith(\"(\") or center.startswith(\"[\"):\n        center = center[1:-1]\n    center = center.split(\",\")\n    center = [float(x) for x in center]\n    return np.array(center)\ndef run_vhacd_with_timeout(args):\n    name_in, name_out, name_log, urdf_file_path, obj_file_name = args\n    id = p.connect(p.DIRECT)\n    proc = Process(target=p.vhacd, args=(name_in, name_out, name_log))"
        },
        {
            "comment": "This code appears to execute a preprocessing step on an URDF file, potentially involving multiple threads. The code first reads the lines of the URDF file and separates them into two lists - one containing lines with \"<collision>\" tags, the other containing all other lines. Then it seems to process the lines in the second list using some number of processes (specified by `num_processes`). If any of these processing steps take too long, they are killed and the file path is added to a dictionary of files that cannot be processed by VHACD. Finally, if all processing runs without interruption, it returns True, indicating successful preprocessing of the URDF file.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":308-350",
            "content": "    proc.start()\n    # Wait for 10 seconds or until process finishes\n    proc.join(200)\n    # If thread is still active\n    if proc.is_alive():\n        print(\"running too long... let's kill it...\")\n        # Terminate\n        proc.kill()\n        proc.join()\n        if urdf_file_path not in sapaien_cannot_vhacd_part_dict.keys():\n            sapaien_cannot_vhacd_part_dict[urdf_file_path] = []\n        sapaien_cannot_vhacd_part_dict[urdf_file_path].append(obj_file_name)\n        p.disconnect(id)\n        return False\n    else:\n        print(\"process finished\")\n        p.disconnect(id)\n        return True\ndef preprocess_urdf(urdf_file_path, num_processes=6):\n    new_lines = []\n    with open(urdf_file_path, 'r') as f:\n        lines = f.readlines()\n    num_lines = len(lines)\n    l_idx = 0\n    to_process_args = []\n    while l_idx < num_lines:\n        line_1 = lines[l_idx]\n        if \"<collision>\" in line_1:\n            new_lines.append(line_1)\n            for l_idx_2 in range(l_idx + 1, num_lines):\n                line_2 = lines[l_idx_2]"
        },
        {
            "comment": "Code reads .obj file names from urdf files and checks if they already have the \"_vhacd.obj\" suffix. If not, it adds them to a list of objects to process. If already processed, it simply updates the line with the new name.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":352-367",
            "content": "                if \".obj\" in line_2:\n                    start_idx = line_2.find('filename=\"') + len('filename=\"')\n                    end_idx = line_2.find('.obj') + len('.obj')\n                    obj_file_name = line_2[start_idx:end_idx]\n                    obj_file_path = osp.join(osp.dirname(urdf_file_path), obj_file_name)\n                    # import pdb; pdb.set_trace()\n                    name_in = obj_file_path\n                    name_out = obj_file_path[:-4] + \"_vhacd.obj\"\n                    name_log = obj_file_path[:-4] + \"_log.txt\"\n                    if not osp.exists(name_out) and obj_file_name not in sapaien_cannot_vhacd_part_dict.get(urdf_file_path, []):\n                        to_process_args.append([name_in, name_out, name_log, urdf_file_path, obj_file_name])\n                        new_lines.append(\"to_be_processed, {}\".format(line_2))\n                    else:\n                        new_name = line_2.replace(obj_file_name, obj_file_name[:-4] + '_vhacd.obj')\n                        new_lines.append(new_name)"
        },
        {
            "comment": "The code performs collision detection and processing in parallel using a non-daemon thread pool. It replaces the file names of objects to be processed with \"_vhacd.obj\" if the processing is successful, and updates the URDF file name as \"_vhacd.urdf\". The code breaks when encountering \"</collision>\" tag and appends lines accordingly.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":369-398",
            "content": "                elif \"</collision>\" in line_2:\n                    new_lines.append(line_2)\n                    l_idx = l_idx_2 \n                    break\n                else:\n                    new_lines.append(line_2)\n        else:\n            new_lines.append(line_1)\n        l_idx += 1\n    # do vhacd in parallel, each has a timeout of 200 seconds\n    with NonDaemonPool(processes=num_processes) as pool: \n        results = pool.map(run_vhacd_with_timeout, to_process_args)\n    processed_idx = 0\n    for l_idx in range(len(new_lines)):\n        if \"to_be_processed\" in new_lines[l_idx]:\n            if results[processed_idx]:\n                new_name = new_lines[l_idx].replace(\"to_be_processed, \", \"\")\n                new_name = new_name.replace(\".obj\", \"_vhacd.obj\")\n                new_lines[l_idx] = new_name\n            else:\n                new_name = new_lines[l_idx].replace(\"to_be_processed, \", \"\")\n                new_lines[l_idx] = new_name\n            processed_idx += 1\n    new_path = urdf_file_path.replace(\".urdf\", \"_vhacd.urdf\")    "
        },
        {
            "comment": "This code defines a function parse_config that parses a configuration file. It extracts various information such as urdf paths, sizes, locations, names, types, and more. The use of a table is determined and joint angles for articulated objects are stored in a dictionary. Spatial relationships are also extracted if present.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":399-433",
            "content": "    with open(new_path, 'w') as f:\n        f.writelines(\"\".join(new_lines))\n    with open(\"data/sapien_cannot_vhacd_part.json\", 'w') as f:\n        json.dump(sapaien_cannot_vhacd_part_dict, f, indent=4)\n    return new_path\ndef parse_config(config, use_bard=True, obj_id=None, use_gpt_size=True, use_vhacd=True):\n    urdf_paths = []\n    urdf_sizes = []\n    urdf_locations = []\n    urdf_names = []\n    urdf_types = []\n    urdf_on_tables = []\n    urdf_movables = []\n    use_table = False\n    articulated_joint_angles = {}\n    spatial_relationships = []\n    distractor_config_path = None\n    for obj in config:\n        print(obj)\n        if \"use_table\" in obj.keys():\n            use_table = obj['use_table']\n        if \"set_joint_angle_object_name\" in obj.keys():\n            new_obj = copy.deepcopy(obj)\n            new_obj.pop('set_joint_angle_object_name')\n            articulated_joint_angles[obj['set_joint_angle_object_name']] = new_obj\n        if \"spatial_relationships\" in obj.keys():\n            spatial_relationships = obj['spatial_relationships']"
        },
        {
            "comment": "This code checks for certain key attributes in the object. If 'task_name' or 'task_description' are present, it continues without doing anything. If 'distractor_config_path' is there, it assigns its value to 'distractor_config_path'. If 'type' isn't included, code will skip this iteration. If 'type' equals 'mesh', it checks for 'uid' and if 'obj_id' is None or not, assigns a random 'uid' from the list or one at 'obj_id' index respectively. Then, it forms the path for 'urdf_file_path' based on 'uid'. If the file doesn't exist, downloads the object and creates new 'urdf_lines' by reading the existing 'urdf_file_path', removing lines with 'vhacd' to create 'new_urdf_lines'.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":435-461",
            "content": "        if 'task_name' in obj.keys() or 'task_description' in obj.keys():\n            continue\n        if \"distractor_config_path\" in obj.keys():\n            distractor_config_path = obj['distractor_config_path']\n        if \"type\" not in obj.keys():\n            continue\n        if obj['type'] == 'mesh':\n            if 'uid' not in obj.keys():\n                continue\n            if obj_id is None:\n                uid = obj['uid'][np.random.randint(len(obj['uid']))]\n            else:\n                uid = obj['uid'][obj_id]\n            urdf_file_path = osp.join(\"objaverse_utils/data/obj\", \"{}\".format(uid), \"material.urdf\")\n            if not os.path.exists(urdf_file_path):\n                down_load_single_object(name=obj['lang'], uids=[uid])\n            new_urdf_file_path = urdf_file_path.replace(\"material.urdf\", \"material_non_vhacd.urdf\")\n            new_urdf_lines = []\n            with open(urdf_file_path, 'r') as f:\n                urdf_lines = f.readlines()\n            for line in urdf_lines:\n                if 'vhacd' in line:"
        },
        {
            "comment": "Code reads the \"type\" of object and based on it, either adds a random mesh file or reads an existing URDF file to update its path. If 'reward_asset_path' is not in object keys, a random object from possible_obj_path list is selected.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":462-486",
            "content": "                    new_line = line.replace(\"_vhacd\", \"\")\n                    new_urdf_lines.append(new_line)\n                else:\n                    new_urdf_lines.append(line)\n            with open(new_urdf_file_path, 'w') as f:\n                f.writelines(new_urdf_lines)\n            urdf_file_path = new_urdf_file_path\n            print(\"object {} choosing uid {} urdf_path {}\".format(obj['lang'], uid, urdf_file_path))\n            urdf_paths.append(urdf_file_path)\n            urdf_types.append('mesh')\n            urdf_movables.append(True) # all mesh objects are movable\n        elif obj['type'] == 'urdf':\n            try:\n                category = obj['lang']\n                possible_obj_path = partnet_mobility_dict[category]\n            except:\n                category = obj['name']\n                if category == 'Computer display':\n                    category = 'Display'\n                possible_obj_path = partnet_mobility_dict[category]\n            if 'reward_asset_path' not in obj.keys():\n                obj_path = np.random.choice(possible_obj_path)"
        },
        {
            "comment": "This code checks the category of an object and assigns corresponding OBJ path, then appends URDF file paths to a list depending on whether VHACD is used or not. By default, URDF objects are not movable unless specified.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":487-507",
            "content": "                if category == 'Toaster':\n                    obj_path = str(103486)\n                if category == 'Microwave':\n                    obj_path = str(7310)\n                if category == \"Oven\":\n                    obj_path = str(101808)\n                if category == 'Refrigerator':\n                    obj_path = str(10638)\n            else:\n                obj_path = obj['reward_asset_path']\n            urdf_file_path = osp.join(\"data/dataset\", obj_path, \"mobility.urdf\")\n            if use_vhacd:\n                new_urdf_file_path = urdf_file_path.replace(\"mobility.urdf\", \"mobility_vhacd.urdf\")\n                if not osp.exists(new_urdf_file_path):\n                    new_urdf_file_path = preprocess_urdf(urdf_file_path)\n                urdf_paths.append(new_urdf_file_path)\n            else:\n                urdf_paths.append(urdf_file_path)\n            urdf_types.append('urdf')\n            urdf_movables.append(obj.get('movable', False)) # by default, urdf objects are not movable, unless specified"
        },
        {
            "comment": "The code snippet creates a robot model in urdf format from object details. It appends size, location, name, and whether it's on a table to separate lists for each urdf attribute. The function then returns these attributes along with additional parameters.\nIn the second part of the code, it takes round images by iterating through different azimuth angles, calculating the view and projection matrices, and appending RGB and depth values for each image. It does this to generate a 3D environment view from various perspectives.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":509-532",
            "content": "        urdf_sizes.append(obj['size'])\n        urdf_locations.append(parse_center(obj['center']))\n        urdf_names.append(obj['name'])\n        urdf_on_tables.append(obj.get('on_table', False))\n    return urdf_paths, urdf_sizes, urdf_locations, urdf_names, urdf_types, urdf_on_tables, use_table, \\\n        articulated_joint_angles, spatial_relationships, distractor_config_path, urdf_movables\ndef take_round_images(env, center, distance, elevation=30, azimuth_interval=30, camera_width=640, camera_height=480,\n                        return_camera_matrices=False, z_near=0.01, z_far=10, save_path=None):\n    camera_target = center\n    delta_z = distance * np.sin(np.deg2rad(elevation))\n    xy_distance = distance * np.cos(np.deg2rad(elevation))\n    env_prev_view_matrix, env_prev_projection_matrix = env.view_matrix, env.projection_matrix\n    rgbs = []\n    depths = []\n    view_camera_matrices = []\n    project_camera_matrices = []\n    for azimuth in range(0, 360, azimuth_interval):\n        delta_x = xy_distance * np.cos(np.deg2rad(azimuth))"
        },
        {
            "comment": "This function takes a set of images around an object using a virtual camera. It calculates the camera position based on distance, azimuth angle, and elevation angle. Then, it sets up the camera and renders the image. The rendered images are stored in separate lists. If return_camera_matrices is False, it returns the list of rendered images and depths. Otherwise, it also returns the view and projection camera matrices.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":533-553",
            "content": "        delta_y = xy_distance * np.sin(np.deg2rad(azimuth))\n        camera_position = [camera_target[0] + delta_x, camera_target[1] + delta_y, camera_target[2] + delta_z]\n        env.setup_camera(camera_position, camera_target, \n                            camera_width=camera_width, camera_height=camera_height)\n        rgb, depth = env.render()\n        rgbs.append(rgb)\n        depths.append(depth)\n        view_camera_matrices.append(env.view_matrix)\n        project_camera_matrices.append(env.projection_matrix)\n    env.view_matrix, env.projection_matrix = env_prev_view_matrix, env_prev_projection_matrix\n    if not return_camera_matrices:\n        return rgbs, depths\n    else:\n        return rgbs, depths, view_camera_matrices, project_camera_matrices\ndef take_round_images_around_object(env, object_name, distance=None, save_path=None, azimuth_interval=30, \n                                    elevation=30, return_camera_matrices=False, camera_width=640, camera_height=480, \n                                    only_object=False):"
        },
        {
            "comment": "The code checks if only one object is specified. If so, it makes all other objects invisible and gets the AABB of the given object. Then, it calculates the camera target, distance, and takes a 3D panoramic image centered on the maximum extent of the object's bounding box.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":554-574",
            "content": "    if only_object:\n        ### make all other objects invisiable\n        prev_rgbas = []\n        object_id = env.urdf_ids[object_name]\n        for obj_name, obj_id in env.urdf_ids.items():\n            if obj_name != object_name:\n                num_links = p.getNumJoints(obj_id, physicsClientId=env.id)\n                for link_idx in range(-1, num_links):\n                    prev_rgba = p.getVisualShapeData(obj_id, link_idx, physicsClientId=env.id)[0][14:18]\n                    prev_rgbas.append(prev_rgba)\n                    p.changeVisualShape(obj_id, link_idx, rgbaColor=[0, 0, 0, 0], physicsClientId=env.id)\n    obj_id = env.urdf_ids[object_name]\n    min_aabb, max_aabb = env.get_aabb(obj_id)\n    camera_target = (max_aabb + min_aabb) / 2\n    if distance is None:\n        distance = np.linalg.norm(max_aabb - min_aabb) * 1.1\n    res = take_round_images(env, camera_target, distance, elevation=elevation, \n                             azimuth_interval=azimuth_interval, camera_width=camera_width, camera_height=camera_height, "
        },
        {
            "comment": "This code is used for manipulating object appearance and positioning the camera in a virtual environment. It changes the visual shape of objects, centers the camera at a specified object, and can save renderings. The code utilizes PyBullet physics engine to interact with the 3D environment.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":575-599",
            "content": "                             save_path=save_path, return_camera_matrices=return_camera_matrices)\n    if only_object:\n        cnt = 0\n        object_id = env.urdf_ids[object_name]\n        for obj_name, obj_id in env.urdf_ids.items():\n            if obj_name != object_name:\n                num_links = p.getNumJoints(obj_id, physicsClientId=env.id)\n                for link_idx in range(-1, num_links):\n                    p.changeVisualShape(obj_id, link_idx, rgbaColor=prev_rgbas[cnt], physicsClientId=env.id)\n                    cnt += 1\n    return res\ndef center_camera_at_object(env, object_name, distance=None, elevation=30, azimuth=0, camera_width=640, camera_height=480):\n    obj_id = env.urdf_ids[object_name]\n    min_aabb, max_aabb = env.get_aabb(obj_id)\n    camera_target = (max_aabb + min_aabb) / 2\n    if distance is None:\n        distance = np.linalg.norm(max_aabb - min_aabb) * 1.1\n    delta_z = distance * np.sin(np.deg2rad(elevation))\n    xy_distance = distance * np.cos(np.deg2rad(elevation))\n    delta_x = xy_distance * np.cos(np.deg2rad(azimuth))"
        },
        {
            "comment": "Calculates camera position and sets it up in the environment.\nDefines a grid with pixel coordinates and depth values from projection and view matrices.\nFilters out \"infinite\" depths if specified.\nTransforms pixel points to world coordinates.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":600-623",
            "content": "    delta_y = xy_distance * np.sin(np.deg2rad(azimuth))\n    camera_position = [camera_target[0] + delta_x, camera_target[1] + delta_y, camera_target[2] + delta_z]\n    env.setup_camera(camera_position, camera_target, \n                        camera_width=camera_width, camera_height=camera_height)\ndef get_pc(proj_matrix, view_matrix, depth, width, height, mask_infinite=False):\n    proj_matrix = np.asarray(proj_matrix).reshape([4, 4], order=\"F\")\n    view_matrix = np.asarray(view_matrix).reshape([4, 4], order=\"F\")\n    tran_pix_world = np.linalg.inv(np.matmul(proj_matrix, view_matrix))\n    # create a grid with pixel coordinates and depth values\n    y, x = np.mgrid[-1:1:2 / height, -1:1:2 / width]\n    y *= -1.\n    x, y, z = x.reshape(-1), y.reshape(-1), depth.reshape(-1)\n    h = np.ones_like(z)\n    pixels = np.stack([x, y, z, h], axis=1)\n    # filter out \"infinite\" depths\n    if mask_infinite:\n        pixels = pixels[z < 0.99]\n    pixels[:, 2] = 2 * pixels[:, 2] - 1\n    # turn pixels to world coordinates\n    points = np.matmul(tran_pix_world, pixels.T).T"
        },
        {
            "comment": "Code defines a setup function for the camera in a 3D simulation environment and extracts point cloud data from depth maps. The view matrix and projection matrix are computed based on camera parameters, and a perspective transformation is applied to the depth map to obtain the point cloud.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":624-651",
            "content": "    points /= points[:, 3: 4]\n    points = points[:, :3]\n    return points\ndef setup_camera_ben(client_id, camera_eye=[0.5, -0.75, 1.5], camera_target=[-0.2, 0, 0.75], camera_width=1920//4, camera_height=1080//4, \n                 z_near=0.01, z_far=100):\n    view_matrix = p.computeViewMatrix(camera_eye, camera_target, [0, 0, 1], physicsClientId=client_id)\n    focal_length = 450 # CAMERA_INTRINSICS[0, 0]\n    fov = (np.arctan((camera_height / 2) / focal_length) * 2 / np.pi) * 180\n    projection_matrix = p.computeProjectionMatrixFOV(fov, camera_width / camera_height, z_near, z_far, physicsClientId=client_id)\n    return view_matrix, projection_matrix\ndef get_pc_ben(depth, view_matrix, projection_matrix, znear, zfar):\n    height, width = depth.shape\n    CAMERA_INTRINSICS = np.array(\n        [\n            [450, 0, width / 2],\n            [0, 450, height / 2],\n            [0, 0, 1],\n        ]\n    )\n    T_CAMGL_2_CAM = np.array(\n        [\n            [1, 0, 0, 0],\n            [0, -1, 0, 0],\n            [0, 0, -1, 0],"
        },
        {
            "comment": "This code appears to be part of a function that calculates the 3D positions of points in world coordinates based on camera intrinsics and extrinsic parameters. It also includes functions for saving environment information and object joint details. This code could potentially be used in robotics applications where 3D positioning is important, such as computer vision or manipulation tasks.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":652-683",
            "content": "            [0, 0, 0, 1],\n        ]\n    )\n    depth = zfar + znear - (2.0 * depth - 1.0) * (zfar - znear)\n    depth = (2.0 * znear * zfar) / depth\n    height, width = depth.shape\n    xlin = np.linspace(0, width - 1, width)\n    ylin = np.linspace(0, height - 1, height)\n    px, py = np.meshgrid(xlin, ylin)\n    px = (px - CAMERA_INTRINSICS[0, 2]) * (depth / CAMERA_INTRINSICS[0, 0])\n    py = (py - CAMERA_INTRINSICS[1, 2]) * (depth / CAMERA_INTRINSICS[1, 1])\n    P_cam = np.float32([px, py, depth]).transpose(1, 2, 0).reshape(-1, 3)\n    T_camgl2world = np.asarray(view_matrix).reshape(4, 4).T\n    T_world2camgl = np.linalg.inv(T_camgl2world)\n    T_world2cam = T_world2camgl @ T_CAMGL_2_CAM\n    Ph_cam = np.concatenate([P_cam, np.ones((len(P_cam), 1))], axis=1)\n    Ph_world = (T_world2cam @ Ph_cam.T).T\n    P_world = Ph_world[:, :3]\n    return P_world\ndef save_env(env, save_path=None):\n    object_joint_angle_dicts = {}\n    object_joint_name_dicts = {}\n    object_link_name_dicts = {}\n    robot_name = env.robot_name\n    for obj_name, obj_id in env.urdf_ids.items():"
        },
        {
            "comment": "This code retrieves joint angles, names of the joints, and link names for each object in the environment. It also stores the base position and orientation of each object.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":684-702",
            "content": "        num_links = p.getNumJoints(obj_id, physicsClientId=env.id)\n        object_joint_angle_dicts[obj_name] = []\n        object_joint_name_dicts[obj_name] = []\n        object_link_name_dicts[obj_name] = []\n        for link_idx in range(0, num_links):\n            joint_angle = p.getJointState(obj_id, link_idx, physicsClientId=env.id)[0]\n            object_joint_angle_dicts[obj_name].append(joint_angle)\n            joint_name = p.getJointInfo(obj_id, link_idx, physicsClientId=env.id)[1].decode('utf-8')\n            object_joint_name_dicts[obj_name].append(joint_name)\n            link_name = p.getJointInfo(obj_id, link_idx, physicsClientId=env.id)[12].decode('utf-8')\n            object_link_name_dicts[obj_name].append(link_name)\n    object_base_position = {}\n    for obj_name, obj_id in env.urdf_ids.items():\n        object_base_position[obj_name] = p.getBasePositionAndOrientation(obj_id, physicsClientId=env.id)[0]\n    object_base_orientation = {}\n    for obj_name, obj_id in env.urdf_ids.items():\n        "
        },
        {
            "comment": "This code is storing object data and environment parameters in a dictionary called 'state'. It includes object joint angles, names, link names, base positions and orientations, activated status, suction information, urdf paths, object sizes, robot name, and optionally table path. The data is saved to the file specified by 'save_path' for future use.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":702-728",
            "content": "object_base_orientation[obj_name] = p.getBasePositionAndOrientation(obj_id, physicsClientId=env.id)[1]\n    activated = env.activated\n    suction_object_id = env.suction_obj_id\n    suction_contact_link = env.suction_contact_link\n    suction_to_obj_pose = env.suction_to_obj_pose\n    state = {\n        'object_joint_angle_dicts': object_joint_angle_dicts,\n        'object_joint_name_dicts': object_joint_name_dicts,\n        'object_link_name_dicts': object_link_name_dicts,\n        'object_base_position': object_base_position,\n        'object_base_orientation': object_base_orientation,     \n        'activated': activated,\n        'suction_object_id': suction_object_id,\n        'suction_contact_link': suction_contact_link,\n        'suction_to_obj_pose': suction_to_obj_pose,\n        \"urdf_paths\": copy.deepcopy(env.urdf_paths),\n        \"object_sizes\": env.simulator_sizes,\n        'robot_name': env.robot_name,\n    }\n    if env.use_table:\n        state['table_path'] = env.table_path\n    if save_path is not None:\n        with open(save_path, 'wb') as f:"
        },
        {
            "comment": "This code defines a function to load and restore the state of an environment, including object positions, orientations, joint angles, and suction. The function takes in an environment (env), a path to save or load from (load_path), and an optional state dictionary. It uses pickle to serialize and deserialize the state data. The function resets the environment based on the stored state by setting object positions, orientations, joint angles, and suction recovery.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":729-754",
            "content": "            pickle.dump(state, f, pickle.HIGHEST_PROTOCOL)\n    return state\ndef load_env(env, load_path=None, state=None):\n    if load_path is not None:\n        with open(load_path, 'rb') as f:\n            state = pickle.load(f)\n    ### set env to stored object position and orientation\n    for obj_name, obj_id in env.urdf_ids.items():\n        p.resetBasePositionAndOrientation(obj_id, state['object_base_position'][obj_name], state['object_base_orientation'][obj_name], physicsClientId=env.id)\n    ### set env to stored object joint angles\n    for obj_name, obj_id in env.urdf_ids.items():\n        num_links = p.getNumJoints(obj_id, physicsClientId=env.id)\n        for link_idx in range(0, num_links):\n            joint_angle = state['object_joint_angle_dicts'][obj_name][link_idx]\n            p.resetJointState(obj_id, link_idx, joint_angle, physicsClientId=env.id)\n    ### recover suction\n    env.activated = state['activated']\n    if state['activated']:\n        env.suction_obj_id = state['suction_object_id']\n        env.suction_contact_link = state['suction_contact_link']"
        },
        {
            "comment": "The code is part of RoboGen, a codebase for robot simulation and manipulation. It initializes the environment based on provided state data. The state contains information such as suction-to-object pose, URDF paths (urdf stands for Unified Robot Description Format), object sizes, robot name, and table path if applicable. The code also has functions for generating URDF files using vhacd and converting OBJ to URDF format.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/utils.py\":755-779",
            "content": "        env.suction_to_obj_pose = state['suction_to_obj_pose']\n        env.create_suction_constraint(env.suction_obj_id, env.suction_contact_link, env.suction_to_obj_pose)\n    if \"urdf_paths\" in state:\n        env.urdf_paths = state[\"urdf_paths\"]\n    if \"object_sizes\" in state:\n        env.simulator_sizes = state[\"object_sizes\"]\n    if \"robot_name\" in state:\n        env.robot_name = state[\"robot_name\"]\n    if \"table_path\" in state and env.use_table:\n        env.table_path = state[\"table_path\"]\n    return state\nif __name__ == '__main__':\n    path = \"/media/yufei/42b0d2d4-94e0-45f4-9930-4d8222ae63e5/yufei/projects/ibm/objaverse_utils/data/obj/6d9c1aa964be4f7881d89cd6b427296c____Small_house_with_wrecked_car\"\n    path = \"/media/yufei/42b0d2d4-94e0-45f4-9930-4d8222ae63e5/yufei/projects/ibm/objaverse_utils/data/obj/94ccd348a1424defaea6efcd1d3418a6____Plastic_monster_toy.\"\n    path = \"objaverse_utils/data/obj/0/006_mustard_bottle/tsdf/\"\n    run_vhacd(path, normalized=False, obj_name='textured')\n    res = obj_to_urdf(path, 1, vhacd=True, normalized=False, obj_name='textured')"
        }
    ]
}