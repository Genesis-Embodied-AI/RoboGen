{
    "summary": "This code imports necessary libraries and defines functions for RoboGen tasks, including distance computation, joint angle retrieval, gripper proximity checks using PyBullet library. It retrieves robot end effector info, provides functions for bounding box info, joint state, link position, grasp detection, joint value setting, bounds checking, and rendering CoM. This code snippet compares images to find differences, determines the best image for reward calculation, calculates a link's center of mass and point clouds for a given object, and retrieves joint indices from link or joint names in objects.",
    "details": [
        {
            "comment": "This code imports necessary libraries and defines functions to compute the distance between two objects, retrieve initial joint angles and positions for an object, and check if a gripper is close to a specific link of an object. These functions seem to be used for manipulation tasks in RoboGen.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_reward_api.py\":0-25",
            "content": "import pybullet as p\nimport numpy as np\nfrom manipulation.utils import get_pc\nfrom manipulation.grasping_utils import get_pc_and_normal\nfrom manipulation.utils import take_round_images\nfrom gpt_4.query import query\nimport os\nfrom scipy import ndimage\ndef compute_obj_to_center_dist(simulator, obj_a, obj_b):\n    obj_a_center = get_position(simulator, obj_a)\n    obj_b_bbox_min, obj_b_bbox_max = get_bounding_box(simulator, obj_b)\n    obj_b_center = (obj_b_bbox_min + obj_b_bbox_max) / 2\n    return np.linalg.norm(obj_a_center - obj_b_center)\ndef get_initial_joint_angle(simulator, object_name, joint_name):\n    object_name = object_name.lower()\n    return simulator.initial_joint_angle[object_name][joint_name]\ndef get_initial_pos_orient(simulator, object_name):\n    object_name = object_name.lower()\n    return simulator.initial_pos[object_name], np.array(p.getEulerFromQuaternion(simulator.initial_orient[object_name]))\n### success check functions\ndef gripper_close_to_object_link(simulator, object_name, link_name):\n    link_pc = get_link_pc(simulator, object_name, link_name)"
        },
        {
            "comment": "This code includes three functions. The first function, \"gripper_pos,\" retrieves the gripper position from the simulator. The second function, \"gripper_close_to_object,\" calculates the distance between the gripper and an object using point cloud data and returns True if the gripper is within 0.06 units of the object. The third function, \"check_grasped,\" checks if a specific object or link name is currently grasped by comparing it to the currently grasped object and/or link names retrieved from the simulator. Lastly, the \"get_grasped_object_name\" function retrieves the currently grasped object name from the simulator based on the object ID.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_reward_api.py\":26-53",
            "content": "    gripper_pos, _ = get_eef_pos(simulator)\n    distance = np.linalg.norm(link_pc.reshape(-1, 3) - gripper_pos.reshape(1, 3), axis=1)\n    if np.min(distance) < 0.06:\n        return True\n    return False\ndef gripper_close_to_object(simulator, object_name):\n    object_pc, _ = get_pc_and_normal(simulator, object_name)\n    gripper_pos, _ = get_eef_pos(simulator)\n    distance = np.linalg.norm(object_pc.reshape(-1, 3) - gripper_pos.reshape(1, 3), axis=1)\n    if np.min(distance) < 0.06:\n        return True\n    return False\ndef check_grasped(self, object_name, link_name=None):\n    object_name = object_name.lower()\n    grasped_object_name, grasped_link_name = get_grasped_object_and_link_name(self)\n    if link_name is None:\n        return grasped_object_name == object_name\n    else:\n        return grasped_object_name == object_name and grasped_link_name == link_name\ndef get_grasped_object_name(simulator):\n    grasped_object_id = simulator.suction_obj_id\n    if grasped_object_id is None:\n        return None\n    id_to_name = {v: k for k, v in simulator.urdf_ids.items()}"
        },
        {
            "comment": "Code snippet returns the grasped object name, link name based on the simulator state. It retrieves joint information and finds the maximum and minimum joint values for a given object and custom joint name.\n\nExplanation:\n- The code defines three functions: \"return_grasped_object\", \"get_grasped_object_and_link_name\", and \"get_joint_limit\". \n- The \"return_grasped_object\" function returns the name of the grasped object based on its ID from the simulator.\n- The \"get_grasped_object_and_link_name\" function identifies the grasped object and link, retrieves the joint information using PyBullet library's p.getJointInfo(), and returns the names of the grasped object and link. If the grasped link ID is -1, it defaults to 'base'.\n- The \"get_joint_limit\" function takes a simulator, object name, and custom joint name as input parameters. It retrieves the ID of the given object from the simulator's urdf_ids dictionary and uses PyBullet library functions to get the number of joints for that object. Then, it iterates over all the joints and finds the maximum and minimum values for the specified custom joint name.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_reward_api.py\":54-82",
            "content": "    return id_to_name[grasped_object_id]\ndef get_grasped_object_and_link_name(simulator):\n    grasped_object_id = simulator.suction_obj_id\n    grasped_link_id = simulator.suction_contact_link\n    if grasped_object_id is None or grasped_link_id is None:\n        return None, None\n    id_to_name = {v: k for k, v in simulator.urdf_ids.items()}\n    grasped_obj_name = id_to_name[grasped_object_id]\n    if grasped_link_id == -1:\n        return grasped_obj_name, \"base\"\n    joint_info = p.getJointInfo(grasped_object_id, grasped_link_id, physicsClientId=simulator.id)\n    link_name = joint_info[12].decode(\"utf-8\")\n    return grasped_obj_name, link_name\ndef get_joint_limit(simulator, object_name, custom_joint_name):\n    object_name = object_name.lower()\n    object_id = simulator.urdf_ids[object_name]\n    num_joints = p.getNumJoints(object_id, physicsClientId=simulator.id)\n    urdf_joint_name = custom_joint_name\n    max_joint_val = 0\n    min_joint_val = 0\n    for j_id in range(num_joints):\n        joint_info = p.getJointInfo(object_id, j_id, physicsClientId=simulator.id)"
        },
        {
            "comment": "The code defines functions to get the position, velocity, and orientation of an object in a simulator. It first ensures the object name is lowercase and retrieves the associated URDF ID. Then, it uses PyBullet physics library functions to obtain the position, velocity, or orientation. The eef_pos function gets the end effector position from the simulator. The code also compares maximum and minimum joint values for a given URDF joint name.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_reward_api.py\":83-109",
            "content": "        if joint_info[1].decode(\"utf-8\") == urdf_joint_name:\n            max_joint_val = joint_info[9]\n            min_joint_val = joint_info[8]\n            break\n    if min_joint_val < max_joint_val:\n        return min_joint_val, max_joint_val\n    else:\n        return max_joint_val, min_joint_val\ndef get_position(simulator, object_name):\n    object_name = object_name.lower()\n    object_id = simulator.urdf_ids[object_name]\n    return np.array(p.getBasePositionAndOrientation(object_id, physicsClientId=simulator.id)[0])\ndef get_velocity(simulator, object_name):\n    object_name = object_name.lower()\n    object_id = simulator.urdf_ids[object_name]\n    return np.array(p.getBaseVelocity(object_id, physicsClientId=simulator.id)[0])\ndef get_orientation(simulator, object_name):\n    object_name = object_name.lower()\n    object_id = simulator.urdf_ids[object_name]\n    return np.array(p.getEulerFromQuaternion(p.getBasePositionAndOrientation(object_id, physicsClientId=simulator.id)[1]))\ndef get_eef_pos(simulator):\n    "
        },
        {
            "comment": "The code retrieves the robot's end effector position and orientation using the simulator. It also gets the left and right finger joint positions, calculates the finger joint angles to determine finger distance apart, and gets the bounding box for a specific object in the simulation environment.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_reward_api.py\":109-122",
            "content": "robot_eef_pos, robot_eef_orient = simulator.robot.get_pos_orient(simulator.robot.right_end_effector)\n    return np.array(robot_eef_pos).flatten(), np.array(p.getEulerFromQuaternion(robot_eef_orient)).flatten()\ndef get_finger_pos(simulator):\n    left_finger_joint_pos =  p.getLinkState(simulator.robot.body, simulator.robot.right_gripper_indices[0], physicsClientId=simulator.id)[0]\n    right_finger_joint_pos = p.getLinkState(simulator.robot.body, simulator.robot.right_gripper_indices[1], physicsClientId=simulator.id)[0]\n    return np.array(left_finger_joint_pos), np.array(right_finger_joint_pos)\ndef get_finger_distance(simulator): \n    left_finger_joint_angle = p.getJointState(simulator.robot.body, simulator.robot.right_gripper_indices[0], physicsClientId=simulator.id)[0]\n    right_finger_joint_angle = p.getJointState(simulator.robot.body, simulator.robot.right_gripper_indices[1], physicsClientId=simulator.id)[0]\n    return left_finger_joint_angle + right_finger_joint_angle\ndef get_bounding_box(simulator, object_name):"
        },
        {
            "comment": "The code provides functions to retrieve bounding box information, joint state, and link position from a simulator. The get_bounding_box_link function returns the AABB of an object's link given its name, while get_joint_state retrieves the state of a specified joint within an object. Both functions require the object and link names to be in lowercase format.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_reward_api.py\":123-148",
            "content": "    object_name = object_name.lower()\n    object_id = simulator.urdf_ids[object_name]\n    if object_name != \"init_table\":\n        return simulator.get_aabb(object_id)\n    else:\n        return simulator.table_bbox_min, simulator.table_bbox_max\ndef get_bounding_box_link(simulator, object_name, link_name):\n    object_name = object_name.lower()\n    link_id = get_link_id_from_name(simulator, object_name, link_name)\n    object_id = simulator.urdf_ids[object_name]\n    return simulator.get_aabb_link(object_id, link_id)\ndef get_joint_state(simulator, object_name, custom_joint_name):\n    object_name = object_name.lower()\n    object_id = simulator.urdf_ids[object_name]\n    num_joints = p.getNumJoints(object_id, physicsClientId=simulator.id)\n    urdf_joint_name = custom_joint_name\n    for i in range(num_joints):\n        joint_info = p.getJointInfo(object_id, i, physicsClientId=simulator.id)\n        if joint_info[1].decode(\"utf-8\") == urdf_joint_name:\n            joint_index = i\n            break\n    return np.array(p.getJointState(object_id, joint_index, physicsClientId=simulator.id)[0])"
        },
        {
            "comment": "get_link_state: Retrieves the position and orientation of a specific link in a simulated object.\nget_link_pc: Retrieves point cloud data for a given link in a simulated object.\nset_joint_value: Sets the value of a joint in a simulated object to either maximum or a specified value.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_reward_api.py\":150-174",
            "content": "def get_link_state(simulator, object_name, custom_link_name):\n    object_name = object_name.lower()\n    object_id = simulator.urdf_ids[object_name]\n    urdf_link_name = custom_link_name\n    link_id = get_link_id_from_name(simulator, object_name, urdf_link_name)\n    link_pos, link_orient = p.getLinkState(object_id, link_id, physicsClientId=simulator.id)[:2]\n    return np.array(link_pos)\ndef get_link_pc(simulator, object_name, custom_link_name):\n    object_name = object_name.lower()\n    urdf_link_name = custom_link_name \n    link_com, all_pc = render_to_get_link_com(simulator, object_name, urdf_link_name)\n    return all_pc\ndef set_joint_value(simulator, object_name, joint_name, joint_value=\"max\"):\n    object_name = object_name.lower()\n    object_id = simulator.urdf_ids[object_name]\n    num_joints = p.getNumJoints(object_id, physicsClientId=simulator.id)\n    joint_index = None\n    max_joint_val = 0\n    min_joint_val = 0\n    for j_id in range(num_joints):\n        joint_info = p.getJointInfo(object_id, j_id, physicsClientId=simulator.id)"
        },
        {
            "comment": "The code initializes joint values and resets joint states based on a given joint value (max, min, or specified). It also checks if the object is within a bounding box, determines if an object has been grasped, and renders a link's center of mass.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_reward_api.py\":175-202",
            "content": "        print(joint_info[1])\n        if joint_info[1].decode(\"utf-8\") == joint_name:\n            joint_index = j_id\n            max_joint_val = joint_info[9]\n            min_joint_val = joint_info[8]\n            break\n    if joint_value == 'max':\n        p.resetJointState(object_id, joint_index, max_joint_val, physicsClientId=simulator.id)\n    elif joint_value == 'min':\n        p.resetJointState(object_id, joint_index, min_joint_val, physicsClientId=simulator.id)\n    else:\n        p.resetJointState(object_id, joint_index, joint_value, physicsClientId=simulator.id)\ndef in_bbox(simulator, pos, bbox_min, bbox_max):\n    if (pos[0] <= bbox_max[0] and pos[0] >= bbox_min[0] and \\\n        pos[1] <= bbox_max[1] and pos[1] >= bbox_min[1] and \\\n        pos[2] <= bbox_max[2] and pos[2] >= bbox_min[2]):\n        return True\n    return False\ndef grasped(simulator, object_name):\n    if object_name in simulator.grasped_object_list:\n        return True\n    return False\ndef render_to_get_link_com(simulator, object_name, urdf_link_name):    "
        },
        {
            "comment": "Code snippet sets the visibility of all objects in the simulation to invisible except for the target object. It then centers the camera to the target object and gets a round of images of the target object from different angles.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_reward_api.py\":203-224",
            "content": "    ### make all other objects invisiable\n    prev_rgbas = []\n    object_id = simulator.urdf_ids[object_name]\n    for obj_name, obj_id in simulator.urdf_ids.items():\n        if obj_name != object_name:\n            num_links = p.getNumJoints(obj_id, physicsClientId=simulator.id)\n            for link_idx in range(-1, num_links):\n                prev_rgba = p.getVisualShapeData(obj_id, link_idx, physicsClientId=simulator.id)[0][14:18]\n                prev_rgbas.append(prev_rgba)\n                p.changeVisualShape(obj_id, link_idx, rgbaColor=[0, 0, 0, 0], physicsClientId=simulator.id)\n    ### center camera to the target object\n    env_prev_view_matrix, env_prev_projection_matrix = simulator.view_matrix, simulator.projection_matrix\n    camera_width = 640\n    camera_height = 480\n    obj_id = object_id\n    min_aabb, max_aabb = simulator.get_aabb(obj_id)\n    camera_target = (max_aabb + min_aabb) / 2\n    distance = np.linalg.norm(max_aabb - min_aabb) * 1.2\n    elevation = 30\n    ### get a round of images of the target object"
        },
        {
            "comment": "This code snippet takes a series of round images from different angles using PyBullet simulator and hides the target object's link. Then it obtains another set of images with the hidden link, compares them to find differences, and determines the best image for calculating rewards.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_reward_api.py\":225-247",
            "content": "    rgbs, depths, view_matrices, projection_matrices = take_round_images(\n        simulator, camera_target, distance, elevation, \n        camera_width=camera_width, camera_height=camera_height, \n        z_near=0.01, z_far=10,\n        return_camera_matrices=True)\n    ### make the target link invisiable\n    link_id = get_link_id_from_name(simulator, object_name, urdf_link_name)\n    # import pdb; pdb.set_trace()\n    prev_link_rgba = p.getVisualShapeData(obj_id, link_id, physicsClientId=simulator.id)[0][14:18]\n    p.changeVisualShape(obj_id, link_id, rgbaColor=[0, 0, 0, 0], physicsClientId=simulator.id)\n    ### get a round of images of the target object with link invisiable\n    rgbs_link_invisiable, _ = take_round_images(\n        simulator, camera_target, distance, elevation,\n        camera_width=camera_width, camera_height=camera_height, \n        z_near=0.01, z_far=10,\n    )\n    ### use subtraction to get the link mask\n    max_num_diff_pixels = 0\n    best_idx = 0\n    for idx, (rgb, rgb_link_invisiable) in enumerate(zip(rgbs, rgbs_link_invisiable)):"
        },
        {
            "comment": "This code calculates the difference between two images and finds the best mask based on the maximum number of different pixels. It then determines the link mask center and back-projects it to obtain the 3D coordinate for the link's center of mass. Finally, it resets the object and link rgba values along with simulator view and projection matrices.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_reward_api.py\":248-272",
            "content": "        diff_image = np.abs(rgb - rgb_link_invisiable)\n        diff_pixels = np.sum(diff_image > 0)\n        if diff_pixels > max_num_diff_pixels:\n            max_num_diff_pixels = diff_pixels\n            best_idx = idx\n    best_mask = np.abs(rgbs[best_idx] - rgbs_link_invisiable[best_idx]) > 0\n    best_mask = np.any(best_mask, axis=2)\n    ### get the link mask center\n    center = ndimage.measurements.center_of_mass(best_mask)\n    center = [int(center[0]), int(center[1])]\n    ### back project the link mask center to get the link com in 3d coordinate\n    best_pc = get_pc(projection_matrices[best_idx], view_matrices[best_idx], depths[best_idx], camera_width, camera_height)\n    pt_idx = center[0] * camera_width + center[1]\n    link_com = best_pc[pt_idx]\n    best_pc = best_pc.reshape((camera_height, camera_width, 3))\n    all_pc = best_pc[best_mask]\n    ### reset the object and link rgba to previous values, and the simulator view matrix and projection matrix\n    p.changeVisualShape(obj_id, link_id, rgbaColor=prev_link_rgba, physicsClientId=simulator.id)"
        },
        {
            "comment": "Code snippet retrieves the link's center of mass and all point clouds for a given object. It iterates through each object in the simulator, changes their visual shapes to previous colors, and then checks if the link is within a bounding box. If not, it adjusts the link's position.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_reward_api.py\":274-296",
            "content": "    cnt = 0\n    object_id = simulator.urdf_ids[object_name]\n    for obj_name, obj_id in simulator.urdf_ids.items():\n        if obj_name != object_name:\n            num_links = p.getNumJoints(obj_id, physicsClientId=simulator.id)\n            for link_idx in range(-1, num_links):\n                p.changeVisualShape(obj_id, link_idx, rgbaColor=prev_rgbas[cnt], physicsClientId=simulator.id)\n                cnt += 1\n    simulator.view_matrix, simulator.projection_matrix = env_prev_view_matrix, env_prev_projection_matrix\n    ### add a safety check here in case the rendering fails\n    bounding_box = get_bounding_box_link(simulator, object_name, urdf_link_name)\n    if not in_bbox(simulator, link_com, bounding_box[0], bounding_box[1]):\n        link_com = (bounding_box[0] + bounding_box[1]) / 2\n    return link_com, all_pc\ndef get_link_id_from_name(simulator, object_name, link_name):\n    object_id = simulator.urdf_ids[object_name]\n    num_joints = p.getNumJoints(object_id, physicsClientId=simulator.id)\n    joint_index = None"
        },
        {
            "comment": "This code retrieves the joint index from either a link or joint name in the given object. It first maps the object name to its ID and then iterates through all joints of that object, comparing their names until it finds a match. Once the matching joint is found, its index is returned.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/manipulation/gpt_reward_api.py\":297-316",
            "content": "    for i in range(num_joints):\n        joint_info = p.getJointInfo(object_id, i, physicsClientId=simulator.id)\n        if joint_info[12].decode(\"utf-8\") == link_name:\n            joint_index = i\n            break\n    return joint_index\ndef get_joint_id_from_name(simulator, object_name, joint_name):\n    object_id = simulator.urdf_ids[object_name]\n    num_joints = p.getNumJoints(object_id, physicsClientId=simulator.id)\n    joint_index = None\n    for i in range(num_joints):\n        joint_info = p.getJointInfo(object_id, i, physicsClientId=simulator.id)\n        if joint_info[1].decode(\"utf-8\") == joint_name:\n            joint_index = i\n            break\n    return joint_index"
        }
    ]
}