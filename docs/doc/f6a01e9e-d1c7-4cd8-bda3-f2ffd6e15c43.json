{
    "summary": "This code trains reinforcement learning agents, saves best models and states, evaluates performance, creates GIFs, registers environments, and returns policy paths for given configurations.",
    "details": [
        {
            "comment": "Code imports necessary libraries and defines two functions, `custom_log_creator` and `setup_config`. The `custom_log_creator` function creates a custom logger with a specified prefix and time stamp. The `setup_config` function sets up a configuration for the PPO algorithm, including batch size, mini-batch size, hyperparameters, and model architecture.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/RL/ray_learn.py\":0-32",
            "content": "import os, sys, ray, shutil, glob\nimport numpy as np\nfrom ray.rllib.agents import ppo, sac\nfrom ray import tune\nfrom manipulation.utils import save_env, save_numpy_as_gif\nimport pickle\nimport datetime\nfrom ray.tune.logger import UnifiedLogger\nimport time\ndef custom_log_creator(custom_path, custom_str):\n    ts = time.time()\n    time_string = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d-%H-%M-%S')\n    logdir_prefix = \"{}_{}\".format(custom_str, time_string)\n    log_dir = os.path.join(custom_path, logdir_prefix)\n    def logger_creator(config):\n        if not os.path.exists(log_dir):\n            os.makedirs(log_dir)\n        return UnifiedLogger(config, log_dir, loggers=None)\n    return logger_creator\ndef setup_config(algo, seed=0, env_config={}, eval=False):\n    if algo == 'ppo':\n        config = ppo.DEFAULT_CONFIG.copy()\n        config['train_batch_size'] = 128 * 100\n        config['num_sgd_iter'] = 50\n        config['sgd_minibatch_size'] = 128\n        config['lambda'] = 0.95\n        config['model']['fcnet_hiddens'] = [128, 128]"
        },
        {
            "comment": "If the algorithm is SAC (Stochastic Actor-Critic), this code sets up the corresponding configuration and returns it. The configuration includes parameters such as the number of timesteps per iteration, learning starts, hidden layers for Q model and policy model, framework to use (Torch), number of workers, seed value for randomness, log level, and environment configuration. Additionally, a policy agent is created either using PPOTrainer or SACTrainer depending on the algorithm chosen. The logger creator function is also specified to customize logging output.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/RL/ray_learn.py\":33-57",
            "content": "    elif algo == 'sac':\n        config = sac.DEFAULT_CONFIG.copy()\n        config['timesteps_per_iteration'] = 400\n        config['learning_starts'] = 1000\n        config['Q_model']['fcnet_hiddens'] = [256, 256, 256]\n        config['policy_model']['fcnet_hiddens'] = [256, 256, 256]\n    config['framework'] = 'torch'\n    if not eval:\n        config['num_workers'] = 8\n    else:\n        config['num_workers'] = 1\n    config['seed'] = seed\n    config['log_level'] = 'ERROR'\n    config[\"env_config\"] = env_config\n    return config\ndef load_policy(algo, env_name, policy_path=None, seed=0, env_config={}, eval=False):\n    if algo == 'ppo':\n        agent = ppo.PPOTrainer(setup_config(algo, seed, env_config, eval=eval), env_name,\n                               logger_creator=custom_log_creator(\"data/local/ray_results\", env_name)\n        )\n    elif algo == 'sac':\n        agent = sac.SACTrainer(setup_config(algo, seed, env_config, eval=eval), env_name, \n                               logger_creator=custom_log_creator(\"data/local/ray_results\", env_name)"
        },
        {
            "comment": "This code restores or loads a policy from a specified directory and initializes the Ray environment for training an agent. If a policy path is provided, it checks if it contains 'checkpoint' and restores the agent using that path. If not, it finds the most recent checkpoint in the given directory and restores the agent with that checkpoint. It then returns the trained agent or initializes the Ray environment if it's not already initialized.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/RL/ray_learn.py\":58-79",
            "content": "        )\n    if policy_path is not None:\n        if 'checkpoint' in policy_path:\n            agent.restore(policy_path)\n        else:\n            # Find the most recent policy in the directory\n            directory = os.path.join(policy_path, algo, env_name)\n            files = [f.split('_')[-1] for f in glob.glob(os.path.join(directory, 'checkpoint_*'))]\n            files_ints = [int(f) for f in files]\n            if files:\n                checkpoint_max = max(files_ints)\n                checkpoint_num = files_ints.index(checkpoint_max)\n                checkpoint_path = os.path.join(directory, 'checkpoint_%s' % files[checkpoint_num], 'checkpoint-%d' % checkpoint_max)\n                agent.restore(checkpoint_path)\n            return agent, None\n    return agent, None\ndef train(env_name, algo, timesteps_total=2000000, save_dir='./trained_models/', load_policy_path='', seed=0, \n          env_config={}, eval_interval=20000, render=False):\n    if not ray.is_initialized():\n        ray.init(num_cpus=8, ignore_reinit_error=True, log_to_driver=False)"
        },
        {
            "comment": "This code loads a policy and trains an agent using reinforcement learning. It saves the best model, state files, and tracks performance metrics such as timesteps, mean/min/max rewards, and training iteration. If a checkpoint path exists, it deletes the old saved policy.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/RL/ray_learn.py\":80-101",
            "content": "    agent, checkpoint_path = load_policy(algo, env_name, load_policy_path, env_config=env_config, seed=seed)\n    env = make_env(env_config, render=render)\n    best_model_save_path = os.path.join(save_dir, 'best_model')\n    best_state_save_path = os.path.join(save_dir, 'best_state')\n    if not os.path.exists(best_state_save_path):\n        os.makedirs(best_state_save_path)\n    timesteps = 0\n    eval_time = 1\n    best_ret = -np.inf\n    best_rgbs = None\n    best_state_files = None\n    while timesteps < timesteps_total:\n        result = agent.train()\n        timesteps = result['timesteps_total']\n        print(f\"Iteration: {result['training_iteration']}, total timesteps: {result['timesteps_total']}, total time: {result['time_total_s']:.1f}, FPS: {result['timesteps_total']/result['time_total_s']:.1f}, mean reward: {result['episode_reward_mean']:.1f}, min/max reward: {result['episode_reward_min']:.1f}/{result['episode_reward_max']:.1f}\")\n        sys.stdout.flush()\n        # Delete the old saved policy\n        if checkpoint_path is not None:"
        },
        {
            "comment": "Saves recently trained policy, then evaluates it using a loop to take actions in the environment, saving states and visuals for evaluation.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/RL/ray_learn.py\":102-126",
            "content": "            shutil.rmtree(os.path.dirname(checkpoint_path), ignore_errors=True)\n        # Save the recently trained policy\n        checkpoint_path = agent.save(save_dir)\n        if timesteps > eval_time * eval_interval:\n            obs = env.reset()\n            done = False\n            ret = 0\n            rgbs = []\n            state_files = []\n            states = []\n            t_idx = 0\n            state_save_path = os.path.join(save_dir, \"eval_{}\".format(eval_time))\n            if not os.path.exists(state_save_path):\n                os.makedirs(state_save_path)\n            while not done:\n                # Compute the next action using the trained policy\n                action = agent.compute_action(obs, explore=False)\n                # Step the simulation forward using the action from our trained policy\n                obs, reward, done, info = env.step(action)\n                ret += reward\n                rgb, depth = env.render()\n                rgbs.append(rgb)\n                state_file_path = os.path.join(state_save_path, \"state_{}.pkl\".format(t_idx))"
        },
        {
            "comment": "This code block saves the environment state at each timestep and evaluates the agent's performance. If the return is better than the previous best, it saves the model, states, and RGBs. It then creates separate GIF files for all states and the best execution.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/RL/ray_learn.py\":127-146",
            "content": "                state = save_env(env, save_path=state_file_path)\n                state_files.append(state_file_path)\n                states.append(state)\n                t_idx += 1\n            save_numpy_as_gif(np.array(rgbs), \"{}/{}.gif\".format(state_save_path, \"execute\"))\n            print(\"evaluating at {} return is {}\".format(timesteps, ret))\n            eval_time += 1\n            if ret > best_ret:\n                best_ret = ret\n                best_model_path = agent.save(best_model_save_path)\n                best_rgbs = rgbs\n                best_state_files = state_files\n                for idx, state in enumerate(states):\n                    with open(os.path.join(best_state_save_path, \"state_{}.pkl\".format(idx)), 'wb') as f:\n                        pickle.dump(state, f, pickle.HIGHEST_PROTOCOL)\n                with open(os.path.join(best_state_save_path, \"return_{}.txt\".format(round(ret, 3))), 'w') as f:\n                    f.write(str(ret))\n                save_numpy_as_gif(np.array(best_rgbs), \"{}/{}.gif\".format(best_state_save_path, \"best\"))"
        },
        {
            "comment": "The code initializes a Ray environment, renders the policy for a given algorithm and policy path, and returns the best model path, RGB frames, and state files. It also makes an environment using a task configuration file and prints it before rendering.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/RL/ray_learn.py\":148-178",
            "content": "    env.disconnect()\n    return best_model_path, best_rgbs, best_state_files\ndef render_policy(env, env_name, algo, policy_path, seed=0, n_episodes=1, env_config={}):\n    ray.init(num_cpus=1, ignore_reinit_error=True, log_to_driver=False)\n    if env is None:\n        env = make_env(env_name)\n    test_agent, _ = load_policy(algo, env_name, policy_path, seed, env_config, eval=True)\n    env.render()\n    frames = []\n    for episode in range(n_episodes):\n        obs = env.reset()\n        done = False\n        while not done:\n            # Compute the next action using the trained policy\n            action = test_agent.compute_action(obs)\n            # Step the simulation forward using the action from our trained policy\n            obs, reward, done, info = env.step(action)\n            env.render()\n    env.disconnect()\ndef make_env(config, render=False):\n    import yaml\n    from manipulation.utils import build_up_env\n    print(config)\n    task_config_path = config['task_config_path']\n    task_name = config['task_name']"
        },
        {
            "comment": "This code builds a robotic environment based on the provided configuration. It takes in parameters like task_config_path, solution_path, task_name, last_restore_state_file, save_path, action_space, algo, timesteps_total, load_policy_path, seed, render, randomize, use_bard, obj_id, use_gpt_size, use_gpt_joint_angle, use_gpt_spatial_relationship, and use_distractor. It returns the built environment for use in reinforcement learning tasks.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/RL/ray_learn.py\":179-207",
            "content": "    last_restore_state_file = config['last_restore_state_file']\n    solution_path = config['solution_path']\n    action_space = config['action_space']\n    env, safe_config = build_up_env(\n            task_config_path, \n            solution_path, \n            task_name, \n            last_restore_state_file, \n            render=render, \n            action_space=action_space, \n            randomize=config['randomize'], \n            obj_id=config['obj_id'],\n        )\n    return env\ndef run_RL(task_config_path, solution_path, task_name, last_restore_state_file, save_path, \n           action_space='delta-translation', algo=\"sac\", timesteps_total=1000000, load_policy_path=None, seed=0, \n           render=False, randomize=False, use_bard=True, obj_id=0, \n           use_gpt_size=True, use_gpt_joint_angle=True, use_gpt_spatial_relationship=True,\n           use_distractor=False):\n    env_name = task_name\n    env_config = {\n        \"task_config_path\": task_config_path,\n        \"solution_path\": solution_path,\n        \"task_name\": task_name,"
        },
        {
            "comment": "This code registers an environment, trains a policy using the registered environment, and returns the best policy path, RGB values, and best trajectory state paths. It also takes in several configurations and sets up evaluation intervals.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/RL/ray_learn.py\":208-227",
            "content": "        \"last_restore_state_file\": last_restore_state_file,\n        \"action_space\": action_space,\n        \"randomize\": randomize,\n        \"use_bard\": use_bard,\n        \"obj_id\": obj_id,\n        \"use_gpt_size\": use_gpt_size,\n        \"use_gpt_joint_angle\": use_gpt_joint_angle,\n        \"use_gpt_spatial_relationship\": use_gpt_spatial_relationship,\n        \"use_distractor\": use_distractor\n    }\n    timesteps_total = 1000000 \n    eval_interval = 20000 \n    tune.register_env(env_name, lambda config: make_env(config))\n    best_policy_path, rgbs, best_traj_state_paths = train(env_name, algo, timesteps_total=timesteps_total, \n                            load_policy_path=load_policy_path, save_dir=save_path, seed=seed, env_config=env_config, render=render,\n                            eval_interval=eval_interval)\n    return best_policy_path, rgbs, best_traj_state_paths"
        }
    ]
}