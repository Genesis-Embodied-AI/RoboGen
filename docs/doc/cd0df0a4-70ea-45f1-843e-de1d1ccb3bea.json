{
    "summary": "RoboGen is a generative robotic agent using Genesis AI, PyBullet, and OMPL for tasks. The `execute_locomotion.py` script with task configuration file executes pre-generated tasks in its environment.",
    "details": [
        {
            "comment": "This code is for a README file of the RoboGen project. It displays the project logo, provides a brief description of the paper and includes links to the arXiv paper and project page.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/readme.md\":1-23",
            "content": "<div align=\"center\">\n  <img width=\"500px\" src=\"imgs/logo.png\"/>\n  # RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation\n</div>\n---\n<div align=\"center\">\n  <img src=\"imgs/teaser.png\"/>\n</div> \n<p align=\"left\">\n    <a href='http://arxiv.org/abs/2311.01455'>\n      <img src='https://img.shields.io/badge/Paper-arXiv-green?style=plastic&logo=arXiv&logoColor=green' alt='Paper arXiv'>\n    </a>\n    <a href='https://robogen-ai.github.io/'>\n      <img src='https://img.shields.io/badge/Project-Page-blue?style=plastic&logo=Google%20chrome&logoColor=blue' alt='Project Page'>\n    </a>\n</p>\nThis is the official repo for the paper:\n> **[RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation](https://robogen-ai.github.io/)**  \n> [Yufei Wang*](https://yufeiwang63.github.io/), [Zhou Xian*](https://zhou-xian.com/), [Feng Chen*](https://robogen-ai.github.io/), [Tsun-Hsuan Wang](https://zswang666.github.io/), [Yian Wang](https://wangyian-me.github.io/), ["
        },
        {
            "comment": "RoboGen is a self-guided and generative robotic agent that proposes new tasks, generates environments, and acquires skills autonomously. It's powered by Genesis, an AI engine for robot learning, which is still under development. The codebase here uses PyBullet for rigid manipulation and locomotion tasks; soft-body manipulation and more tasks will be added later with Genesis. The code is currently being cleaned and tested, stay tuned!",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/readme.md\":24-34",
            "content": "Katerina Fragkiadaki](https://www.cs.cmu.edu/~katef/), [Zackory Erickson](https://zackory.com/), [David Held](https://davheld.github.io/), [Chuang Gan](https://people.csail.mit.edu/ganchuang/)   \nRoboGen is a **self-guided** and **generative** robotic agent that autonomously proposes **new tasks**, generates corresponding **environments**, and acquires **new robotic skills** continuously.\nRoboGen is powered by [Genesis](https://github.com/Genesis-Embodied-AI/Genesis), a multi-material multi-solver generative simulation engine for general-purpose robot learning. \nGenesis is still under active development and will be released soon. This repo contains a re-implementation of RoboGen using PyBullet, containing generation and learning of rigid manipulation and locomotion tasks. Our full pipeline containing soft-body manipulation and more tasks will be released later together with Genesis.\nWe are still in the process of cleaning the code & testing, please stay tuned!\n## Table of Contents\n- [Setup](#setup)"
        },
        {
            "comment": "The code is providing instructions for setting up the RoboGen environment using Git and a conda environment. It also explains that the project utilizes Open Motion Planning Library (OMPL) for motion planning as part of its pipeline, and provides commands to install OMPL system-wide for Python.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/readme.md\":35-62",
            "content": "  - [RoboGen](#RoboGen)\n  - [OMPL](#Open-Motion-Planning-Library)\n  - [Dataset](#dataset)\n- [Let's Rock!](#lets-rock)\n  - [Automated Task Generation & Skill Learning](#One-click-for-all)\n  - [Generate Tasks](#Generate-tasks)\n  - [Learn Skills](#Learn-skills)\n  - [Pre-generated Tasks](#Pre-generated-tasks)\n## Setup\n### RoboGen\nClone this git repo.\n```\ngit clone https://github.com/Genesis-Embodied-AI/RoboGen.git\n```\nWe recommend working with a conda environment.\n```\nconda env create -f environment.yaml\nconda activate robogen\n```\nIf installing from this yaml file doesn't work, manual installation of missing packages should also work.\n### Open Motion Planning Library\nRoboGen leverages [Open Motion Planning Library (OMPL)](https://ompl.kavrakilab.org/) for motion planning as part of the pipeline to solve the generated task. \nTo install OMPL, run\n```\n./install_ompl_1.5.2.sh --python\n```\nwhich will install the ompl with system-wide python. Note at line 19 of the installation script OMPL requries you to run `sudo apt-g"
        },
        {
            "comment": "Upgrade the system packages, which might cause trouble if commented during installation (not fully tested). Export OMPL installation to conda environment for use with RoboGen. Update the path accordingly. RoboGen uses PartNet-Mobility dataset and provides parsed version [here]. Download and unzip it to `data/dataset`. Retrieve objects from objaverse using SentenceBert.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/readme.md\":62-73",
            "content": "et -y upgrade`. This might cause trouble for your system packages, so you could probably comment this line during installation (the installation might fail, not fully tested with the line commented).\nThen, export the installation to the conda environment to be used with RoboGen:\n```\necho \"path_to_your_ompl_installation_from_last_step/OMPL/ompl-1.5.2/py-bindings\" >> ~/miniconda3/envs/robogen/lib/python3.9/site-packages/ompl.pth\n```\nremember to change the path to be your ompl installed path and conda environment path.\n### Dataset\nRoboGen uses [PartNet-Mobility](https://sapien.ucsd.edu/browse) for task generation and scene population. We provide a parsed version [here](https://drive.google.com/file/d/1d-1txzcg_ke17NkHKAolXlfDnmPePFc6/view?usp=sharing) (which parses the urdf to extract the articulation tree as a shortened input to GPT-4). After downloading, please unzip it and put it in the `data` folder, so it looks like `data/dataset`.\nFor retrieving objects from objaverse, we embed object descriptions from objaverse using [SentenceBert](https://www.sbert.net/). "
        },
        {
            "comment": "The code provides instructions for generating embeddings, obtaining default tag embeddings from a Google Drive link or generating them by running specific scripts. It also mentions the requirement to place the files in a specified folder structure and highlights the need to put an OpenAI API key in `gpt_4/query.py` before running the main script (`run.py`) for RoboGen to generate tasks, build scenes, and solve problems to learn skills.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/readme.md\":74-98",
            "content": "If you want to generate these embeddings by yourself, run\n```\npython objaverse_utils/embed_all_annotations.py\npython objaverse_utils/embed_cap3d.py\npython objaverse_utils/embed_partnet_annotations.py\n```\nWe also provide the embeddings [here](https://drive.google.com/file/d/1dFDpG3tlckTUSy7VYdfkNqtfVctpn3T6/view?usp=sharing) if generation takes too much time. After downloading, unzip and put it under `objaverse_utils/data/` folder, so it looks like \n```\nobjaverse_utils/data/default_tag_embeddings_*.pt\nobjaverse_utils/data/default_tag_names_*.pt\nobjaverse_utils/data/default_tag_uids_*.pt\nobjaverse_utils/data/cap3d_sentence_bert_embeddings.pt\nobjaverse_utils/data/partnet_mobility_category_embeddings.pt\n```\n## Let's Rock!\n### One click for all\nPut your OpenAI API key at the top of `gpt_4/query.py`, and simply run\n```\nsource prepare.sh\npython run.py\n``` \nRoboGen will then generate the task, build the scene in pybullet, and solve it to learn the corresponding skill.  \nIf you wish to generate manipulation tasks relevant to a specific object, e.g., microwave, you can run  "
        },
        {
            "comment": "This code describes how to use the RoboGen software for generating tasks, learning skills, and executing them. The script allows users to generate tasks by running `python run.py --train 0` or generate tasks from a given description using `python gpt_4/prompts/prompt_from_description.py`. Skills can be learned by running the appropriate execute or execute_locomotion scripts with the generated task configuration as input, such as `python execute.py --task_config_path example_tasks/Change_Lamp_Direction/Change_Lam`.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/readme.md\":99-126",
            "content": "```\npython run.py --category Microwave\n```\n### Generate tasks\nIf you wish to just generate the tasks, run\n```\npython run.py --train 0\n```\nwhich will generate the tasks, scene config yaml files, and training supervisions. The generated tasks will be stored at `data/generated_tasks_release/`.  \nIf you want to generate task given a text description, you can run\n```\npython gpt_4/prompts/prompt_from_description.py --task_description [TASK_DESCRIPTION] --object [PARTNET_ARTICULATION_OBJECT_CATEGORY]\n``` \nFor example,\n```\npython gpt_4/prompts/prompt_from_description.py --task_description \"Put a pen into the box\" --object \"Box\"\n```\n### Learn skills\nIf you wish to just learn the skill with a generated task, run\n```\npython execute.py --task_config_path [PATH_TO_THE_GENERATED_TASK_CONFIG] # for manipulation tasks\npython execute_locomotion.py --task_config_path [PATH_TO_THE_GENERATED_TASK_CONFIG] # for locomotion tasks\n```\nFor example,\n```\npython execute.py --task_config_path example_tasks/Change_Lamp_Direction/Change_Lam"
        },
        {
            "comment": "This code snippet is calling the `execute_locomotion.py` script with a task configuration file located at `example_tasks/task_Turn_right/Turn_right.yaml`. The script executes a pre-generated task related to turning right, possibly for learning and transfer purposes in RoboGen's environment.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/readme.md\":126-135",
            "content": "p_Direction_The_robotic_arm_will_alter_the_lamps_light_direction_by_manipulating_the_lamps_head.yaml  \npython execute_locomotion.py --task_config_path example_tasks/task_Turn_right/Turn_right.yaml\n```\n### Pre-generated tasks\nIn `example_tasks` we include a number of generated tasks from RoboGen. We hope this could be useful for, e.g., language conditioned multi-task learning & transfer learning & low-level skill learning. We hope to keep updating the list! \n## Acknowledgements\n- The interface between OMPL and pybullet is based on [pybullet_ompl](https://github.com/lyfkyle/pybullet_ompl).\n- Part of the objaverse annotations are from [Scalable 3D Captioning with Pretrained Models](https://arxiv.org/abs/2306.07279)"
        }
    ]
}