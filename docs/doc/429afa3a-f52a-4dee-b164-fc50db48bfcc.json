{
    "summary": "The code utilizes multiprocessing to calculate costs for state-action trajectories and applies a cost function in parallel using a Pool context. An instance of the ParallelRolloutWorker class is created and actions are sampled to obtain costs.",
    "details": [
        {
            "comment": "This function uses multiprocessing to calculate costs for a given state and action trajectories. It creates a new environment for each process, resets it, calculates the rewards for each action in the trajectory, and returns the total negative reward and the individual rewards as the cost.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/cem_policy/parallel_worker.py\":0-32",
            "content": "import multiprocessing as mp\nfrom multiprocessing import Pool\nimport numpy as np\nimport pybullet as p\nfrom .utils import *\n# env = None\ndef get_cost(args):\n    cur_state, action_trajs, env_class, env_kwargs, worker_i = args\n    # global env\n    # if env is None:\n    #     # Need to create the env inside the function such that the GPU buffer is associated with the child process and avoid any deadlock.\n    #     # Use the global variable to access the child process-specific memory\n    # import pdb; pdb.set_trace()\n    print(\"env \", env_kwargs)\n    env = env_class(**env_kwargs)\n    env.reset()\n    N = action_trajs.shape[0]\n    costs = []\n    for i in range(N):\n        # print(worker_i, f'{i}/{N}')\n        load_env(env, state=cur_state)\n        ret = 0\n        rewards = []\n        for action in action_trajs[i, :]:\n            _, reward, _, _ = env.step(action)\n            ret += reward\n            rewards.append(reward)\n        costs.append([-ret, rewards])\n        # print('get_cost {}: {}'.format(i, ret))\n    p.disconnect(env.id)"
        },
        {
            "comment": "Class for parallel rollout of trajectories in RoboGen's cem_policy package. Initializes worker number, planning horizon, action dimension and environment class and kwargs. Defines cost_function that splits action trajectories into chunks and applies get_cost function in a Pool (parallel) context to calculate costs. Returns a flattened list of costs.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/cem_policy/parallel_worker.py\":33-54",
            "content": "    return costs\nclass ParallelRolloutWorker(object):\n    \"\"\" Rollout a set of trajectory in parallel. \"\"\"\n    def __init__(self, env_class, env_kwargs, plan_horizon, action_dim, num_worker=32):\n        self.num_worker = num_worker\n        self.plan_horizon, self.action_dim = plan_horizon, action_dim\n        self.env_class, self.env_kwargs = env_class, env_kwargs\n        self.pool = Pool(processes=num_worker)\n    def cost_function(self, init_state, action_trajs):\n        action_trajs = action_trajs.reshape([-1, self.plan_horizon, self.action_dim])\n        splitted_action_trajs = np.array_split(action_trajs, self.num_worker)\n        ret = self.pool.map(get_cost, [(init_state, splitted_action_trajs[i], self.env_class, self.env_kwargs, i) for i in range(self.num_worker)])\n        # ret = get_cost((init_state, action_trajs, self.env_class, self.env_kwargs))\n        flat_costs = [item for sublist in ret for item in sublist]  # ret is indexed first by worker_num then traj_num\n        return flat_costs\nif __name__ == '__main__':"
        },
        {
            "comment": "The code initializes a simple environment, sets the task configuration, and creates an instance of the ParallelRolloutWorker class. It then samples actions and calculates the cost associated with those actions using the rollout worker's cost function. Finally, it prints the calculated cost.",
            "location": "\"/media/root/Prima/works/RoboGen/docs/src/cem_policy/parallel_worker.py\":55-82",
            "content": "    # Can be used to benchmark the system\n    from manipulation.sim import SimpleEnv\n    import copy\n    from RL.train_RL_api import default_config\n    import pickle\n    task_config = \"gpt_4/data/parsed_configs_semantic_articulated/test_without_table.yaml\"\n    config = copy.deepcopy(default_config)\n    config['config_path'] = task_config\n    config['gui'] = False\n    env = SimpleEnv(**config)\n    env.reset()\n    env_class = SimpleEnv\n    env_kwargs = config\n    initial_state = \"manipulation/gpt_tasks/Load_Dishes_into_Dishwasher/12594/RL/open_the_dishwasher_door/best_final_state.pkl\"\n    with open(initial_state, 'rb') as f:\n        initial_state = pickle.load(f)\n    action_trajs = []\n    for i in range(700):\n        action = env.action_space.sample()\n        action_trajs.append(action)\n    action_trajs = np.array(action_trajs)\n    rollout_worker = ParallelRolloutWorker(env_class, env_kwargs, 10, 7)\n    cost = rollout_worker.cost_function(initial_state, action_trajs)\n    print('cost:', cost)"
        }
    ]
}